 ______                 ______            _
(_____ \               (_____ \          | |
 _____) ) _   _  ____   _____) )___    __| |
|  __  / | | | ||  _ \ |  ____// _ \  / _  |
| |  \ \ | |_| || | | || |    | |_| |( (_| |
|_|   |_||____/ |_| |_||_|     \___/  \____|

For detailed documentation and guides, please visit:
[1;34mhttps://docs.runpod.io/[0m and [1;34mhttps://blog.runpod.io/[0m


[?2004hroot@7dada50e0e4b:/workspace/ChatterBox-Finetuning# [Kroot@7dada50e0e4b:/workspace/ChatterBox-Finetuning# screen[1Pclearrm -rf outputs/[10Pcleardeepspeed --include localhost:0 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-previewM[C[C[C[C[C[C[C[C[Cclear[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
[?2004l[H[J[?2004hroot@7dada50e0e4b:/workspace/ChatterBox-Finetuning# clearscreen[1Pclearrm -rf outputs/[10Pcleardeepspeed --include localhost:0 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview[C[C[CM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C --master_port 54906 custom_train_stage1.py --version[1P llava-llama-2-13b-chat-lightning-previewM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C2 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-previewM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C, --master_port 54906 custom_train_stage1.py --versi[1@oM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C3 --master_port 54906 custom_train_stage1.py --vers[1@iM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C, --master_port 54906 custom_train_stage1.py --ver[1@sM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C4 --master_port 54906 custom_train_stage1.py --ve[1@rM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C, --master_port 54906 custom_train_stage1.py --v[1@eM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C5 --master_port 54906 custom_train_stage1.py --[1@vM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C, --master_port 54906 custom_train_stage1.py -[1@-M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C6 --master_port 54906 custom_train_stage1.py [C[1@-M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C, --master_port 54906 custom_train_stage1.py[1@ M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C7 --master_port 54906 custom_train_stage1.p[1@yM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C, --master_port 54906 custom_train_stage1.[1@pM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C8 --master_port 54906 custom_train_stage1[1@.M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C, --master_port 54906 custom_train_stage[1@1M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C9 --master_port 54906 custom_train_stag[1@eM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C

[?2004l[2024-02-19 00:38:18,125] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-02-19 00:38:20.601311: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 00:38:20.604809: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 00:38:20.642611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 00:38:21.387260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-02-19 00:38:22,448] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-02-19 00:38:22,448] [INFO] [runner.py:570:main] cmd = /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMiwgMywgNCwgNSwgNiwgNywgOCwgOV19 --master_addr=127.0.0.1 --master_port=54906 --enable_each_rank_log=None custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview
[2024-02-19 00:38:24,065] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-02-19 00:38:26.450415: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 00:38:26.453879: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 00:38:26.491779: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 00:38:27.234789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-02-19 00:38:28,296] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8
[2024-02-19 00:38:28,296] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1
[2024-02-19 00:38:28,296] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1
[2024-02-19 00:38:28,296] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-02-19 00:38:28,296] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8
[2024-02-19 00:38:28,296] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-02-19 00:38:28,296] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1
[2024-02-19 00:38:28,296] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2, 3, 4, 5, 6, 7, 8, 9]}
[2024-02-19 00:38:28,297] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-02-19 00:38:28,297] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-02-19 00:38:28,297] [INFO] [launch.py:163:main] dist_world_size=8
[2024-02-19 00:38:28,297] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2,3,4,5,6,7,8,9
[2024-02-19 00:38:30,027] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 00:38:30,101] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 00:38:30,147] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 00:38:30,194] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 00:38:30,223] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 00:38:30,254] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 00:38:30,280] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 00:38:30,328] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-02-19 00:38:32.637758: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 00:38:32.641760: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 00:38:32.687581: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 00:38:32.713011: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 00:38:32.717084: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 00:38:32.762637: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 00:38:32.772665: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 00:38:32.776718: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 00:38:32.823140: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 00:38:32.875193: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 00:38:32.878700: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 00:38:32.918445: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 00:38:32.947299: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 00:38:32.951676: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 00:38:32.978973: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 00:38:32.982933: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 00:38:32.982993: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 00:38:32.986480: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 00:38:32.998763: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 00:38:33.024850: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 00:38:33.030295: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 00:38:33.122468: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 00:38:33.126885: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 00:38:33.174027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 00:38:33.709236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 00:38:33.729778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 00:38:33.742430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 00:38:33.771980: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 00:38:33.829014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 00:38:33.989523: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 00:38:33.997724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 00:38:33.999378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.23s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.08s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.09s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.10s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.39s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.05s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.09s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.13s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.01s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:09<00:04,  4.95s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:04,  4.98s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.01s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.09s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:09<00:04,  4.97s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.15s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.40s/it]
Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:09<00:04,  4.96s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:09<00:04,  4.97s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.11s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.35s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.15s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.39s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.17s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.41s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.21s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.48s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.15s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.38s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.12s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.36s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.13s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.38s/it]
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-19 00:40:23,805] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 00:40:23,806] [INFO] [comm.py:637:init_distributed] cdb=None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-19 00:40:29,765] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 00:40:29,765] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 00:40:29,891] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 00:40:29,891] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 00:40:29,895] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 00:40:29,896] [INFO] [comm.py:637:init_distributed] cdb=None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-19 00:40:30,908] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 00:40:30,908] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 00:40:30,909] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-19 00:40:32,458] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 00:40:32,459] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 00:40:32,574] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 00:40:32,574] [INFO] [comm.py:637:init_distributed] cdb=None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-19 00:40:33,112] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 00:40:33,113] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 00:40:54,047] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationUsing /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...

Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.0862114429473877 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...Loading extension module fused_adam...

Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.10138463973999023 seconds
Time to load fused_adam op: 0.10164999961853027 seconds
Time to load fused_adam op: 0.10216236114501953 seconds
Time to load fused_adam op: 0.10184359550476074 seconds
Time to load fused_adam op: 0.10152816772460938 seconds
Time to load fused_adam op: 0.10222125053405762 seconds
Time to load fused_adam op: 0.10237646102905273 seconds
[2024-02-19 00:40:54,484] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-02-19 00:40:54,484] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-02-19 00:40:54,837] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-02-19 00:40:54,838] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-02-19 00:40:54,838] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-02-19 00:40:54,838] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 500000000
[2024-02-19 00:40:54,838] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 500000000
[2024-02-19 00:40:54,838] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: False
[2024-02-19 00:40:54,838] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
[2024-02-19 00:41:15,064] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-02-19 00:41:15,065] [INFO] [utils.py:803:see_memory_usage] MA 26.25 GB         Max_MA 26.43 GB         CA 26.61 GB         Max_CA 27 GB 
[2024-02-19 00:41:15,065] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 41.94 GB, percent = 8.3%
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2024-02-19 00:41:15,267] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-02-19 00:41:15,268] [INFO] [utils.py:803:see_memory_usage] MA 26.99 GB         Max_MA 27.36 GB         CA 27.72 GB         Max_CA 28 GB 
[2024-02-19 00:41:15,268] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 40.8 GB, percent = 8.1%
[2024-02-19 00:41:15,268] [INFO] [stage_1_and_2.py:513:__init__] optimizer state initialized
[2024-02-19 00:41:15,477] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-02-19 00:41:15,478] [INFO] [utils.py:803:see_memory_usage] MA 26.99 GB         Max_MA 26.99 GB         CA 27.72 GB         Max_CA 28 GB 
[2024-02-19 00:41:15,478] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 41.6 GB, percent = 8.3%
[2024-02-19 00:41:15,480] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-02-19 00:41:15,481] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-02-19 00:41:15,481] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f1f27f0bfa0>
[2024-02-19 00:41:15,481] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2024-02-19 00:41:15,485] [INFO] [config.py:968:print] DeepSpeedEngine configuration:
[2024-02-19 00:41:15,485] [INFO] [config.py:972:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-02-19 00:41:15,485] [INFO] [config.py:972:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-02-19 00:41:15,485] [INFO] [config.py:972:print]   amp_enabled .................. False
[2024-02-19 00:41:15,485] [INFO] [config.py:972:print]   amp_params ................... False
[2024-02-19 00:41:15,485] [INFO] [config.py:972:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-02-19 00:41:15,485] [INFO] [config.py:972:print]   bfloat16_enabled ............. False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   checkpoint_parallel_write_pipeline  False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   checkpoint_tag_validation_enabled  True
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   checkpoint_tag_validation_fail  False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1f1c22be20>
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   communication_data_type ...... None
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   curriculum_enabled_legacy .... False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   curriculum_params_legacy ..... False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   data_efficiency_enabled ...... False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   dataloader_drop_last ......... False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   disable_allgather ............ False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   dump_state ................... False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   dynamic_loss_scale_args ...... None
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   eigenvalue_enabled ........... False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   eigenvalue_gas_boundary_resolution  1
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   eigenvalue_layer_num ......... 0
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   eigenvalue_max_iter .......... 100
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   eigenvalue_stability ......... 1e-06
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   eigenvalue_tol ............... 0.01
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   eigenvalue_verbose ........... False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   elasticity_enabled ........... False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   fp16_auto_cast ............... False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   fp16_enabled ................. True
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   fp16_master_weights_and_gradients  False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   global_rank .................. 0
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   grad_accum_dtype ............. None
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   gradient_accumulation_steps .. 20
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   gradient_clipping ............ 1.0
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   gradient_predivide_factor .... 1.0
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   initial_dynamic_scale ........ 65536
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   load_universal_checkpoint .... False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   loss_scale ................... 0
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   memory_breakdown ............. False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   mics_hierarchial_params_gather  False
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   mics_shard_size .............. -1
[2024-02-19 00:41:15,486] [INFO] [config.py:972:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   optimizer_legacy_fusion ...... False
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   optimizer_name ............... adamw
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   optimizer_params ............. {'lr': 1e-05, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   pld_enabled .................. False
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   pld_params ................... False
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   prescale_gradients ........... False
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   scheduler_name ............... WarmupDecayLR
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   scheduler_params ............. {'total_num_steps': 15000, 'warmup_min_lr': 0, 'warmup_max_lr': 1e-05, 'warmup_num_steps': 50, 'warmup_type': 'linear'}
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   sparse_attention ............. None
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   sparse_gradients_enabled ..... False
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   steps_per_print .............. 10
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   train_batch_size ............. 2560
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   train_micro_batch_size_per_gpu  16
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   use_node_local_storage ....... False
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   wall_clock_breakdown ......... False
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   weight_quantization_config ... None
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   world_size ................... 8
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   zero_allow_untested_optimizer  False
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   zero_enabled ................. True
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   zero_force_ds_cpu_optimizer .. True
[2024-02-19 00:41:15,487] [INFO] [config.py:972:print]   zero_optimization_stage ...... 2
[2024-02-19 00:41:15,487] [INFO] [config.py:958:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 20, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 1e-05, 
            "weight_decay": 0.0, 
            "betas": [0.9, 0.95]
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "total_num_steps": 1.500000e+04, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 1e-05, 
            "warmup_num_steps": 50, 
            "warmup_type": "linear"
        }
    }, 
    "fp16": {
        "enabled": true
    }, 
    "bf16": {
        "enabled": false
    }, 
    "gradient_clipping": 1.0, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "allgather_bucket_size": 5.000000e+08
    }
}
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Traceback (most recent call last):
  File "/workspace/ChatterBox-Finetuning/custom_train_stage1.py", line 445, in <module>
    if __name__ == "__main__":
  File "/workspace/ChatterBox-Finetuning/custom_train_stage1.py", line 303, in main
    print("==> Epoch {}/{}".format(epoch, args.epochs))
  File "/workspace/ChatterBox-Finetuning/custom_train_stage1.py", line 385, in train
    # with torch.cuda.amp.autocast(enabled=True):
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 1807, in forward
    loss = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/ChatterBox-Finetuning/model/ChatterBox_Referrring_Grounding_grounding_dino.py", line 368, in forward
    loss_dict = self.criterion_grounding(outputs, target, initial_pred_embeddings[gt_ids].unsqueeze(1))
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/ChatterBox-Finetuning/model/GroundingDINO/groundingdino.py", line 569, in forward
    inds = self.matcher(for_match, [targets[j]], label_map_list[j])
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/workspace/ChatterBox-Finetuning/model/GroundingDINO/matcher.py", line 111, in forward
    cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))
  File "/workspace/ChatterBox-Finetuning/utils/box_ops.py", line 65, in generalized_box_iou
    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()
AssertionError
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2024-02-19 00:41:20,528] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 320818
[2024-02-19 00:41:20,920] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 320819
[2024-02-19 00:41:21,913] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 320820
[2024-02-19 00:41:22,343] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 320821
[2024-02-19 00:41:22,849] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 320822
[2024-02-19 00:41:23,360] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 320823
[2024-02-19 00:41:23,871] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 320824
[2024-02-19 00:41:24,392] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 320825
[2024-02-19 00:41:24,392] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python', '-u', 'custom_train_stage1.py', '--local_rank=7', '--version', 'llava-llama-2-13b-chat-lightning-preview'] exits with return code = 1
[?2004hroot@7dada50e0e4b:/workspace/ChatterBox-Finetuning# deepspeed --include localhost:2,3,4,5,6,7,8,9 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-previewMclear[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cscreen[1Pclearrm -rf outputs/[10Pcleardeepspeed --include localhost:0 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-previewM[C[C[C[C[C[C[C[C[Cclear[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cdeepspeed --include localhost:0 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview^C[?2004l[?2004h[?2004l
[?2004hroot@7dada50e0e4b:/workspace/ChatterBox-Finetuning# clear
[?2004l[H[J[?2004hroot@7dada50e0e4b:/workspace/ChatterBox-Finetuning# cleardeepspeed --include localhost:2,3,4,5,6,7,8,9 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview
[?2004l[2024-02-19 01:41:07,450] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-02-19 01:41:09.990501: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 01:41:09.993931: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 01:41:10.031912: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 01:41:10.777513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-02-19 01:41:11,834] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-02-19 01:41:11,834] [INFO] [runner.py:570:main] cmd = /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMiwgMywgNCwgNSwgNiwgNywgOCwgOV19 --master_addr=127.0.0.1 --master_port=54906 --enable_each_rank_log=None custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview
[2024-02-19 01:41:13,473] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-02-19 01:41:15.867588: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 01:41:15.871033: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 01:41:15.908698: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 01:41:16.652809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-02-19 01:41:17,730] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8
[2024-02-19 01:41:17,730] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1
[2024-02-19 01:41:17,730] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1
[2024-02-19 01:41:17,730] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-02-19 01:41:17,730] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8
[2024-02-19 01:41:17,730] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-02-19 01:41:17,730] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1
[2024-02-19 01:41:17,730] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2, 3, 4, 5, 6, 7, 8, 9]}
[2024-02-19 01:41:17,730] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-02-19 01:41:17,730] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-02-19 01:41:17,730] [INFO] [launch.py:163:main] dist_world_size=8
[2024-02-19 01:41:17,730] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2,3,4,5,6,7,8,9
[2024-02-19 01:41:19,470] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 01:41:19,570] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 01:41:19,572] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 01:41:19,613] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 01:41:19,625] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 01:41:19,640] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 01:41:19,701] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-19 01:41:19,756] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-02-19 01:41:22.060438: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 01:41:22.066303: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 01:41:22.161222: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 01:41:22.681152: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 01:41:22.681151: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 01:41:22.684739: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 01:41:22.684740: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 01:41:22.722026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 01:41:22.722026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 01:41:22.772135: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 01:41:22.776603: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 01:41:22.800448: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 01:41:22.800445: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 01:41:22.804909: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 01:41:22.804909: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 01:41:22.823810: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 01:41:22.851282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 01:41:22.851281: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 01:41:22.890506: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 01:41:22.890505: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 01:41:22.895165: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 01:41:22.895163: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-19 01:41:22.942371: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 01:41:22.942373: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-19 01:41:23.085031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 01:41:23.526580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 01:41:23.557155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 01:41:23.758350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 01:41:23.774603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 01:41:23.785006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 01:41:23.819160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-19 01:41:23.831205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.38s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.22s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.28s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.08s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.33s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.35s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.21s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.01s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.12s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.04s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.02s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:09<00:04,  4.95s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.04s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.09s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.01s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:09<00:04,  4.91s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.24s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.50s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.20s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.44s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.15s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.41s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.16s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.42s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.13s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.36s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.20s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.47s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.16s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.41s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.08s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.31s/it]
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-19 01:43:20,906] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 01:43:20,906] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 01:43:21,011] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 01:43:21,011] [INFO] [comm.py:637:init_distributed] cdb=None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-19 01:43:22,437] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 01:43:22,438] [INFO] [comm.py:637:init_distributed] cdb=None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-19 01:43:22,874] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 01:43:22,875] [INFO] [comm.py:637:init_distributed] cdb=None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-19 01:43:22,971] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 01:43:22,971] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 01:43:22,971] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-19 01:43:23,036] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 01:43:23,037] [INFO] [comm.py:637:init_distributed] cdb=None
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-19 01:43:23,130] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 01:43:23,132] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 01:43:23,274] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-19 01:43:23,275] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-19 01:43:43,387] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combinationUsing /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...

Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.0847480297088623 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10208344459533691 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...Loading extension module fused_adam...

Loading extension module fused_adam...
Time to load fused_adam op: 0.10134005546569824 seconds
Time to load fused_adam op: 0.10191082954406738 seconds
Time to load fused_adam op: 0.10158181190490723 seconds
Time to load fused_adam op: 0.10191702842712402 seconds
Time to load fused_adam op: 0.10185742378234863 seconds
Time to load fused_adam op: 0.10211849212646484 seconds
[2024-02-19 01:43:43,826] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-02-19 01:43:43,826] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-02-19 01:43:44,113] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-02-19 01:43:44,113] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-02-19 01:43:44,113] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-02-19 01:43:44,113] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 500000000
[2024-02-19 01:43:44,113] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 500000000
[2024-02-19 01:43:44,113] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: False
[2024-02-19 01:43:44,113] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
[2024-02-19 01:43:52,461] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-02-19 01:43:52,462] [INFO] [utils.py:803:see_memory_usage] MA 26.25 GB         Max_MA 26.43 GB         CA 26.61 GB         Max_CA 27 GB 
[2024-02-19 01:43:52,462] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 48.48 GB, percent = 9.6%
[2024-02-19 01:43:52,756] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-02-19 01:43:52,757] [INFO] [utils.py:803:see_memory_usage] MA 26.99 GB         Max_MA 27.36 GB         CA 27.72 GB         Max_CA 28 GB 
[2024-02-19 01:43:52,757] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 49.34 GB, percent = 9.8%
[2024-02-19 01:43:52,758] [INFO] [stage_1_and_2.py:513:__init__] optimizer state initialized
[2024-02-19 01:43:52,992] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-02-19 01:43:52,994] [INFO] [utils.py:803:see_memory_usage] MA 26.99 GB         Max_MA 26.99 GB         CA 27.72 GB         Max_CA 28 GB 
[2024-02-19 01:43:52,994] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 49.36 GB, percent = 9.8%
[2024-02-19 01:43:52,998] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-02-19 01:43:52,998] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-02-19 01:43:52,998] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7efe04117fa0>
[2024-02-19 01:43:52,998] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2024-02-19 01:43:53,002] [INFO] [config.py:968:print] DeepSpeedEngine configuration:
[2024-02-19 01:43:53,003] [INFO] [config.py:972:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-02-19 01:43:53,003] [INFO] [config.py:972:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-02-19 01:43:53,003] [INFO] [config.py:972:print]   amp_enabled .................. False
[2024-02-19 01:43:53,003] [INFO] [config.py:972:print]   amp_params ................... False
[2024-02-19 01:43:53,003] [INFO] [config.py:972:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-02-19 01:43:53,003] [INFO] [config.py:972:print]   bfloat16_enabled ............. False
[2024-02-19 01:43:53,003] [INFO] [config.py:972:print]   checkpoint_parallel_write_pipeline  False
[2024-02-19 01:43:53,003] [INFO] [config.py:972:print]   checkpoint_tag_validation_enabled  True
[2024-02-19 01:43:53,003] [INFO] [config.py:972:print]   checkpoint_tag_validation_fail  False
[2024-02-19 01:43:53,003] [INFO] [config.py:972:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efde973fe20>
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   communication_data_type ...... None
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   curriculum_enabled_legacy .... False
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   curriculum_params_legacy ..... False
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   data_efficiency_enabled ...... False
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   dataloader_drop_last ......... False
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   disable_allgather ............ False
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   dump_state ................... False
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   dynamic_loss_scale_args ...... None
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   eigenvalue_enabled ........... False
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   eigenvalue_gas_boundary_resolution  1
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   eigenvalue_layer_num ......... 0
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   eigenvalue_max_iter .......... 100
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   eigenvalue_stability ......... 1e-06
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   eigenvalue_tol ............... 0.01
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   eigenvalue_verbose ........... False
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   elasticity_enabled ........... False
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-02-19 01:43:53,004] [INFO] [config.py:972:print]   fp16_auto_cast ............... False
[2024-02-19 01:43:53,005] [INFO] [config.py:972:print]   fp16_enabled ................. True
[2024-02-19 01:43:53,005] [INFO] [config.py:972:print]   fp16_master_weights_and_gradients  False
[2024-02-19 01:43:53,005] [INFO] [config.py:972:print]   global_rank .................. 0
[2024-02-19 01:43:53,005] [INFO] [config.py:972:print]   grad_accum_dtype ............. None
[2024-02-19 01:43:53,005] [INFO] [config.py:972:print]   gradient_accumulation_steps .. 20
[2024-02-19 01:43:53,005] [INFO] [config.py:972:print]   gradient_clipping ............ 1.0
[2024-02-19 01:43:53,005] [INFO] [config.py:972:print]   gradient_predivide_factor .... 1.0
[2024-02-19 01:43:53,005] [INFO] [config.py:972:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-02-19 01:43:53,005] [INFO] [config.py:972:print]   initial_dynamic_scale ........ 65536
[2024-02-19 01:43:53,005] [INFO] [config.py:972:print]   load_universal_checkpoint .... False
[2024-02-19 01:43:53,005] [INFO] [config.py:972:print]   loss_scale ................... 0
[2024-02-19 01:43:53,005] [INFO] [config.py:972:print]   memory_breakdown ............. False
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   mics_hierarchial_params_gather  False
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   mics_shard_size .............. -1
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   optimizer_legacy_fusion ...... False
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   optimizer_name ............... adamw
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   optimizer_params ............. {'lr': 1e-05, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   pld_enabled .................. False
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   pld_params ................... False
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   prescale_gradients ........... False
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   scheduler_name ............... WarmupDecayLR
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   scheduler_params ............. {'total_num_steps': 15000, 'warmup_min_lr': 0, 'warmup_max_lr': 1e-05, 'warmup_num_steps': 50, 'warmup_type': 'linear'}
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   sparse_attention ............. None
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   sparse_gradients_enabled ..... False
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   steps_per_print .............. 10
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   train_batch_size ............. 2560
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   train_micro_batch_size_per_gpu  16
[2024-02-19 01:43:53,006] [INFO] [config.py:972:print]   use_node_local_storage ....... False
[2024-02-19 01:43:53,007] [INFO] [config.py:972:print]   wall_clock_breakdown ......... False
[2024-02-19 01:43:53,007] [INFO] [config.py:972:print]   weight_quantization_config ... None
[2024-02-19 01:43:53,007] [INFO] [config.py:972:print]   world_size ................... 8
[2024-02-19 01:43:53,007] [INFO] [config.py:972:print]   zero_allow_untested_optimizer  False
[2024-02-19 01:43:53,007] [INFO] [config.py:972:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-02-19 01:43:53,007] [INFO] [config.py:972:print]   zero_enabled ................. True
[2024-02-19 01:43:53,007] [INFO] [config.py:972:print]   zero_force_ds_cpu_optimizer .. True
[2024-02-19 01:43:53,007] [INFO] [config.py:972:print]   zero_optimization_stage ...... 2
[2024-02-19 01:43:53,007] [INFO] [config.py:958:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 20, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 1e-05, 
            "weight_decay": 0.0, 
            "betas": [0.9, 0.95]
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "total_num_steps": 1.500000e+04, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 1e-05, 
            "warmup_num_steps": 50, 
            "warmup_type": "linear"
        }
    }, 
    "fp16": {
        "enabled": true
    }, 
    "bf16": {
        "enabled": false
    }, 
    "gradient_clipping": 1.0, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "allgather_bucket_size": 5.000000e+08
    }
}
==> Epoch 0/30
==> Epoch 0/30
==> Epoch 0/30
==> Epoch 0/30
==> Epoch 0/30
==> Epoch 0/30
==> Epoch 0/30
==> Epoch 0/30
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2024-02-19 01:47:30,780] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
Epoch: [0][  1/500]     Time 217.477 (208.455)  Loss 27.6724 (20.8174)  VQALoss 0.0000 (0.0000) VGLoss 27.6724 (20.8174)
[2024-02-19 01:47:30,787] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1 is about to be saved!
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-19 01:47:42,757] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step1/mp_rank_00_model_states.pt
[2024-02-19 01:47:42,757] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/mp_rank_00_model_states.pt...
[2024-02-19 01:49:58,671] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/mp_rank_00_model_states.pt.
[2024-02-19 01:49:59,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-19 01:49:59,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-19 01:49:59,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-19 01:49:59,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-19 01:49:59,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-19 01:49:59,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-19 01:49:59,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-19 01:49:59,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-19 01:50:04,071] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-19 01:50:04,072] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step1/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-19 01:50:04,072] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2024-02-19 01:50:04,077] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-19 01:50:04,077] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step1/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-19 01:50:04,077] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2024-02-19 01:50:04,088] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-19 01:50:04,088] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step1/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-19 01:50:04,088] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2024-02-19 01:50:04,099] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-19 01:50:04,100] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step1/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-19 01:50:04,100] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2024-02-19 01:50:04,221] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-19 01:50:04,222] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step1/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-19 01:50:04,222] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2024-02-19 01:50:04,266] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-19 01:50:04,275] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-19 01:50:04,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2024-02-19 01:53:13,330] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-19 01:53:13,330] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step1/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-19 01:53:13,330] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2024-02-19 01:53:14,852] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-19 01:53:14,852] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step1/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-19 01:53:14,852] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2024-02-19 01:55:40,606] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
curr_lr =  [0.0]
curr_lr =  [0.0]
curr_lr =  [0.0]
curr_lr =  [0.0]
curr_lr =  [0.0]
curr_lr =  [0.0]
curr_lr =  [0.0]
curr_lr =  [0.0]
curr_lr =  [2.0000000000000002e-07]
curr_lr =  [2.0000000000000002e-07]
curr_lr =  [2.0000000000000002e-07]
curr_lr =  [2.0000000000000002e-07]
curr_lr =  [2.0000000000000002e-07]
curr_lr =  [2.0000000000000002e-07]
[2024-02-19 01:58:07,213] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
curr_lr =  [2.0000000000000002e-07]
curr_lr =  [2.0000000000000002e-07]
[2024-02-19 02:00:32,818] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
curr_lr =  [4.0000000000000003e-07]
curr_lr =  [4.0000000000000003e-07]
curr_lr = curr_lr =   [4.0000000000000003e-07][4.0000000000000003e-07]

curr_lr =  [4.0000000000000003e-07]
curr_lr =  [4.0000000000000003e-07]
curr_lr =  [4.0000000000000003e-07]
curr_lr =  [4.0000000000000003e-07]
curr_lr = curr_lr =   [6.000000000000001e-07]
[6.000000000000001e-07]
curr_lr =  curr_lr = [6.000000000000001e-07] 
[6.000000000000001e-07]
curr_lr =  [6.000000000000001e-07]
curr_lr =  [6.000000000000001e-07]
curr_lr =  [6.000000000000001e-07]
[2024-02-19 02:02:59,600] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
curr_lr =  [6.000000000000001e-07]
[2024-02-19 02:05:26,537] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
curr_lr = curr_lr = curr_lr =  curr_lr =   Epoch: [0][  6/500]  Time 146.935 (215.151)  Loss 18.3009 (20.5803)  VQALoss 0.0000 (0.0000) VGLoss 18.3009 (20.5803) [8.000000000000001e-07]

[8.000000000000001e-07][8.000000000000001e-07]curr_lr = 
[8.000000000000001e-07]

curr_lr =  curr_lr =  [8.000000000000001e-07][8.000000000000001e-07]

 [8.000000000000001e-07]
curr_lr =  [8.000000000000001e-07]
curr_lr =  [1.0000000000000002e-06]
[2024-02-19 02:07:51,860] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
curr_lr =  curr_lr = [1.0000000000000002e-06]
 [1.0000000000000002e-06]
curr_lr =  [1.0000000000000002e-06]
curr_lr =  [1.0000000000000002e-06]
curr_lr =  [1.0000000000000002e-06]
curr_lr =  [1.0000000000000002e-06]
curr_lr =  [1.0000000000000002e-06]
curr_lr =  [1.2000000000000002e-06]
curr_lr =  [1.2000000000000002e-06]
[2024-02-19 02:10:18,463] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
curr_lr = curr_lr =   [1.2000000000000002e-06]
[1.2000000000000002e-06]
curr_lr = curr_lr =   [1.2000000000000002e-06][1.2000000000000002e-06]

curr_lr =  [1.2000000000000002e-06]
curr_lr =  [1.2000000000000002e-06]
curr_lr =  [2024-02-19 02:12:45,458] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608
[1.4000000000000001e-06]
curr_lr =  [1.4000000000000001e-06]
curr_lr =  [1.4000000000000001e-06]
curr_lr =  [1.4000000000000001e-06]
curr_lr =  [1.4000000000000001e-06]
curr_lr = curr_lr =   [1.4000000000000001e-06][1.4000000000000001e-06]

curr_lr =  [1.4000000000000001e-06]
curr_lr =  [1.6000000000000001e-06]
curr_lr =  curr_lr = [2024-02-19 02:15:12,470] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304
 [1.6000000000000001e-06]
[1.6000000000000001e-06]
curr_lr =  [1.6000000000000001e-06]
curr_lr =  [1.6000000000000001e-06]
curr_lr =  [1.6000000000000001e-06]
[2024-02-19 02:15:12,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[1.4000000000000001e-06], mom=[(0.9, 0.95)]
curr_lr =  [1.6000000000000001e-06]
[2024-02-19 02:15:12,472] [INFO] [timer.py:260:stop] epoch=0/micro_step=200/global_step=10, RunningAvgSamplesPerSec=17.48528303155135, CurrSamplesPerSec=17.41361876405232, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [1.6000000000000001e-06]
[2024-02-19 02:17:40,298] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152
Epoch: [0][ 11/500]     Time 147.827 (146.752)  Loss 16.7349 (21.0075)  VQALoss 0.0000 (0.0000) VGLoss 16.7349 (21.0075)
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =     curr_lr =   [1.8000000000000001e-06][1.8000000000000001e-06][1.8000000000000001e-06][1.8000000000000001e-06][1.8000000000000001e-06]



curr_lr = 
[1.8000000000000001e-06]
 [1.8000000000000001e-06]
curr_lr =  [1.8000000000000001e-06]
[2024-02-19 02:20:45,463] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
curr_lr =  [2.0000000000000003e-06]
curr_lr = curr_lr =   [2.0000000000000003e-06][2.0000000000000003e-06]

curr_lr =  [2.0000000000000003e-06]
curr_lr =  [2.0000000000000003e-06]
curr_lr =  [2.0000000000000003e-06]
curr_lr =  [2.0000000000000003e-06]
curr_lr =  [2.0000000000000003e-06]
[2024-02-19 02:24:47,921] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288
curr_lr =  [2.2e-06]
curr_lr =  curr_lr =  [2.2e-06]
[2.2e-06]
curr_lr =  curr_lr = [2.2e-06]
 [2.2e-06]
curr_lr =  [2.2e-06]
curr_lr =  [2.2e-06]
curr_lr =  [2.2e-06]
[2024-02-19 02:28:31,888] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144
curr_lr =  [2.4000000000000003e-06]
curr_lr =  [2.4000000000000003e-06]
curr_lr = curr_lr =   [2.4000000000000003e-06][2.4000000000000003e-06]

curr_lr =  [2.4000000000000003e-06]
curr_lr =  [2.4000000000000003e-06]
curr_lr =  [2.4000000000000003e-06]
curr_lr =  [2.4000000000000003e-06]
[2024-02-19 02:31:00,687] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
curr_lr =  [2.6e-06]
curr_lr = curr_lr =   [2.6e-06]
[2.6e-06]
curr_lr =  [2.6e-06]
curr_lr =  [2.6e-06]
curr_lr =  [2.6e-06]
curr_lr =  [2.6e-06]
curr_lr =  [2.6e-06]
[2024-02-19 02:33:30,205] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =     Epoch: [0][ 16/500]     Time 149.518 (189.981)  Loss 17.4251 (20.7180)  VQALoss 0.0000 (0.0000) VGLoss 17.4251 (20.7180) [2.8000000000000003e-06] curr_lr = [2.8000000000000003e-06]

[2.8000000000000003e-06][2.8000000000000003e-06]
[2.8000000000000003e-06] 

[2.8000000000000003e-06]

[2.8000000000000003e-06]
curr_lr =  [2.8000000000000003e-06]
curr_lr =  [3e-06]
[2024-02-19 02:35:58,406] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
curr_lr =  curr_lr = [3e-06] 
[3e-06]
curr_lr =  [3e-06]
curr_lr =  curr_lr = [3e-06]
 [3e-06]
curr_lr =  [3e-06]
curr_lr =  [3e-06]
[2024-02-19 02:38:27,164] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
curr_lr =  [3.2000000000000003e-06]
curr_lr = curr_lr = curr_lr =    [3.2000000000000003e-06]
[3.2000000000000003e-06]
curr_lr = [3.2000000000000003e-06]
 [3.2000000000000003e-06]
curr_lr = curr_lr =   [3.2000000000000003e-06]
[3.2000000000000003e-06]
curr_lr =  [3.2000000000000003e-06]
[2024-02-19 02:40:56,332] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
curr_lr =  [3.4000000000000005e-06]
curr_lr =  [3.4000000000000005e-06]
curr_lr =  [3.4000000000000005e-06]
curr_lr =  [3.4000000000000005e-06]
curr_lr =  curr_lr = [3.4000000000000005e-06]
 [3.4000000000000005e-06]
curr_lr =  [3.4000000000000005e-06]
curr_lr =  [3.4000000000000005e-06]
curr_lr = curr_lr =   [3.6000000000000003e-06]
[3.6000000000000003e-06]
[2024-02-19 02:43:25,022] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
curr_lr =  [3.6000000000000003e-06]
curr_lr =  [3.6000000000000003e-06]
curr_lr =  [3.6000000000000003e-06]
curr_lr =  [3.6000000000000003e-06]curr_lr = 
 [3.6000000000000003e-06]
[2024-02-19 02:43:25,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=20, lr=[3.4000000000000005e-06], mom=[(0.9, 0.95)]
[2024-02-19 02:43:25,024] [INFO] [timer.py:260:stop] epoch=0/micro_step=400/global_step=20, RunningAvgSamplesPerSec=16.095803348091867, CurrSamplesPerSec=17.217044667602632, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [3.6000000000000003e-06]
[2024-02-19 02:45:52,179] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
Epoch: [0][ 21/500]     Time 147.156 (148.395)  Loss 31.0920 (21.0223)  VQALoss 0.0000 (0.0000) VGLoss 31.0920 (21.0223)
curr_lr = curr_lr =  curr_lr = curr_lr = curr_lr =   [3.8000000000000005e-06]
  curr_lr = [3.8000000000000005e-06][3.8000000000000005e-06]

[3.8000000000000005e-06] [3.8000000000000005e-06]

[3.8000000000000005e-06]curr_lr = 
 [3.8000000000000005e-06]
curr_lr =  [3.8000000000000005e-06]
curr_lr = [2024-02-19 02:48:34,211] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
 [4.000000000000001e-06]
curr_lr =  curr_lr = [4.000000000000001e-06]
 [4.000000000000001e-06]
curr_lr =  curr_lr = [4.000000000000001e-06]
 [4.000000000000001e-06]
curr_lr =  [4.000000000000001e-06]
curr_lr =  [4.000000000000001e-06]
curr_lr =  [4.000000000000001e-06]
curr_lr =  [4.4e-06]
curr_lr = curr_lr =   [4.4e-06][4.4e-06]

curr_lr =  [4.4e-06]curr_lr = 
 [4.4e-06]
curr_lr =  [4.4e-06]
curr_lr =  [4.4e-06]
curr_lr =  [4.4e-06]
curr_lr =  [4.800000000000001e-06]
curr_lr =  [4.800000000000001e-06]
curr_lr =  [4.800000000000001e-06]
curr_lr =  [4.800000000000001e-06]
curr_lr =  [4.800000000000001e-06]
curr_lr =  [4.800000000000001e-06]
curr_lr =  [4.800000000000001e-06]
curr_lr =  [4.800000000000001e-06]
curr_lr =  [5.2e-06]
curr_lr = curr_lr =   [5.2e-06]
[5.2e-06]
curr_lr =  [5.2e-06]
curr_lr =  curr_lr = [5.2e-06]
 [5.2e-06]
curr_lr =  [5.2e-06]
curr_lr =  [5.2e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    curr_lr = curr_lr =   Epoch: [0][ 26/500]  Time 148.995 (238.841)  Loss 16.1575 (19.8463)  VQALoss 0.0000 (0.0000) VGLoss 16.1575 (19.8463) [5.600000000000001e-06][5.600000000000001e-06]
[5.600000000000001e-06]
[5.600000000000001e-06]
[5.600000000000001e-06] 

[5.600000000000001e-06]

[5.600000000000001e-06]
curr_lr =  [5.600000000000001e-06]
curr_lr = curr_lr =   curr_lr = [6e-06]
 [6e-06]
curr_lr = [6e-06]
 [6e-06]
curr_lr = curr_lr =   [6e-06][6e-06]

curr_lr =  [6e-06]
curr_lr =  [6e-06]
curr_lr =  [6.4000000000000006e-06]
curr_lr = curr_lr =   curr_lr =  [6.4000000000000006e-06][6.4000000000000006e-06]

[6.4000000000000006e-06]
curr_lr = curr_lr =   [6.4000000000000006e-06][6.4000000000000006e-06]

curr_lr =  curr_lr = [6.4000000000000006e-06]
 [6.4000000000000006e-06]
curr_lr =  curr_lr = curr_lr = [6.800000000000001e-06]
  [6.800000000000001e-06][6.800000000000001e-06]

curr_lr =  [6.800000000000001e-06]
curr_lr =  curr_lr =  [6.800000000000001e-06]
[6.800000000000001e-06]
curr_lr =  [6.800000000000001e-06]
curr_lr =  [6.800000000000001e-06]
[2024-02-19 03:15:40,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=22, lr=[7e-06], mom=[(0.9, 0.95)]
curr_lr =  [7.2000000000000005e-06]
curr_lr = curr_lr =   [7.2000000000000005e-06]
[7.2000000000000005e-06]curr_lr = 
 [7.2000000000000005e-06]curr_lr = curr_lr = 
  [7.2000000000000005e-06][7.2000000000000005e-06]

curr_lr =  [7.2000000000000005e-06]
[2024-02-19 03:15:40,397] [INFO] [timer.py:260:stop] epoch=0/micro_step=600/global_step=30, RunningAvgSamplesPerSec=14.941912325601281, CurrSamplesPerSec=17.18401600324748, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [7.2000000000000005e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =        Epoch: [0][ 31/500]        Time 147.857 (148.374)  Loss 15.3959 (17.5450)  VQALoss 0.0000 (0.0000) VGLoss 15.3959 (17.5450)
[7.600000000000001e-06]
[7.600000000000001e-06][7.600000000000001e-06]
[7.600000000000001e-06][7.600000000000001e-06][7.600000000000001e-06]



[7.600000000000001e-06]
curr_lr =  [7.600000000000001e-06]
curr_lr = curr_lr =  curr_lr =   [8.000000000000001e-06][8.000000000000001e-06]curr_lr = 

[8.000000000000001e-06]
 [8.000000000000001e-06]
curr_lr =  [8.000000000000001e-06]
curr_lr =  [8.000000000000001e-06]
curr_lr =  [8.000000000000001e-06]
curr_lr =  [8.000000000000001e-06]
curr_lr = curr_lr = curr_lr =    curr_lr =  [8.400000000000001e-06][8.400000000000001e-06]

[8.400000000000001e-06][8.400000000000001e-06]

curr_lr =  [8.400000000000001e-06]
curr_lr =  [8.400000000000001e-06]
curr_lr =  [8.400000000000001e-06]
curr_lr =  [8.400000000000001e-06]
curr_lr = curr_lr =   curr_lr = [8.8e-06][8.8e-06]
 
curr_lr = [8.8e-06]
 [8.8e-06]
curr_lr =  [8.8e-06]
curr_lr =  curr_lr = [8.8e-06]
 [8.8e-06]
curr_lr =  [8.8e-06]
curr_lr = curr_lr = curr_lr =    [9.200000000000002e-06]
[9.200000000000002e-06][9.200000000000002e-06]

curr_lr = curr_lr =   [9.200000000000002e-06]
[9.200000000000002e-06]
curr_lr =  [9.200000000000002e-06]
curr_lr =  [9.200000000000002e-06]
curr_lr =  [9.200000000000002e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =     curr_lr = Epoch: [0][ 36/500]   Time 148.618 (148.240)  Loss 27.0109 (15.8643)  VQALoss 0.0000 (0.0000) VGLoss 27.0109 (15.8643)   [9.600000000000001e-06][9.600000000000001e-06][9.600000000000001e-06]
[9.600000000000001e-06]


[9.600000000000001e-06][9.600000000000001e-06]
[9.600000000000001e-06]


curr_lr =  [9.600000000000001e-06]
curr_lr =  curr_lr = curr_lr = [1e-05]
  [1e-05][1e-05]

curr_lr = curr_lr =   [1e-05]curr_lr = [1e-05]

 [1e-05]
curr_lr =  [1e-05]
curr_lr =  [1e-05]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.998662207357859e-06][9.998662207357859e-06]

  [9.998662207357859e-06]
[9.998662207357859e-06]
curr_lr =  [9.998662207357859e-06]
curr_lr =  [9.998662207357859e-06]
curr_lr =  [9.998662207357859e-06]
curr_lr =  [9.998662207357859e-06]
curr_lr =  curr_lr =  curr_lr = [9.99732441471572e-06] 
[9.99732441471572e-06]
[9.99732441471572e-06]
curr_lr =  [9.99732441471572e-06]
curr_lr =  curr_lr = [9.99732441471572e-06]
 [9.99732441471572e-06]
curr_lr =  [9.99732441471572e-06]
curr_lr =  [9.99732441471572e-06]
[2024-02-19 03:41:21,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=22, lr=[9.99665551839465e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = [9.995986622073579e-06]
 curr_lr =  [9.995986622073579e-06]
[9.995986622073579e-06]
curr_lr = curr_lr =   [9.995986622073579e-06]
curr_lr =  [9.995986622073579e-06]
[9.995986622073579e-06]
curr_lr =  [9.995986622073579e-06]
[2024-02-19 03:41:21,703] [INFO] [timer.py:260:stop] epoch=0/micro_step=800/global_step=40, RunningAvgSamplesPerSec=15.349594111050456, CurrSamplesPerSec=12.272072395462414, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.995986622073579e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = Epoch: [0][ 41/500]       Time 229.567 (176.363)  Loss 11.6857 (15.0603)  VQALoss 0.0000 (0.0000) VGLoss 11.6857 (15.0603)curr_lr =   
 [9.994648829431439e-06] [9.994648829431439e-06]

[9.994648829431439e-06][9.994648829431439e-06]

[9.994648829431439e-06][9.994648829431439e-06]

 [9.994648829431439e-06]
curr_lr =  [9.994648829431439e-06]
curr_lr =  curr_lr =  [9.993311036789299e-06]
curr_lr = [9.993311036789299e-06]
 curr_lr = [9.993311036789299e-06]
 [9.993311036789299e-06]
curr_lr =  [9.993311036789299e-06]
curr_lr =  [9.993311036789299e-06]
curr_lr =  [9.993311036789299e-06]
curr_lr =  [9.993311036789299e-06]
curr_lr =  [9.991973244147159e-06]
curr_lr =  curr_lr =  curr_lr = [9.991973244147159e-06][9.991973244147159e-06]
 
[9.991973244147159e-06]
curr_lr =  [9.991973244147159e-06]
curr_lr =  [9.991973244147159e-06]
curr_lr =  [9.991973244147159e-06]
curr_lr =  [9.991973244147159e-06]
curr_lr = curr_lr = curr_lr =    [9.990635451505017e-06][9.990635451505017e-06]
[9.990635451505017e-06]

curr_lr =  [9.990635451505017e-06]
curr_lr =  [9.990635451505017e-06]
curr_lr =  [9.990635451505017e-06]
curr_lr =  [9.990635451505017e-06]curr_lr = 
 [9.990635451505017e-06]
curr_lr =  [9.989297658862878e-06]
curr_lr = curr_lr =   [9.989297658862878e-06]
[9.989297658862878e-06]
curr_lr = curr_lr =   [9.989297658862878e-06]
[9.989297658862878e-06]
curr_lr =  [9.989297658862878e-06]
curr_lr =  [9.989297658862878e-06]
curr_lr =  [9.989297658862878e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr =      [9.987959866220737e-06]
[9.987959866220737e-06]
[9.987959866220737e-06][9.987959866220737e-06]Epoch: [0][ 46/500]       Time 148.104 (148.212)  Loss 12.7107 (14.3821)  VQALoss 0.0000 (0.0000) VGLoss 12.7107 (14.3821)[9.987959866220737e-06]


[9.987959866220737e-06][9.987959866220737e-06]


curr_lr =  [9.987959866220737e-06]
curr_lr =  curr_lr =  [9.986622073578597e-06]
curr_lr = [9.986622073578597e-06] 
[9.986622073578597e-06]
curr_lr = curr_lr =   [9.986622073578597e-06][9.986622073578597e-06]

curr_lr =  curr_lr = [9.986622073578597e-06]
 [9.986622073578597e-06]
curr_lr =  [9.986622073578597e-06]
curr_lr =  curr_lr =  [9.985284280936456e-06]curr_lr = 
 [9.985284280936456e-06]
curr_lr = [9.985284280936456e-06]
 [9.985284280936456e-06]
curr_lr =  curr_lr = [9.985284280936456e-06] 
[9.985284280936456e-06]
curr_lr =  [9.985284280936456e-06]
curr_lr =  [9.985284280936456e-06]
curr_lr =  curr_lr =  [9.983946488294315e-06]
[9.983946488294315e-06]
curr_lr =  [9.983946488294315e-06]
curr_lr =  curr_lr =  [9.983946488294315e-06]
[9.983946488294315e-06]curr_lr = 
 [9.983946488294315e-06]
curr_lr =  [9.983946488294315e-06]
curr_lr =  [9.983946488294315e-06]
[2024-02-19 04:10:02,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=22, lr=[9.983277591973245e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =   [9.982608695652175e-06][9.982608695652175e-06]
curr_lr = 
curr_lr =   [9.982608695652175e-06]
[9.982608695652175e-06]
curr_lr = curr_lr =   [9.982608695652175e-06]
[9.982608695652175e-06]
curr_lr =  [9.982608695652175e-06]
[2024-02-19 04:10:02,530] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=50, RunningAvgSamplesPerSec=15.25027578836547, CurrSamplesPerSec=17.27533008749438, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.982608695652175e-06]
curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr =  curr_lr =   [9.981270903010034e-06][9.981270903010034e-06] 

[9.981270903010034e-06] 
[9.981270903010034e-06]
[9.981270903010034e-06]
[9.981270903010034e-06][9.981270903010034e-06]

Epoch: [0][ 51/500]     Time 148.153 (179.670)  Loss 11.1575 (14.0690)  VQALoss 0.0000 (0.0000) VGLoss 11.1575 (14.0690)
curr_lr =  [9.981270903010034e-06]
[2024-02-19 04:12:34,845] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step51 is about to be saved!
[2024-02-19 04:12:46,902] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step51/mp_rank_00_model_states.pt
[2024-02-19 04:12:46,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step51/mp_rank_00_model_states.pt...
[2024-02-19 04:22:19,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step51/mp_rank_00_model_states.pt.
[2024-02-19 04:22:20,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step51/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-19 04:22:20,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step51/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-19 04:22:20,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step51/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-19 04:22:20,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step51/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-19 04:22:20,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step51/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-19 04:22:20,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step51/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-19 04:22:20,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step51/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-19 04:22:20,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step51/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-19 04:22:24,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step51/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-19 04:22:24,535] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step51/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-19 04:22:24,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step51 is ready now!
[2024-02-19 04:22:24,554] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step51/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-19 04:22:24,555] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step51/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-19 04:22:24,555] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step51 is ready now!
[2024-02-19 04:22:24,631] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step51/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-19 04:22:24,631] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step51/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-19 04:22:24,631] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step51 is ready now!
[2024-02-19 04:22:24,660] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step51/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-19 04:22:24,660] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step51/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-19 04:22:24,660] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step51 is ready now!
[2024-02-19 04:22:24,712] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step51/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-19 04:22:24,713] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step51/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-19 04:22:24,713] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step51 is ready now!
[2024-02-19 04:22:24,823] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step51/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-19 04:22:24,824] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step51/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-19 04:22:24,824] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step51 is ready now!
[2024-02-19 04:22:24,858] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step51/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-19 04:22:24,860] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step51/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-19 04:22:24,860] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step51/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-19 04:22:24,860] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step51 is ready now!
[2024-02-19 04:22:24,866] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step51/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-19 04:22:24,866] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step51 is ready now!
curr_lr = curr_lr =   [9.979933110367894e-06]curr_lr = 
[9.979933110367894e-06]
curr_lr =   [9.979933110367894e-06]
[9.979933110367894e-06]
curr_lr =  [9.979933110367894e-06]
curr_lr =  [9.979933110367894e-06]
curr_lr =  [9.979933110367894e-06]
curr_lr =  [9.979933110367894e-06]
curr_lr = curr_lr =   [9.978595317725753e-06]curr_lr = curr_lr = [9.978595317725753e-06]

  [9.978595317725753e-06]
[9.978595317725753e-06]
curr_lr =  curr_lr =  [9.978595317725753e-06]
[9.978595317725753e-06]
curr_lr =  [9.978595317725753e-06]
curr_lr =  [9.978595317725753e-06]
curr_lr = curr_lr = curr_lr =    [9.977257525083612e-06]
[9.977257525083612e-06][9.977257525083612e-06]

curr_lr =  [9.977257525083612e-06]
curr_lr = curr_lr = curr_lr =    [9.977257525083612e-06]
[9.977257525083612e-06]
[9.977257525083612e-06]
curr_lr =  [9.977257525083612e-06]
curr_lr = curr_lr = curr_lr =    [9.975919732441472e-06]
[9.975919732441472e-06]
[9.975919732441472e-06]
curr_lr =  [9.975919732441472e-06]
curr_lr = curr_lr =   [9.975919732441472e-06][9.975919732441472e-06]

curr_lr =  [9.975919732441472e-06]
curr_lr =  [9.975919732441472e-06]
curr_lr = curr_lr =  curr_lr = curr_lr =  curr_lr =  curr_lr =  [9.974581939799332e-06]curr_lr = 
 [9.974581939799332e-06]Epoch: [0][ 56/500]     Time 148.107 (267.092)  Loss 16.3954 (13.2702)  VQALoss 0.0000 (0.0000) VGLoss 16.3954 (13.2702) 
[9.974581939799332e-06] [9.974581939799332e-06]

[9.974581939799332e-06]

[9.974581939799332e-06]
[9.974581939799332e-06]
curr_lr =  [9.974581939799332e-06]
curr_lr =  curr_lr =  [9.973244147157192e-06]curr_lr = 
 [9.973244147157192e-06]
[9.973244147157192e-06]
curr_lr =  curr_lr =  [9.973244147157192e-06]
[9.973244147157192e-06]
curr_lr =  [9.973244147157192e-06]
curr_lr =  [9.973244147157192e-06]
curr_lr =  [9.973244147157192e-06]
curr_lr =  [9.97190635451505e-06]
curr_lr =  [9.97190635451505e-06]
curr_lr =  [9.97190635451505e-06]
curr_lr =  [9.97190635451505e-06]
curr_lr = curr_lr =   [9.97190635451505e-06]
[9.97190635451505e-06]
curr_lr =  [9.97190635451505e-06]
curr_lr =  [9.97190635451505e-06]
curr_lr =  curr_lr = [9.97056856187291e-06]
curr_lr =   [9.97056856187291e-06]
[9.97056856187291e-06]
curr_lr =  curr_lr = [9.97056856187291e-06]
 curr_lr = [9.97056856187291e-06] 
[9.97056856187291e-06]
curr_lr =  [9.97056856187291e-06]
curr_lr =  [9.97056856187291e-06]
[2024-02-19 04:44:38,945] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=22, lr=[9.96989966555184e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = curr_lr = [9.96923076923077e-06] curr_lr = 
  [9.96923076923077e-06][9.96923076923077e-06]

[9.96923076923077e-06]
curr_lr =  curr_lr = [9.96923076923077e-06] 
[9.96923076923077e-06]
curr_lr =  [9.96923076923077e-06]
[2024-02-19 04:44:39,147] [INFO] [timer.py:260:stop] epoch=0/micro_step=1200/global_step=60, RunningAvgSamplesPerSec=15.565504375776605, CurrSamplesPerSec=17.234257743958693, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.96923076923077e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = Epoch: [0][ 61/500]   Time 147.206 (148.042)  Loss 8.8656 (12.7409)   VQALoss 0.0000 (0.0000) VGLoss 8.8656 (12.7409)  curr_lr =  curr_lr =  
   [9.96789297658863e-06][9.96789297658863e-06][9.96789297658863e-06][9.96789297658863e-06][9.96789297658863e-06][9.96789297658863e-06]



[9.96789297658863e-06]


curr_lr =  [9.96789297658863e-06]
curr_lr = curr_lr =   [9.966555183946488e-06]
curr_lr = [9.966555183946488e-06]
curr_lr =   [9.966555183946488e-06]
[9.966555183946488e-06]
curr_lr =  curr_lr =  [9.966555183946488e-06]
[9.966555183946488e-06]
curr_lr =  [9.966555183946488e-06]curr_lr = 
 [9.966555183946488e-06]
curr_lr = curr_lr =   curr_lr = [9.965217391304348e-06]
 [9.965217391304348e-06]
curr_lr = [9.965217391304348e-06]
 [9.965217391304348e-06]
curr_lr =  curr_lr =  [9.965217391304348e-06]
[9.965217391304348e-06]
curr_lr =  [9.965217391304348e-06]
curr_lr =  [9.965217391304348e-06]
curr_lr = curr_lr =   curr_lr = [9.963879598662208e-06] 
[9.963879598662208e-06]
curr_lr = [9.963879598662208e-06]
 curr_lr =  [9.963879598662208e-06]
[9.963879598662208e-06]
curr_lr =  [9.963879598662208e-06]
curr_lr =  [9.963879598662208e-06]
curr_lr =  [9.963879598662208e-06]
curr_lr = curr_lr =  curr_lr =   [9.962541806020068e-06][9.962541806020068e-06]curr_lr = 

 [9.962541806020068e-06]
[9.962541806020068e-06]curr_lr = 
 [9.962541806020068e-06]curr_lr = 
 [9.962541806020068e-06]
curr_lr =  [9.962541806020068e-06]
curr_lr =  [9.962541806020068e-06]
curr_lr = curr_lr =  curr_lr = curr_lr =    Epoch: [0][ 66/500] Time 148.820 (148.456)  Loss 12.7464 (12.5910)  VQALoss 0.0000 (0.0000) VGLoss 12.7464 (12.5910)curr_lr = [9.961204013377928e-06]curr_lr = 

[9.961204013377928e-06][9.961204013377928e-06] [9.961204013377928e-06]
 

curr_lr =  [9.961204013377928e-06]
[9.961204013377928e-06]
[9.961204013377928e-06]
curr_lr =  [9.961204013377928e-06]
curr_lr =  [9.959866220735786e-06]curr_lr = 
 [9.959866220735786e-06]
curr_lr = curr_lr =   [9.959866220735786e-06]
[9.959866220735786e-06]
curr_lr = curr_lr =   [9.959866220735786e-06]
[9.959866220735786e-06]
curr_lr =  [9.959866220735786e-06]
curr_lr =  [9.959866220735786e-06]
curr_lr = curr_lr =  curr_lr = [9.958528428093646e-06] 
curr_lr =   [9.958528428093646e-06][9.958528428093646e-06]

[9.958528428093646e-06]
curr_lr =  curr_lr =  [9.958528428093646e-06]
[9.958528428093646e-06]
curr_lr =  [9.958528428093646e-06]
curr_lr =  [9.958528428093646e-06]
curr_lr =  curr_lr =  [9.957190635451506e-06]
[9.957190635451506e-06]
curr_lr =  curr_lr =  curr_lr = [9.957190635451506e-06]
[9.957190635451506e-06]
 curr_lr = [9.957190635451506e-06]
 [9.957190635451506e-06]
curr_lr =  [9.957190635451506e-06]
curr_lr =  [9.957190635451506e-06]
[2024-02-19 05:09:21,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=22, lr=[9.956521739130436e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr = curr_lr =    [9.955852842809364e-06]
[9.955852842809364e-06]
curr_lr =  [9.955852842809364e-06]
[9.955852842809364e-06]
curr_lr =  [9.955852842809364e-06]curr_lr = 
 [9.955852842809364e-06]
curr_lr =  [9.955852842809364e-06]
[2024-02-19 05:09:21,378] [INFO] [timer.py:260:stop] epoch=0/micro_step=1400/global_step=70, RunningAvgSamplesPerSec=15.796131905282211, CurrSamplesPerSec=17.258800332915833, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.955852842809364e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =  curr_lr =  Epoch: [0][ 71/500]    Time 147.536 (148.056)  Loss 8.4453 (12.6830)   VQALoss 0.0000 (0.0000) VGLoss 8.4453 (12.6830)  [9.954515050167226e-06] 

[9.954515050167226e-06][9.954515050167226e-06]
 
[9.954515050167226e-06]
[9.954515050167226e-06]
[9.954515050167226e-06]
[9.954515050167226e-06]
curr_lr =  [9.954515050167226e-06]
curr_lr =  [9.953177257525084e-06]curr_lr = 
curr_lr =   [9.953177257525084e-06]
[9.953177257525084e-06]
curr_lr = curr_lr = curr_lr =    [9.953177257525084e-06][9.953177257525084e-06]
[9.953177257525084e-06]

curr_lr =  [9.953177257525084e-06]
curr_lr =  [9.953177257525084e-06]
curr_lr =  curr_lr =  curr_lr = [9.951839464882944e-06][9.951839464882944e-06]curr_lr = 

  [9.951839464882944e-06]
[9.951839464882944e-06]
curr_lr =  curr_lr =  [9.951839464882944e-06]
[9.951839464882944e-06]
curr_lr =  [9.951839464882944e-06]
curr_lr =  [9.951839464882944e-06]
curr_lr =  curr_lr =  curr_lr = [9.950501672240804e-06]
 curr_lr = [9.950501672240804e-06]
 [9.950501672240804e-06]
[9.950501672240804e-06]
curr_lr = curr_lr =   [9.950501672240804e-06][9.950501672240804e-06]

curr_lr =  [9.950501672240804e-06]
curr_lr =  [9.950501672240804e-06]
curr_lr = curr_lr =   [9.949163879598664e-06]
[9.949163879598664e-06]
curr_lr =  [9.949163879598664e-06]
curr_lr =  curr_lr = [9.949163879598664e-06] 
curr_lr = [9.949163879598664e-06]
 [9.949163879598664e-06]
curr_lr =  [9.949163879598664e-06]
curr_lr =  [9.949163879598664e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =      curr_lr =  Epoch: [0][ 76/500] Time 148.560 (148.316)  Loss 10.1266 (12.1478)  VQALoss 0.0000 (0.0000) VGLoss 10.1266 (12.1478) [9.947826086956522e-06][9.947826086956522e-06][9.947826086956522e-06]
[9.947826086956522e-06][9.947826086956522e-06]

[9.947826086956522e-06]


[9.947826086956522e-06]

curr_lr =  [9.947826086956522e-06]
curr_lr =  [9.946488294314382e-06]
curr_lr =  curr_lr =  [9.946488294314382e-06]
[9.946488294314382e-06]
curr_lr = curr_lr =   [9.946488294314382e-06][9.946488294314382e-06]

curr_lr =  [9.946488294314382e-06]
curr_lr =  [9.946488294314382e-06]
curr_lr =  [9.946488294314382e-06]
curr_lr =  curr_lr = [9.945150501672242e-06]
 [9.945150501672242e-06]curr_lr = 
 [9.945150501672242e-06]
curr_lr = curr_lr =   [9.945150501672242e-06]
[9.945150501672242e-06]
curr_lr =  [9.945150501672242e-06]
curr_lr =  [9.945150501672242e-06]
curr_lr =  [9.945150501672242e-06]
curr_lr =  [9.9438127090301e-06]curr_lr = 
curr_lr =   [9.9438127090301e-06]
[9.9438127090301e-06]curr_lr = 
 [9.9438127090301e-06]
curr_lr =  curr_lr = [9.9438127090301e-06] 
[9.9438127090301e-06]
curr_lr =  [9.9438127090301e-06]
curr_lr =  [9.9438127090301e-06]
[2024-02-19 05:34:02,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=22, lr=[9.94314381270903e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = curr_lr =   [9.942474916387962e-06]
[9.942474916387962e-06][9.942474916387962e-06]

curr_lr = curr_lr =   [9.942474916387962e-06]
[9.942474916387962e-06]
curr_lr =  [9.942474916387962e-06]
curr_lr =  [9.942474916387962e-06]
[2024-02-19 05:34:02,610] [INFO] [timer.py:260:stop] epoch=0/micro_step=1600/global_step=80, RunningAvgSamplesPerSec=15.973444506272564, CurrSamplesPerSec=17.273423649459186, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.942474916387962e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =    curr_lr =   Epoch: [0][ 81/500] Time 147.652 (147.954)  Loss 10.0080 (12.0625)  VQALoss 0.0000 (0.0000) VGLoss 10.0080 (12.0625)[9.94113712374582e-06] [9.94113712374582e-06][9.94113712374582e-06]

[9.94113712374582e-06]
[9.94113712374582e-06]

[9.94113712374582e-06]
[9.94113712374582e-06]

curr_lr =  [9.94113712374582e-06]
curr_lr = curr_lr =   [9.93979933110368e-06]
[9.93979933110368e-06]
curr_lr =  curr_lr = [9.93979933110368e-06]
 [9.93979933110368e-06]curr_lr = 
 [9.93979933110368e-06]
curr_lr =  [9.93979933110368e-06]
curr_lr =  [9.93979933110368e-06]
curr_lr =  [9.93979933110368e-06]
curr_lr =  [9.93846153846154e-06]
curr_lr = curr_lr = curr_lr =    [9.93846153846154e-06]
[9.93846153846154e-06][9.93846153846154e-06]

curr_lr =  [9.93846153846154e-06]
curr_lr =  [9.93846153846154e-06]
curr_lr =  [9.93846153846154e-06]
curr_lr =  [9.93846153846154e-06]
curr_lr = curr_lr = curr_lr =    curr_lr = [9.9371237458194e-06]
[9.9371237458194e-06] 
[9.9371237458194e-06]
[9.9371237458194e-06]
curr_lr =  curr_lr = [9.9371237458194e-06]
 [9.9371237458194e-06]
curr_lr =  [9.9371237458194e-06]
curr_lr =  [9.9371237458194e-06]
curr_lr = curr_lr =   curr_lr = [9.935785953177258e-06] 
[9.935785953177258e-06][9.935785953177258e-06]

curr_lr =  [9.935785953177258e-06]
curr_lr =  [9.935785953177258e-06]
curr_lr =  [9.935785953177258e-06]
curr_lr =  [9.935785953177258e-06]
curr_lr =  [9.935785953177258e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     curr_lr = [9.934448160535118e-06][9.934448160535118e-06]curr_lr = 

[9.934448160535118e-06]curr_lr =  
[9.934448160535118e-06]  
Epoch: [0][ 86/500]     Time 148.859 (148.380)  Loss 8.0856 (11.4242)   VQALoss 0.0000 (0.0000) VGLoss 8.0856 (11.4242)[9.934448160535118e-06]

[9.934448160535118e-06][9.934448160535118e-06]

curr_lr =  [9.934448160535118e-06]
curr_lr =  curr_lr =  [9.933110367892978e-06]
[9.933110367892978e-06]
curr_lr =  [9.933110367892978e-06]
curr_lr =  curr_lr = [9.933110367892978e-06]
curr_lr =   [9.933110367892978e-06]
[9.933110367892978e-06]
curr_lr =  [9.933110367892978e-06]
curr_lr =  [9.933110367892978e-06]
curr_lr = curr_lr =   curr_lr = [9.931772575250836e-06]curr_lr = 
[9.931772575250836e-06] 
 [9.931772575250836e-06]
[9.931772575250836e-06]curr_lr = 
 [9.931772575250836e-06]
curr_lr =  [9.931772575250836e-06]
curr_lr =  [9.931772575250836e-06]
curr_lr =  [9.931772575250836e-06]
curr_lr = curr_lr =   curr_lr =  [9.930434782608697e-06][9.930434782608697e-06]

curr_lr = [9.930434782608697e-06]
 [9.930434782608697e-06]
curr_lr =  curr_lr = [9.930434782608697e-06]
 [9.930434782608697e-06]
curr_lr =  [9.930434782608697e-06]
curr_lr =  [9.930434782608697e-06]
[2024-02-19 05:58:45,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=22, lr=[9.929765886287627e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr =  [9.929096989966556e-06]
curr_lr =  [9.929096989966556e-06][9.929096989966556e-06]
curr_lr = 
 [9.929096989966556e-06]
curr_lr = curr_lr =   [9.929096989966556e-06]
[9.929096989966556e-06]
curr_lr =  [9.929096989966556e-06]
[2024-02-19 05:58:45,926] [INFO] [timer.py:260:stop] epoch=0/micro_step=1800/global_step=90, RunningAvgSamplesPerSec=16.110883041032704, CurrSamplesPerSec=17.188806785984806, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.929096989966556e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    Epoch: [0][ 91/500]    Time 147.406 (148.234)  Loss 7.9508 (11.5464)   VQALoss 0.0000 (0.0000) VGLoss 7.9508 (11.5464)  
  [9.927759197324415e-06][9.927759197324415e-06]
[9.927759197324415e-06][9.927759197324415e-06]


[9.927759197324415e-06][9.927759197324415e-06]
[9.927759197324415e-06]

curr_lr =  [9.927759197324415e-06]
curr_lr =  [9.926421404682275e-06]
curr_lr = curr_lr =   curr_lr = [9.926421404682275e-06]
 [9.926421404682275e-06]
[9.926421404682275e-06]
curr_lr = curr_lr =   [9.926421404682275e-06]
[9.926421404682275e-06]
curr_lr =  [9.926421404682275e-06]
curr_lr =  [9.926421404682275e-06]
curr_lr =  curr_lr = curr_lr =  [9.925083612040135e-06] 
[9.925083612040135e-06]
[9.925083612040135e-06]
curr_lr =  [9.925083612040135e-06]
curr_lr =  [9.925083612040135e-06]
curr_lr =  curr_lr =  [9.925083612040135e-06]
[9.925083612040135e-06]
curr_lr =  [9.925083612040135e-06]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.923745819397994e-06] 
 [9.923745819397994e-06]
[9.923745819397994e-06]
[9.923745819397994e-06]
curr_lr = curr_lr =   [9.923745819397994e-06][9.923745819397994e-06]

curr_lr =  [9.923745819397994e-06]
curr_lr =  [9.923745819397994e-06]
curr_lr =  curr_lr = curr_lr =  [9.922408026755853e-06]
 curr_lr = [9.922408026755853e-06]
 [9.922408026755853e-06]
[9.922408026755853e-06]
curr_lr =  [9.922408026755853e-06]
curr_lr =  [9.922408026755853e-06]
curr_lr =  [9.922408026755853e-06]
curr_lr =  [9.922408026755853e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =  curr_lr =     [9.921070234113713e-06]Epoch: [0][ 96/500]  Time 148.282 (148.238)  Loss 8.9025 (10.8779)   VQALoss 0.0000 (0.0000) VGLoss 8.9025 (10.8779) [9.921070234113713e-06]
[9.921070234113713e-06]
[9.921070234113713e-06]
[9.921070234113713e-06]

[9.921070234113713e-06]

[9.921070234113713e-06]
curr_lr =  [9.921070234113713e-06]
curr_lr =  [9.919732441471572e-06]
curr_lr =  curr_lr =  [9.919732441471572e-06]
[9.919732441471572e-06]
curr_lr =  curr_lr = curr_lr =  [9.919732441471572e-06]
 [9.919732441471572e-06]
[9.919732441471572e-06]
curr_lr =  [9.919732441471572e-06]
curr_lr =  [9.919732441471572e-06]
curr_lr = curr_lr =  curr_lr = curr_lr =    [9.918394648829433e-06]
[9.918394648829433e-06]
[9.918394648829433e-06][9.918394648829433e-06]

curr_lr = curr_lr =   [9.918394648829433e-06]
[9.918394648829433e-06]
curr_lr =  [9.918394648829433e-06]
curr_lr =  [9.918394648829433e-06]
curr_lr =  [9.917056856187291e-06]curr_lr = curr_lr = 
  [9.917056856187291e-06][9.917056856187291e-06]

curr_lr =  curr_lr = [9.917056856187291e-06]
 curr_lr =  [9.917056856187291e-06]
[9.917056856187291e-06]
curr_lr =  [9.917056856187291e-06]
curr_lr =  [9.917056856187291e-06]
[2024-02-19 06:23:28,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=22, lr=[9.916387959866221e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = [9.915719063545151e-06]curr_lr = 
  curr_lr = [9.915719063545151e-06][9.915719063545151e-06]

 [9.915719063545151e-06]
curr_lr =  [9.915719063545151e-06]
curr_lr =  [9.915719063545151e-06]
curr_lr =  [9.915719063545151e-06]
[2024-02-19 06:23:28,666] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=100, RunningAvgSamplesPerSec=16.222507865578038, CurrSamplesPerSec=17.253832110915262, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.915719063545151e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =        [9.914381270903011e-06]Epoch: [0][101/500] Time 147.741 (148.377)  Loss 6.2193 (10.9477)   VQALoss 0.0000 (0.0000) VGLoss 6.2193 (10.9477)[9.914381270903011e-06]
[9.914381270903011e-06][9.914381270903011e-06][9.914381270903011e-06][9.914381270903011e-06][9.914381270903011e-06]






curr_lr =  [9.914381270903011e-06]
[2024-02-19 06:26:00,817] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step101 is about to be saved!
[2024-02-19 06:26:12,635] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step101/mp_rank_00_model_states.pt
[2024-02-19 06:26:12,635] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step101/mp_rank_00_model_states.pt...
[2024-02-19 06:28:27,811] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step101/mp_rank_00_model_states.pt.
[2024-02-19 06:28:29,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step101/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-19 06:28:29,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step101/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-19 06:28:29,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step101/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-19 06:28:29,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step101/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-19 06:28:29,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step101/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-19 06:28:29,114] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step101/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-19 06:28:29,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step101/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-19 06:28:29,114] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step101/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-19 06:28:33,245] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step101/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-19 06:28:33,246] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step101/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-19 06:28:33,246] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step101 is ready now!
[2024-02-19 06:28:33,298] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step101/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-19 06:28:33,298] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step101/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-19 06:28:33,298] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step101 is ready now!
[2024-02-19 06:28:33,338] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step101/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-19 06:28:33,338] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step101/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-19 06:28:33,338] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step101 is ready now!
[2024-02-19 06:28:33,347] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step101/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-19 06:28:33,347] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step101/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-19 06:28:33,348] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step101 is ready now!
[2024-02-19 06:28:33,402] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step101/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-19 06:28:33,402] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step101/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-19 06:28:33,403] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step101 is ready now!
[2024-02-19 06:28:33,439] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step101/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-19 06:28:33,440] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step101/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-19 06:28:33,440] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step101 is ready now!
[2024-02-19 06:28:33,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step101/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-19 06:28:33,444] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step101/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-19 06:28:33,444] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step101 is ready now!
[2024-02-19 06:28:33,454] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step101/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-19 06:28:33,462] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step101/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-19 06:28:33,462] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step101 is ready now!
curr_lr = curr_lr =  curr_lr =  [9.913043478260871e-06] 
curr_lr = [9.913043478260871e-06]
 [9.913043478260871e-06]
[9.913043478260871e-06]
curr_lr =  curr_lr = [9.913043478260871e-06]
 [9.913043478260871e-06]
curr_lr =  [9.913043478260871e-06]
curr_lr =  [9.913043478260871e-06]
curr_lr =  curr_lr =  curr_lr = [9.91170568561873e-06]
curr_lr =   [9.91170568561873e-06]
[9.91170568561873e-06]
[9.91170568561873e-06]
curr_lr =  [9.91170568561873e-06]curr_lr = 
 [9.91170568561873e-06]
curr_lr =  [9.91170568561873e-06]
curr_lr =  [9.91170568561873e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     [9.91036789297659e-06][9.91036789297659e-06]

[9.91036789297659e-06][9.91036789297659e-06]

curr_lr =  [9.91036789297659e-06]
curr_lr =  [9.91036789297659e-06]
curr_lr =  curr_lr =  [9.91036789297659e-06]
[9.91036789297659e-06]
curr_lr =  curr_lr =  [9.909030100334449e-06]
[9.909030100334449e-06]
curr_lr =  curr_lr = curr_lr =  [9.909030100334449e-06]
 [9.909030100334449e-06]
[9.909030100334449e-06]
curr_lr = curr_lr =   [9.909030100334449e-06]
[9.909030100334449e-06]
curr_lr =  [9.909030100334449e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =  curr_lr = curr_lr =   curr_lr =  [9.907692307692309e-06][9.907692307692309e-06] 

 Epoch: [0][106/500]    Time 148.475 (179.646)  Loss 10.7908 (10.6119)  VQALoss 0.0000 (0.0000) VGLoss 10.7908 (10.6119)[9.907692307692309e-06][9.907692307692309e-06][9.907692307692309e-06]


[9.907692307692309e-06][9.907692307692309e-06]


curr_lr =  [9.907692307692309e-06]
curr_lr = curr_lr =   [9.906354515050169e-06]
[9.906354515050169e-06]
curr_lr = curr_lr =   [9.906354515050169e-06]
[9.906354515050169e-06]
curr_lr = curr_lr =   [9.906354515050169e-06][9.906354515050169e-06]

curr_lr =  [9.906354515050169e-06]
curr_lr =  [9.906354515050169e-06]
curr_lr = curr_lr =   curr_lr = [9.905016722408027e-06]
 [9.905016722408027e-06]
curr_lr = [9.905016722408027e-06] 
[9.905016722408027e-06]curr_lr = 
 [9.905016722408027e-06]
curr_lr =  [9.905016722408027e-06]
curr_lr =  [9.905016722408027e-06]
curr_lr =  [9.905016722408027e-06]
curr_lr =  curr_lr =  [9.903678929765887e-06]
[9.903678929765887e-06]
curr_lr = curr_lr =   [9.903678929765887e-06]
[9.903678929765887e-06]
curr_lr =  [9.903678929765887e-06]
curr_lr =  [9.903678929765887e-06]
curr_lr =  [9.903678929765887e-06]
curr_lr =  [9.903678929765887e-06]
[2024-02-19 06:50:47,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=22, lr=[9.903010033444817e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =   [9.902341137123747e-06][9.902341137123747e-06]curr_lr = 

 [9.902341137123747e-06]
curr_lr =  curr_lr = [9.902341137123747e-06]
 [9.902341137123747e-06]
curr_lr =  [9.902341137123747e-06]
curr_lr =  [9.902341137123747e-06]
[2024-02-19 06:50:48,098] [INFO] [timer.py:260:stop] epoch=0/micro_step=2200/global_step=110, RunningAvgSamplesPerSec=16.314947055187467, CurrSamplesPerSec=17.20380148859207, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.902341137123747e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =  curr_lr =  Epoch: [0][111/500]    Time 147.872 (148.266)  Loss 20.3813 (10.6947)  VQALoss 0.0000 (0.0000) VGLoss 20.3813 (10.6947)  
[9.901003344481607e-06] 
[9.901003344481607e-06]
 [9.901003344481607e-06]
[9.901003344481607e-06][9.901003344481607e-06]
[9.901003344481607e-06]

[9.901003344481607e-06]
curr_lr =  [9.901003344481607e-06]
curr_lr =  curr_lr = [9.899665551839465e-06]
curr_lr =   [9.899665551839465e-06][9.899665551839465e-06]

curr_lr = curr_lr =  curr_lr = [9.899665551839465e-06] 
 [9.899665551839465e-06][9.899665551839465e-06]

curr_lr =  [9.899665551839465e-06]curr_lr = 
 [9.899665551839465e-06]
curr_lr =  curr_lr = [9.898327759197325e-06] 
curr_lr = [9.898327759197325e-06] 
[9.898327759197325e-06]
curr_lr =  [9.898327759197325e-06]
curr_lr = curr_lr = curr_lr =    [9.898327759197325e-06]
[9.898327759197325e-06]
[9.898327759197325e-06]
curr_lr =  [9.898327759197325e-06]
curr_lr =  curr_lr = curr_lr = curr_lr =  [9.896989966555185e-06] 
 [9.896989966555185e-06]
[9.896989966555185e-06][9.896989966555185e-06]

curr_lr = curr_lr =   [9.896989966555185e-06]
[9.896989966555185e-06]
curr_lr =  [9.896989966555185e-06]
curr_lr =  [9.896989966555185e-06]
curr_lr = curr_lr =   [9.895652173913045e-06]
[9.895652173913045e-06]curr_lr = 
curr_lr =   [9.895652173913045e-06][9.895652173913045e-06]

curr_lr = curr_lr =   [9.895652173913045e-06]
[9.895652173913045e-06]
curr_lr =  [9.895652173913045e-06]
curr_lr =  [9.895652173913045e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     curr_lr = curr_lr = Epoch: [0][116/500]     Time 148.696 (148.313)  Loss 27.2933 (11.1288)  VQALoss 0.0000 (0.0000) VGLoss 27.2933 (11.1288)[9.894314381270905e-06][9.894314381270905e-06] [9.894314381270905e-06][9.894314381270905e-06] 

curr_lr = 

[9.894314381270905e-06]

 [9.894314381270905e-06]
[9.894314381270905e-06]
curr_lr =  [9.894314381270905e-06]
curr_lr = curr_lr =   [9.892976588628763e-06]curr_lr = 
[9.892976588628763e-06]
 [9.892976588628763e-06]
curr_lr =  [9.892976588628763e-06]
curr_lr =  [9.892976588628763e-06]
curr_lr =  [9.892976588628763e-06]
curr_lr =  [9.892976588628763e-06]
curr_lr =  [9.892976588628763e-06]
curr_lr = curr_lr =   curr_lr = [9.891638795986623e-06][9.891638795986623e-06]

 [9.891638795986623e-06]
curr_lr =  curr_lr =  [9.891638795986623e-06]
curr_lr = [9.891638795986623e-06] 
[9.891638795986623e-06]
curr_lr =  [9.891638795986623e-06]
curr_lr =  [9.891638795986623e-06]
curr_lr =  curr_lr = [9.890301003344483e-06]
curr_lr =  curr_lr =   [9.890301003344483e-06]
[9.890301003344483e-06][9.890301003344483e-06]

curr_lr =  [9.890301003344483e-06]
curr_lr =  [9.890301003344483e-06]
curr_lr =  [9.890301003344483e-06]
curr_lr =  [9.890301003344483e-06]
[2024-02-19 07:15:30,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=22, lr=[9.889632107023413e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr =  [9.888963210702341e-06]
curr_lr = [9.888963210702341e-06]
 curr_lr =  [9.888963210702341e-06]
[9.888963210702341e-06]
curr_lr =  [9.888963210702341e-06]
curr_lr =  [9.888963210702341e-06]
curr_lr =  [9.888963210702341e-06]
[2024-02-19 07:15:30,838] [INFO] [timer.py:260:stop] epoch=0/micro_step=2400/global_step=120, RunningAvgSamplesPerSec=16.392274740820234, CurrSamplesPerSec=17.19027986502629, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.888963210702341e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =       Epoch: [0][121/500] Time 147.694 (148.200)  Loss 8.4141 (10.6440)   VQALoss 0.0000 (0.0000) VGLoss 8.4141 (10.6440) [9.8876254180602e-06]

[9.8876254180602e-06][9.8876254180602e-06][9.8876254180602e-06]

[9.8876254180602e-06]
[9.8876254180602e-06]

[9.8876254180602e-06]
curr_lr =  [9.8876254180602e-06]
curr_lr =  [9.88628762541806e-06]
curr_lr = curr_lr =   curr_lr =  [9.88628762541806e-06]
[9.88628762541806e-06]curr_lr = [9.88628762541806e-06]

 curr_lr = [9.88628762541806e-06]
 [9.88628762541806e-06]
curr_lr =  [9.88628762541806e-06]
curr_lr =  [9.88628762541806e-06]
curr_lr =  curr_lr = [9.88494983277592e-06] curr_lr = curr_lr = 
  [9.88494983277592e-06]
[9.88494983277592e-06]
[9.88494983277592e-06]
curr_lr =  [9.88494983277592e-06]
curr_lr =  [9.88494983277592e-06]
curr_lr =  [9.88494983277592e-06]
curr_lr =  [9.88494983277592e-06]
curr_lr = curr_lr = curr_lr =    curr_lr = [9.88361204013378e-06][9.88361204013378e-06]

 [9.88361204013378e-06]
[9.88361204013378e-06]
curr_lr = curr_lr =   [9.88361204013378e-06]
[9.88361204013378e-06]
curr_lr =  [9.88361204013378e-06]
curr_lr =  [9.88361204013378e-06]
curr_lr = curr_lr =   curr_lr =  [9.88227424749164e-06]
curr_lr = [9.88227424749164e-06]
[9.88227424749164e-06]
curr_lr =  curr_lr =   [9.88227424749164e-06][9.88227424749164e-06][9.88227424749164e-06]


curr_lr =  [9.88227424749164e-06]
curr_lr =  [9.88227424749164e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =       Epoch: [0][126/500] Time 148.316 (148.394)  Loss 5.7698 (10.3718)   VQALoss 0.0000 (0.0000) VGLoss 5.7698 (10.3718) [9.880936454849499e-06]
[9.880936454849499e-06][9.880936454849499e-06][9.880936454849499e-06][9.880936454849499e-06][9.880936454849499e-06]





[9.880936454849499e-06]
curr_lr =  [9.880936454849499e-06]
curr_lr =  curr_lr = [9.879598662207359e-06]
 curr_lr =  [9.879598662207359e-06]
[9.879598662207359e-06]
curr_lr =  curr_lr = [9.879598662207359e-06]
 [9.879598662207359e-06]
curr_lr =  curr_lr = [9.879598662207359e-06]
 [9.879598662207359e-06]
curr_lr =  [9.879598662207359e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     [9.878260869565218e-06][9.878260869565218e-06][9.878260869565218e-06][9.878260869565218e-06]



curr_lr =  [9.878260869565218e-06]
curr_lr =  [9.878260869565218e-06]
curr_lr =  [9.878260869565218e-06]
curr_lr =  [9.878260869565218e-06]
curr_lr =  [9.876923076923077e-06]
curr_lr = curr_lr =   [9.876923076923077e-06]
[9.876923076923077e-06]
curr_lr = curr_lr =   curr_lr =  [9.876923076923077e-06][9.876923076923077e-06]

[9.876923076923077e-06]
curr_lr =  [9.876923076923077e-06]
curr_lr =  [9.876923076923077e-06]
[2024-02-19 07:40:13,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=22, lr=[9.876254180602007e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr = curr_lr =    curr_lr = [9.875585284280937e-06]
 [9.875585284280937e-06]
[9.875585284280937e-06]
[9.875585284280937e-06]
curr_lr =  [9.875585284280937e-06]curr_lr = 
 [9.875585284280937e-06]
curr_lr =  [9.875585284280937e-06]
[2024-02-19 07:40:13,991] [INFO] [timer.py:260:stop] epoch=0/micro_step=2600/global_step=130, RunningAvgSamplesPerSec=16.457774906222625, CurrSamplesPerSec=17.211090680787596, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.875585284280937e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =       [9.874247491638797e-06][9.874247491638797e-06]
[9.874247491638797e-06]
[9.874247491638797e-06][9.874247491638797e-06][9.874247491638797e-06]

[9.874247491638797e-06]


Epoch: [0][131/500]     Time 147.556 (148.209)  Loss 13.1187 (10.5588)  VQALoss 0.0000 (0.0000) VGLoss 13.1187 (10.5588)
curr_lr =  [9.874247491638797e-06]
curr_lr =  [9.872909698996656e-06]
curr_lr = curr_lr =   [9.872909698996656e-06]
[9.872909698996656e-06]curr_lr = 
 curr_lr = [9.872909698996656e-06] 
curr_lr =  [9.872909698996656e-06]
[9.872909698996656e-06]
curr_lr =  curr_lr =  [9.872909698996656e-06]
[9.872909698996656e-06]
curr_lr =  curr_lr = curr_lr =  [9.871571906354516e-06]curr_lr = 
  [9.871571906354516e-06]
[9.871571906354516e-06][9.871571906354516e-06]

curr_lr =  [9.871571906354516e-06]
curr_lr =  [9.871571906354516e-06]
curr_lr =  [9.871571906354516e-06]
curr_lr =  [9.871571906354516e-06]
curr_lr =  curr_lr =  [9.870234113712376e-06]
[9.870234113712376e-06]curr_lr = 
curr_lr =   [9.870234113712376e-06]
[9.870234113712376e-06]
curr_lr =  [9.870234113712376e-06]
curr_lr =  [9.870234113712376e-06]
curr_lr =  [9.870234113712376e-06]
curr_lr =  [9.870234113712376e-06]
curr_lr = curr_lr =   curr_lr = [9.868896321070234e-06][9.868896321070234e-06] 

curr_lr = [9.868896321070234e-06]
 [9.868896321070234e-06]
curr_lr =  [9.868896321070234e-06]
curr_lr =  [9.868896321070234e-06]
curr_lr =  [9.868896321070234e-06]
curr_lr =  [9.868896321070234e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =       [9.867558528428094e-06]
[9.867558528428094e-06][9.867558528428094e-06][9.867558528428094e-06]
[9.867558528428094e-06]curr_lr = 


[9.867558528428094e-06]
 Epoch: [0][136/500]    Time 148.408 (148.421)  Loss 8.5841 (10.3972)   VQALoss 0.0000 (0.0000) VGLoss 8.5841 (10.3972)
[9.867558528428094e-06]
curr_lr =  [9.867558528428094e-06]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.866220735785954e-06] 
 [9.866220735785954e-06]
[9.866220735785954e-06]
[9.866220735785954e-06]
curr_lr = curr_lr =   [9.866220735785954e-06][9.866220735785954e-06]

curr_lr =  [9.866220735785954e-06]
curr_lr =  [9.866220735785954e-06]
curr_lr =  [9.864882943143812e-06]
curr_lr = curr_lr = curr_lr =    [9.864882943143812e-06]
[9.864882943143812e-06]
[9.864882943143812e-06]
curr_lr =  [9.864882943143812e-06]
curr_lr = curr_lr =   [9.864882943143812e-06]
[9.864882943143812e-06]
curr_lr =  [9.864882943143812e-06]
curr_lr =  curr_lr = [9.863545150501674e-06]
 [9.863545150501674e-06]
curr_lr =  curr_lr = [9.863545150501674e-06] 
curr_lr =  curr_lr = [9.863545150501674e-06] [9.863545150501674e-06]

[9.863545150501674e-06]
curr_lr =  [9.863545150501674e-06]
curr_lr =  [9.863545150501674e-06]
[2024-02-19 08:04:56,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=22, lr=[9.862876254180604e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr =  [9.862207357859532e-06]
curr_lr = [9.862207357859532e-06]
 [9.862207357859532e-06]
curr_lr =  [9.862207357859532e-06]
curr_lr =  curr_lr =  [9.862207357859532e-06]
[9.862207357859532e-06]
curr_lr =  [9.862207357859532e-06]
[2024-02-19 08:04:56,651] [INFO] [timer.py:260:stop] epoch=0/micro_step=2800/global_step=140, RunningAvgSamplesPerSec=16.514511498833034, CurrSamplesPerSec=17.244356067586466, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.862207357859532e-06]
curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =  curr_lr =  Epoch: [0][141/500]      Time 147.710 (148.146)  Loss 6.2452 (9.8784)    VQALoss 0.0000 (0.0000) VGLoss 6.2452 (9.8784)[9.860869565217392e-06]  

curr_lr =  [9.860869565217392e-06]
[9.860869565217392e-06] 
[9.860869565217392e-06][9.860869565217392e-06]

[9.860869565217392e-06]
[9.860869565217392e-06]
curr_lr =  [9.860869565217392e-06]
curr_lr =  curr_lr =  curr_lr = [9.859531772575252e-06]
 [9.859531772575252e-06]curr_lr = 
 [9.859531772575252e-06]
curr_lr = [9.859531772575252e-06] 
[9.859531772575252e-06]
curr_lr =  [9.859531772575252e-06]
curr_lr =  [9.859531772575252e-06]
curr_lr =  [9.859531772575252e-06]
curr_lr =  curr_lr = [9.858193979933112e-06]
 curr_lr = [9.858193979933112e-06] 
[9.858193979933112e-06]
curr_lr =  [9.858193979933112e-06]
curr_lr =  curr_lr =  [9.858193979933112e-06]
[9.858193979933112e-06]
curr_lr =  [9.858193979933112e-06]
curr_lr =  [9.858193979933112e-06]
curr_lr =  curr_lr =  [9.85685618729097e-06]
curr_lr = [9.85685618729097e-06]
 curr_lr = [9.85685618729097e-06] 
[9.85685618729097e-06]
curr_lr =  [9.85685618729097e-06]curr_lr = 
 [9.85685618729097e-06]
curr_lr =  [9.85685618729097e-06]
curr_lr =  [9.85685618729097e-06]
curr_lr =  curr_lr = curr_lr = [9.85551839464883e-06]curr_lr = 
   [9.85551839464883e-06]
[9.85551839464883e-06][9.85551839464883e-06]

curr_lr = curr_lr =   [9.85551839464883e-06][9.85551839464883e-06]

curr_lr =  [9.85551839464883e-06]
curr_lr =  [9.85551839464883e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =  curr_lr = Epoch: [0][146/500] Time 148.875 (148.499)  Loss 7.8644 (10.0471)   VQALoss 0.0000 (0.0000) VGLoss 7.8644 (10.0471)  
 curr_lr = [9.85418060200669e-06][9.85418060200669e-06]curr_lr = [9.85418060200669e-06]
[9.85418060200669e-06]

[9.85418060200669e-06]
 
 [9.85418060200669e-06][9.85418060200669e-06]

curr_lr =  [9.85418060200669e-06]
curr_lr =  curr_lr =  [9.852842809364548e-06]
[9.852842809364548e-06]
curr_lr =  curr_lr = [9.852842809364548e-06]
 [9.852842809364548e-06]
curr_lr =  [9.852842809364548e-06]
curr_lr =  [9.852842809364548e-06]
curr_lr =  [9.852842809364548e-06]
curr_lr =  [9.852842809364548e-06]
curr_lr =  [9.85150501672241e-06]
curr_lr = curr_lr =   [9.85150501672241e-06]
[9.85150501672241e-06]
curr_lr =  [9.85150501672241e-06]
curr_lr =  [9.85150501672241e-06]curr_lr = 
 [9.85150501672241e-06]
curr_lr =  [9.85150501672241e-06]
curr_lr =  [9.85150501672241e-06]
curr_lr = curr_lr =   curr_lr =  [9.850167224080268e-06][9.850167224080268e-06]

[9.850167224080268e-06]
curr_lr =  curr_lr = curr_lr = [9.850167224080268e-06] 
 [9.850167224080268e-06]
[9.850167224080268e-06]
curr_lr =  [9.850167224080268e-06]
curr_lr =  [9.850167224080268e-06]
[2024-02-19 08:29:40,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=22, lr=[9.849498327759198e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = [9.848829431438128e-06]curr_lr = 
  curr_lr = [9.848829431438128e-06]
 [9.848829431438128e-06]
[9.848829431438128e-06]
curr_lr =  [9.848829431438128e-06]
curr_lr = curr_lr =   [9.848829431438128e-06]
[9.848829431438128e-06]
[2024-02-19 08:29:40,323] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=150, RunningAvgSamplesPerSec=16.56334881136126, CurrSamplesPerSec=17.251597655913308, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.848829431438128e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     curr_lr = curr_lr = curr_lr = [9.847491638795988e-06][9.847491638795988e-06]
[9.847491638795988e-06]

  [9.847491638795988e-06]
 Epoch: [0][151/500]    Time 147.742 (148.237)  Loss 6.8873 (9.8320)    VQALoss 0.0000 (0.0000) VGLoss 6.8873 (9.8320)
[9.847491638795988e-06][9.847491638795988e-06]

[9.847491638795988e-06]
curr_lr =  [9.847491638795988e-06]
[2024-02-19 08:32:12,517] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step151 is about to be saved!
[2024-02-19 08:32:24,390] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step151/mp_rank_00_model_states.pt
[2024-02-19 08:32:24,390] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step151/mp_rank_00_model_states.pt...
[2024-02-19 08:34:41,668] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step151/mp_rank_00_model_states.pt.
[2024-02-19 08:34:42,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step151/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-19 08:34:42,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step151/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-19 08:34:42,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step151/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-19 08:34:42,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step151/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-19 08:34:42,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step151/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-19 08:34:42,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step151/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-19 08:34:42,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step151/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-19 08:34:42,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step151/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-19 08:34:46,935] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step151/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-19 08:34:46,935] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step151/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-19 08:34:46,935] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step151 is ready now!
[2024-02-19 08:34:46,941] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step151/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-19 08:34:46,941] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step151/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-19 08:34:46,941] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step151 is ready now!
[2024-02-19 08:34:47,002] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step151/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-19 08:34:47,003] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step151/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-19 08:34:47,003] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step151 is ready now!
[2024-02-19 08:34:47,011] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step151/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-19 08:34:47,012] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step151/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-19 08:34:47,012] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step151 is ready now!
[2024-02-19 08:34:47,208] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step151/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-19 08:34:47,209] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step151/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-19 08:34:47,209] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step151 is ready now!
[2024-02-19 08:34:47,209] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step151/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-19 08:34:47,210] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step151/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-19 08:34:47,210] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step151 is ready now!
[2024-02-19 08:34:47,211] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step151/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-19 08:34:47,211] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step151/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-19 08:34:47,212] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step151 is ready now!
[2024-02-19 08:34:47,329] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step151/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-19 08:34:47,339] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step151/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-19 08:34:47,339] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step151 is ready now!
curr_lr =  curr_lr = curr_lr = [9.846153846153848e-06] 
 [9.846153846153848e-06]
[9.846153846153848e-06]curr_lr = 
 curr_lr = curr_lr = [9.846153846153848e-06]
  [9.846153846153848e-06]
[9.846153846153848e-06]
curr_lr =  [9.846153846153848e-06]
curr_lr =  [9.846153846153848e-06]
curr_lr = curr_lr =   curr_lr = [9.844816053511706e-06][9.844816053511706e-06] 
curr_lr = 
 [9.844816053511706e-06]
[9.844816053511706e-06]
curr_lr = curr_lr =   [9.844816053511706e-06]
[9.844816053511706e-06]
curr_lr =  [9.844816053511706e-06]
curr_lr =  [9.844816053511706e-06]
curr_lr =  curr_lr = curr_lr =   [9.843478260869566e-06]
[9.843478260869566e-06][9.843478260869566e-06]

curr_lr =  curr_lr = [9.843478260869566e-06]curr_lr =  
 [9.843478260869566e-06]
[9.843478260869566e-06]
curr_lr =  [9.843478260869566e-06]
curr_lr =  [9.843478260869566e-06]
curr_lr =  curr_lr =  [9.842140468227426e-06]
[9.842140468227426e-06]
curr_lr =  curr_lr = [9.842140468227426e-06]curr_lr = 
  curr_lr = [9.842140468227426e-06]
[9.842140468227426e-06] 
[9.842140468227426e-06]
curr_lr =  [9.842140468227426e-06]
curr_lr =  [9.842140468227426e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =   curr_lr =  Epoch: [0][156/500]   Time 149.040 (180.491)  Loss 27.9672 (10.3242)  VQALoss 0.0000 (0.0000) VGLoss 27.9672 (10.3242) [9.840802675585284e-06] 

[9.840802675585284e-06][9.840802675585284e-06] 
[9.840802675585284e-06]

[9.840802675585284e-06][9.840802675585284e-06]

[9.840802675585284e-06]
curr_lr =  [9.840802675585284e-06]
curr_lr =  [9.839464882943146e-06]curr_lr = 
 curr_lr = [9.839464882943146e-06]curr_lr = curr_lr = 
   [9.839464882943146e-06][9.839464882943146e-06]

[9.839464882943146e-06]
curr_lr = curr_lr =   [9.839464882943146e-06]
[9.839464882943146e-06]
curr_lr =  [9.839464882943146e-06]
curr_lr =  [9.838127090301004e-06]
curr_lr = curr_lr =   curr_lr = [9.838127090301004e-06]
 [9.838127090301004e-06]
[9.838127090301004e-06]curr_lr = curr_lr = 
  [9.838127090301004e-06][9.838127090301004e-06]

curr_lr =  [9.838127090301004e-06]
curr_lr =  [9.838127090301004e-06]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.836789297658864e-06] 
 [9.836789297658864e-06][9.836789297658864e-06]

[9.836789297658864e-06]
curr_lr =  [9.836789297658864e-06]
curr_lr =  [9.836789297658864e-06]
curr_lr =  [9.836789297658864e-06]
curr_lr =  [9.836789297658864e-06]
[2024-02-19 08:57:04,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=22, lr=[9.836120401337794e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = [9.835451505016724e-06]
 curr_lr = curr_lr = [9.835451505016724e-06] 
 [9.835451505016724e-06]
[9.835451505016724e-06]
curr_lr = curr_lr =   [9.835451505016724e-06][9.835451505016724e-06]

curr_lr =  [9.835451505016724e-06]
[2024-02-19 08:57:04,411] [INFO] [timer.py:260:stop] epoch=0/micro_step=3200/global_step=160, RunningAvgSamplesPerSec=16.605366968241427, CurrSamplesPerSec=17.234846054125104, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.835451505016724e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =  curr_lr =   Epoch: [0][161/500]     Time 147.733 (148.325)  Loss 5.3538 (9.4351)    VQALoss 0.0000 (0.0000) VGLoss 5.3538 (9.4351) curr_lr = [9.834113712374582e-06] [9.834113712374582e-06]

[9.834113712374582e-06]

[9.834113712374582e-06][9.834113712374582e-06]
 
[9.834113712374582e-06]
[9.834113712374582e-06]
curr_lr =  [9.834113712374582e-06]
curr_lr =  [9.832775919732442e-06]
curr_lr = curr_lr = curr_lr =    [9.832775919732442e-06]
[9.832775919732442e-06][9.832775919732442e-06]

curr_lr =  [9.832775919732442e-06]
curr_lr = curr_lr =   [9.832775919732442e-06]
[9.832775919732442e-06]
curr_lr =  [9.832775919732442e-06]
curr_lr =  curr_lr =  [9.831438127090302e-06]
[9.831438127090302e-06]
curr_lr =  [9.831438127090302e-06]
curr_lr = curr_lr = curr_lr =    [9.831438127090302e-06][9.831438127090302e-06][9.831438127090302e-06]


curr_lr =  [9.831438127090302e-06]
curr_lr =  [9.831438127090302e-06]
curr_lr = curr_lr =   curr_lr =  [9.830100334448162e-06]
[9.830100334448162e-06]
[9.830100334448162e-06]
curr_lr =  [9.830100334448162e-06]
curr_lr =  [9.830100334448162e-06]
curr_lr =  [9.830100334448162e-06]
curr_lr =  [9.830100334448162e-06]
curr_lr =  [9.830100334448162e-06]
curr_lr =  curr_lr = curr_lr =  curr_lr = [9.82876254180602e-06] 
 [9.82876254180602e-06][9.82876254180602e-06]

[9.82876254180602e-06]
curr_lr =  [9.82876254180602e-06]
curr_lr = curr_lr =   [9.82876254180602e-06]
[9.82876254180602e-06]
curr_lr =  [9.82876254180602e-06]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.827424749163881e-06] [9.827424749163881e-06]

 curr_lr = Epoch: [0][166/500]  Time 149.394 (148.425)  Loss 4.5862 (9.4383)    VQALoss 0.0000 (0.0000) VGLoss 4.5862 (9.4383)curr_lr = curr_lr =  
[9.827424749163881e-06] [9.827424749163881e-06]

 [9.827424749163881e-06]
[9.827424749163881e-06]
[9.827424749163881e-06]
curr_lr =  [9.827424749163881e-06]
curr_lr = curr_lr =   curr_lr =  [9.82608695652174e-06][9.82608695652174e-06]

[9.82608695652174e-06]
curr_lr =  curr_lr =  [9.82608695652174e-06]
[9.82608695652174e-06]
curr_lr =  [9.82608695652174e-06]
curr_lr =  [9.82608695652174e-06]
curr_lr =  [9.82608695652174e-06]
curr_lr =  curr_lr = curr_lr = [9.8247491638796e-06] 
 curr_lr = [9.8247491638796e-06] 
[9.8247491638796e-06]
curr_lr = [9.8247491638796e-06]
curr_lr =   [9.8247491638796e-06]
[9.8247491638796e-06]
curr_lr =  [9.8247491638796e-06]
curr_lr =  [9.8247491638796e-06]
curr_lr =  curr_lr = [9.82341137123746e-06]
 curr_lr = [9.82341137123746e-06]
 [9.82341137123746e-06]curr_lr = 
 [9.82341137123746e-06]
curr_lr =  curr_lr =  [9.82341137123746e-06]
[9.82341137123746e-06]
curr_lr =  [9.82341137123746e-06]
curr_lr =  [9.82341137123746e-06]
[2024-02-19 09:21:47,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=22, lr=[9.82274247491639e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = [9.822073578595318e-06]
 curr_lr =  [9.822073578595318e-06]
curr_lr = [9.822073578595318e-06] 
[9.822073578595318e-06]
curr_lr =  [9.822073578595318e-06]
curr_lr = curr_lr =   [9.822073578595318e-06]
[9.822073578595318e-06]
[2024-02-19 09:21:47,525] [INFO] [timer.py:260:stop] epoch=0/micro_step=3400/global_step=170, RunningAvgSamplesPerSec=16.643556167336396, CurrSamplesPerSec=17.218455826155, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.822073578595318e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    Epoch: [0][171/500]      Time 147.449 (148.141)  Loss 6.5095 (9.7836)    VQALoss 0.0000 (0.0000) VGLoss 6.5095 (9.7836)  
 [9.820735785953178e-06][9.820735785953178e-06][9.820735785953178e-06]

[9.820735785953178e-06][9.820735785953178e-06]


[9.820735785953178e-06]curr_lr = 
 [9.820735785953178e-06]
curr_lr =  [9.820735785953178e-06]
curr_lr =  curr_lr =  curr_lr = [9.819397993311037e-06]
 [9.819397993311037e-06]
[9.819397993311037e-06]
curr_lr = curr_lr =   curr_lr = [9.819397993311037e-06] 
[9.819397993311037e-06]
[9.819397993311037e-06]
curr_lr =  [9.819397993311037e-06]
curr_lr =  [9.819397993311037e-06]
curr_lr =  curr_lr = [9.818060200668897e-06]
 [9.818060200668897e-06]
curr_lr =  curr_lr = curr_lr = [9.818060200668897e-06]  
[9.818060200668897e-06]
[9.818060200668897e-06]
curr_lr =  [9.818060200668897e-06]curr_lr = 
 [9.818060200668897e-06]
curr_lr =  [9.818060200668897e-06]
curr_lr = curr_lr =   [9.816722408026757e-06]curr_lr = 
[9.816722408026757e-06]
 curr_lr =  [9.816722408026757e-06]
[9.816722408026757e-06]
curr_lr = curr_lr =   [9.816722408026757e-06]
[9.816722408026757e-06]
curr_lr =  [9.816722408026757e-06]
curr_lr =  [9.816722408026757e-06]
curr_lr =  curr_lr = curr_lr =  [9.815384615384617e-06]
 [9.815384615384617e-06]
[9.815384615384617e-06]
curr_lr =  curr_lr =  [9.815384615384617e-06]
[9.815384615384617e-06]
curr_lr =  [9.815384615384617e-06]
curr_lr =  [9.815384615384617e-06]
curr_lr =  [9.815384615384617e-06]
curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =    [9.814046822742475e-06] curr_lr = curr_lr = 
Epoch: [0][176/500]     Time 148.620 (148.346)  Loss 7.7730 (9.7911)    VQALoss 0.0000 (0.0000) VGLoss 7.7730 (9.7911)  
[9.814046822742475e-06][9.814046822742475e-06][9.814046822742475e-06]


[9.814046822742475e-06][9.814046822742475e-06][9.814046822742475e-06]


curr_lr =  [9.814046822742475e-06]
curr_lr =  curr_lr = [9.812709030100335e-06]
 curr_lr =  [9.812709030100335e-06]
[9.812709030100335e-06]
curr_lr =  curr_lr =  [9.812709030100335e-06]
[9.812709030100335e-06]
curr_lr =  [9.812709030100335e-06]
curr_lr =  [9.812709030100335e-06]
curr_lr =  [9.812709030100335e-06]
curr_lr = curr_lr =   [9.811371237458195e-06]
curr_lr = [9.811371237458195e-06]
 [9.811371237458195e-06]
curr_lr = curr_lr =   [9.811371237458195e-06]
[9.811371237458195e-06]
curr_lr =  [9.811371237458195e-06]
curr_lr =  [9.811371237458195e-06]
curr_lr =  [9.811371237458195e-06]
curr_lr = curr_lr =   curr_lr = [9.810033444816053e-06]
 [9.810033444816053e-06][9.810033444816053e-06]

curr_lr = curr_lr =  [9.810033444816053e-06] 
curr_lr = [9.810033444816053e-06] 
[9.810033444816053e-06]
curr_lr =  [9.810033444816053e-06]
curr_lr =  [9.810033444816053e-06]
[2024-02-19 09:46:30,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=22, lr=[9.809364548494983e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =   curr_lr = [9.808695652173913e-06]
[9.808695652173913e-06]
 curr_lr =  [9.808695652173913e-06]
[9.808695652173913e-06]
curr_lr =  [9.808695652173913e-06]
curr_lr =  [9.808695652173913e-06]
curr_lr =  [9.808695652173913e-06]
[2024-02-19 09:46:31,183] [INFO] [timer.py:260:stop] epoch=0/micro_step=3600/global_step=180, RunningAvgSamplesPerSec=16.67727738467585, CurrSamplesPerSec=17.195857204194173, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.808695652173913e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr =    Epoch: [0][181/500]    Time 147.894 (148.474)  Loss 5.7801 (9.0905)    VQALoss 0.0000 (0.0000) VGLoss 5.7801 (9.0905) [9.807357859531773e-06][9.807357859531773e-06]
curr_lr = 

[9.807357859531773e-06][9.807357859531773e-06][9.807357859531773e-06]


[9.807357859531773e-06] 
[9.807357859531773e-06]
curr_lr =  [9.807357859531773e-06]
curr_lr = curr_lr =   curr_lr = [9.806020066889633e-06]curr_lr = [9.806020066889633e-06] 

 [9.806020066889633e-06]
[9.806020066889633e-06]
curr_lr =  curr_lr =  [9.806020066889633e-06]
[9.806020066889633e-06]
curr_lr =  [9.806020066889633e-06]
curr_lr =  [9.806020066889633e-06]
curr_lr = curr_lr =   [9.804682274247493e-06]
[9.804682274247493e-06]
curr_lr =  [9.804682274247493e-06]
curr_lr =  curr_lr = [9.804682274247493e-06] 
[9.804682274247493e-06]
curr_lr =  [9.804682274247493e-06]
curr_lr =  [9.804682274247493e-06]
curr_lr =  [9.804682274247493e-06]
curr_lr = curr_lr =   [9.803344481605353e-06][9.803344481605353e-06]curr_lr = 

 [9.803344481605353e-06]curr_lr = 
 [9.803344481605353e-06]
curr_lr = curr_lr =   [9.803344481605353e-06][9.803344481605353e-06]

curr_lr =  [9.803344481605353e-06]
curr_lr =  [9.803344481605353e-06]
curr_lr = curr_lr =   curr_lr = [9.802006688963211e-06] 
[9.802006688963211e-06]
curr_lr = [9.802006688963211e-06]
 curr_lr =  [9.802006688963211e-06]
curr_lr = [9.802006688963211e-06]
 [9.802006688963211e-06]
curr_lr =  [9.802006688963211e-06]
curr_lr =  [9.802006688963211e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr =     [9.800668896321071e-06] [9.800668896321071e-06]

[9.800668896321071e-06][9.800668896321071e-06]

[9.800668896321071e-06][9.800668896321071e-06]Epoch: [0][186/500]       Time 148.999 (148.586)  Loss 4.1961 (9.0640)    VQALoss 0.0000 (0.0000) VGLoss 4.1961 (9.0640)[9.800668896321071e-06]



curr_lr =  [9.800668896321071e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     [9.799331103678931e-06][9.799331103678931e-06]

[9.799331103678931e-06]
[9.799331103678931e-06]
curr_lr =  [9.799331103678931e-06]
curr_lr =  [9.799331103678931e-06]
curr_lr =  [9.799331103678931e-06]
curr_lr =  [9.799331103678931e-06]
curr_lr =  [9.79799331103679e-06]
curr_lr = curr_lr =   [9.79799331103679e-06][9.79799331103679e-06]

curr_lr =  curr_lr = [9.79799331103679e-06] 
[9.79799331103679e-06]curr_lr = 
 [9.79799331103679e-06]
curr_lr =  [9.79799331103679e-06]
curr_lr =  [9.79799331103679e-06]
curr_lr =  curr_lr = curr_lr = [9.796655518394649e-06] 
 [9.796655518394649e-06]
[9.796655518394649e-06]
curr_lr =  curr_lr = [9.796655518394649e-06]
 [9.796655518394649e-06]
curr_lr =  curr_lr =  [9.796655518394649e-06][9.796655518394649e-06]

curr_lr =  [9.796655518394649e-06]
[2024-02-19 10:11:15,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=22, lr=[9.795986622073579e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = curr_lr = curr_lr =  [9.795317725752509e-06] 
 [9.795317725752509e-06]
[9.795317725752509e-06][9.795317725752509e-06]

curr_lr =  [9.795317725752509e-06]
curr_lr =  [9.795317725752509e-06]curr_lr = 
 [9.795317725752509e-06]
[2024-02-19 10:11:15,813] [INFO] [timer.py:260:stop] epoch=0/micro_step=3800/global_step=190, RunningAvgSamplesPerSec=16.707009039579987, CurrSamplesPerSec=17.243028438361687, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.795317725752509e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =     curr_lr =   Epoch: [0][191/500] Time 148.034 (148.368)  Loss 6.0503 (9.1941)    VQALoss 0.0000 (0.0000) VGLoss 6.0503 (9.1941)[9.793979933110369e-06] [9.793979933110369e-06][9.793979933110369e-06][9.793979933110369e-06][9.793979933110369e-06]

[9.793979933110369e-06]




[9.793979933110369e-06]
curr_lr =  [9.793979933110369e-06]
curr_lr = curr_lr =   [9.792642140468229e-06]
[9.792642140468229e-06]
curr_lr =  [9.792642140468229e-06]
curr_lr =  [9.792642140468229e-06]
curr_lr =  [9.792642140468229e-06]
curr_lr =  [9.792642140468229e-06]
curr_lr =  [9.792642140468229e-06]
curr_lr =  [9.792642140468229e-06]
curr_lr =  curr_lr = [9.791304347826089e-06]curr_lr = curr_lr = 
   [9.791304347826089e-06]
[9.791304347826089e-06][9.791304347826089e-06]

curr_lr =  [9.791304347826089e-06]
curr_lr =  [9.791304347826089e-06]
curr_lr =  [9.791304347826089e-06]
curr_lr =  [9.791304347826089e-06]
curr_lr =  curr_lr =  [9.789966555183947e-06]
curr_lr = [9.789966555183947e-06]
 [9.789966555183947e-06]
curr_lr =  curr_lr = [9.789966555183947e-06]
 [9.789966555183947e-06]
curr_lr = curr_lr =   [9.789966555183947e-06]
[9.789966555183947e-06]
curr_lr =  [9.789966555183947e-06]
curr_lr =  [9.788628762541807e-06]curr_lr = curr_lr = 
  curr_lr = [9.788628762541807e-06][9.788628762541807e-06]

 curr_lr = [9.788628762541807e-06] 
[9.788628762541807e-06]
curr_lr =  [9.788628762541807e-06]
curr_lr =  [9.788628762541807e-06]
curr_lr =  [9.788628762541807e-06]
curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = Epoch: [0][196/500] Time 149.348 (148.795)  Loss 8.9286 (9.3039)    VQALoss 0.0000 (0.0000) VGLoss 8.9286 (9.3039)  
curr_lr = curr_lr =   [9.787290969899667e-06][9.787290969899667e-06]

[9.787290969899667e-06][9.787290969899667e-06][9.787290969899667e-06]


 [9.787290969899667e-06]
[9.787290969899667e-06]
curr_lr =  [9.787290969899667e-06]
curr_lr =  curr_lr = [9.785953177257525e-06]
 curr_lr =  [9.785953177257525e-06]
[9.785953177257525e-06]
curr_lr = curr_lr = curr_lr =    [9.785953177257525e-06][9.785953177257525e-06]

[9.785953177257525e-06]
curr_lr =  [9.785953177257525e-06]
curr_lr =  [9.785953177257525e-06]
curr_lr = curr_lr = curr_lr =    [9.784615384615387e-06]
[9.784615384615387e-06]
[9.784615384615387e-06]
curr_lr = curr_lr =   curr_lr = [9.784615384615387e-06]
[9.784615384615387e-06]
 [9.784615384615387e-06]
curr_lr =  [9.784615384615387e-06]
curr_lr =  [9.784615384615387e-06]
curr_lr =  curr_lr = curr_lr = [9.783277591973245e-06] 
curr_lr =   [9.783277591973245e-06]
[9.783277591973245e-06]
[9.783277591973245e-06]
curr_lr = curr_lr =   [9.783277591973245e-06]
[9.783277591973245e-06]
curr_lr =  [9.783277591973245e-06]
curr_lr =  [9.783277591973245e-06]
[2024-02-19 10:36:01,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=22, lr=[9.782608695652175e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.781939799331105e-06]
curr_lr = curr_lr =   [9.781939799331105e-06]
[9.781939799331105e-06]
curr_lr = curr_lr =   [9.781939799331105e-06]
[9.781939799331105e-06]
curr_lr =  [9.781939799331105e-06]
curr_lr =  [9.781939799331105e-06]
[2024-02-19 10:36:01,979] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=200, RunningAvgSamplesPerSec=16.73296594186646, CurrSamplesPerSec=17.204459311240043, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.781939799331105e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =   curr_lr =    [9.780602006688965e-06] 
[9.780602006688965e-06][9.780602006688965e-06]

[9.780602006688965e-06][9.780602006688965e-06][9.780602006688965e-06]Epoch: [0][201/500]        Time 145.374 (147.906)  Loss 7.7817 (9.0196)    VQALoss 0.0000 (0.0000) VGLoss 7.7817 (9.0196)[9.780602006688965e-06]




curr_lr =  [9.780602006688965e-06]
[2024-02-19 10:38:31,471] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step201 is about to be saved!
[2024-02-19 10:38:43,231] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step201/mp_rank_00_model_states.pt
[2024-02-19 10:38:43,231] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step201/mp_rank_00_model_states.pt...
[2024-02-19 10:40:58,570] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step201/mp_rank_00_model_states.pt.
[2024-02-19 10:40:59,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step201/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-19 10:40:59,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step201/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-19 10:40:59,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step201/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-19 10:40:59,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step201/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-19 10:40:59,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step201/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-19 10:40:59,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step201/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-19 10:40:59,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step201/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-19 10:40:59,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step201/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-19 10:41:03,848] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step201/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-19 10:41:03,848] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step201/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-19 10:41:03,848] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step201 is ready now!
[2024-02-19 10:41:03,938] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step201/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-19 10:41:03,939] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step201/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-19 10:41:03,939] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step201 is ready now!
[2024-02-19 10:41:03,965] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step201/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-19 10:41:03,965] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step201/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-19 10:41:03,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step201 is ready now!
[2024-02-19 10:41:03,966] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step201/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-19 10:41:03,966] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step201/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-19 10:41:03,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step201 is ready now!
[2024-02-19 10:41:04,076] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step201/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-19 10:41:04,076] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step201/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-19 10:41:04,076] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step201 is ready now!
[2024-02-19 10:41:04,146] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step201/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-19 10:41:04,147] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step201/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-19 10:41:04,147] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step201 is ready now!
[2024-02-19 10:41:04,193] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step201/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-19 10:41:04,193] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step201/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-19 10:41:04,193] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step201 is ready now!
[2024-02-19 10:41:04,270] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step201/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-19 10:41:04,278] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step201/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-19 10:41:04,278] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step201 is ready now!
curr_lr =  curr_lr = curr_lr =   [9.779264214046823e-06]
[9.779264214046823e-06][9.779264214046823e-06]

curr_lr =  curr_lr = curr_lr = [9.779264214046823e-06]
  [9.779264214046823e-06]
[9.779264214046823e-06]
curr_lr =  [9.779264214046823e-06]
curr_lr =  [9.779264214046823e-06]
curr_lr = curr_lr = curr_lr =    curr_lr =  [9.777926421404683e-06][9.777926421404683e-06][9.777926421404683e-06]


[9.777926421404683e-06]
curr_lr =  [9.777926421404683e-06]
curr_lr =  [9.777926421404683e-06]
curr_lr =  curr_lr = [9.777926421404683e-06]
 [9.777926421404683e-06]
curr_lr =  [9.776588628762543e-06]
curr_lr =  curr_lr = [9.776588628762543e-06] 
[9.776588628762543e-06]
curr_lr =  [9.776588628762543e-06]
curr_lr =  [9.776588628762543e-06]
curr_lr = curr_lr =   [9.776588628762543e-06][9.776588628762543e-06]

curr_lr =  [9.776588628762543e-06]
curr_lr =  curr_lr = curr_lr = [9.775250836120403e-06] curr_lr = 
 [9.775250836120403e-06]
 [9.775250836120403e-06]
[9.775250836120403e-06]
curr_lr = curr_lr =   [9.775250836120403e-06]
[9.775250836120403e-06]
curr_lr =  [9.775250836120403e-06]
curr_lr =  [9.775250836120403e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    curr_lr =    [9.77391304347826e-06] [9.77391304347826e-06]
[9.77391304347826e-06]
Epoch: [0][206/500]     Time 148.542 (180.176)  Loss 16.8365 (9.1600)   VQALoss 0.0000 (0.0000) VGLoss 16.8365 (9.1600)
[9.77391304347826e-06]

[9.77391304347826e-06]
[9.77391304347826e-06]
[9.77391304347826e-06]
curr_lr =  [9.77391304347826e-06]
curr_lr = curr_lr =   curr_lr = [9.772575250836122e-06][9.772575250836122e-06]

curr_lr =   [9.772575250836122e-06]
[9.772575250836122e-06]
curr_lr =  [9.772575250836122e-06]
curr_lr = curr_lr =   [9.772575250836122e-06]
[9.772575250836122e-06]
curr_lr =  [9.772575250836122e-06]
curr_lr = curr_lr =   curr_lr = curr_lr =  [9.77123745819398e-06][9.77123745819398e-06] 

[9.77123745819398e-06]
[9.77123745819398e-06]
curr_lr =  curr_lr = [9.77123745819398e-06]
 [9.77123745819398e-06]
curr_lr =  [9.77123745819398e-06]
curr_lr =  [9.77123745819398e-06]
curr_lr = curr_lr = curr_lr =    [9.76989966555184e-06][9.76989966555184e-06]

[9.76989966555184e-06]
curr_lr =  [9.76989966555184e-06]
curr_lr =  [9.76989966555184e-06]
curr_lr =  [9.76989966555184e-06]
curr_lr =  [9.76989966555184e-06]
curr_lr =  [9.76989966555184e-06]
[2024-02-19 11:03:21,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=22, lr=[9.76923076923077e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr = curr_lr =    [9.7685618729097e-06]
[9.7685618729097e-06][9.7685618729097e-06]
curr_lr = 
 [9.7685618729097e-06]
curr_lr =  [9.7685618729097e-06]
curr_lr =  curr_lr =  [9.7685618729097e-06]
[9.7685618729097e-06]
[2024-02-19 11:03:21,872] [INFO] [timer.py:260:stop] epoch=0/micro_step=4200/global_step=210, RunningAvgSamplesPerSec=16.758214227976385, CurrSamplesPerSec=17.21644675093337, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.7685618729097e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    curr_lr =  Epoch: [0][211/500]   Time 148.456 (148.420)  Loss 6.9790 (8.9222)    VQALoss 0.0000 (0.0000) VGLoss 6.9790 (8.9222) [9.767224080267559e-06]  [9.767224080267559e-06][9.767224080267559e-06]


[9.767224080267559e-06]
[9.767224080267559e-06]
[9.767224080267559e-06]
[9.767224080267559e-06]

curr_lr =  [9.767224080267559e-06]
curr_lr = curr_lr =   [9.765886287625419e-06][9.765886287625419e-06]

curr_lr =  [9.765886287625419e-06]
curr_lr = curr_lr = curr_lr =    [9.765886287625419e-06]
[9.765886287625419e-06]
[9.765886287625419e-06]
curr_lr =  [9.765886287625419e-06]
curr_lr =  [9.765886287625419e-06]
curr_lr =  curr_lr = curr_lr = [9.764548494983278e-06] 
 [9.764548494983278e-06]
curr_lr = [9.764548494983278e-06] 
[9.764548494983278e-06]curr_lr = 
 [9.764548494983278e-06]
curr_lr =  [9.764548494983278e-06]
curr_lr =  [9.764548494983278e-06]
curr_lr =  [9.764548494983278e-06]
curr_lr = curr_lr =   curr_lr =  [9.763210702341138e-06]
[9.763210702341138e-06]
[9.763210702341138e-06]
curr_lr =  curr_lr = [9.763210702341138e-06]
curr_lr =   [9.763210702341138e-06]
[9.763210702341138e-06]
curr_lr =  [9.763210702341138e-06]
curr_lr =  [9.763210702341138e-06]
curr_lr =  [9.761872909698997e-06]
curr_lr = curr_lr =  curr_lr =  curr_lr =   [9.761872909698997e-06][9.761872909698997e-06]

[9.761872909698997e-06][9.761872909698997e-06]

curr_lr =  [9.761872909698997e-06]
curr_lr =  [9.761872909698997e-06]
curr_lr =  [9.761872909698997e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =    curr_lr =    [9.760535117056858e-06][9.760535117056858e-06][9.760535117056858e-06][9.760535117056858e-06]
[9.760535117056858e-06]



[9.760535117056858e-06][9.760535117056858e-06]

Epoch: [0][216/500]     Time 149.340 (148.275)  Loss 6.3867 (9.4006)    VQALoss 0.0000 (0.0000) VGLoss 6.3867 (9.4006)
curr_lr =  [9.760535117056858e-06]
curr_lr = curr_lr =   curr_lr =  curr_lr = [9.759197324414716e-06]
[9.759197324414716e-06]
[9.759197324414716e-06] 
[9.759197324414716e-06]
curr_lr =  curr_lr = [9.759197324414716e-06] 
[9.759197324414716e-06]
curr_lr =  [9.759197324414716e-06]
curr_lr =  [9.759197324414716e-06]
curr_lr = curr_lr =   curr_lr = [9.757859531772576e-06]
curr_lr =  [9.757859531772576e-06]
 [9.757859531772576e-06]
[9.757859531772576e-06]
curr_lr =  curr_lr = [9.757859531772576e-06] 
[9.757859531772576e-06]
curr_lr =  [9.757859531772576e-06]
curr_lr =  [9.757859531772576e-06]
curr_lr = curr_lr =   curr_lr = [9.756521739130436e-06]
[9.756521739130436e-06] 
curr_lr =  [9.756521739130436e-06]
[9.756521739130436e-06]
curr_lr =  curr_lr = [9.756521739130436e-06]
 [9.756521739130436e-06]
curr_lr =  [9.756521739130436e-06]
curr_lr =  [9.756521739130436e-06]
[2024-02-19 11:28:05,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=22, lr=[9.755852842809366e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =   [9.755183946488294e-06]
[9.755183946488294e-06]
curr_lr =  curr_lr =  [9.755183946488294e-06]
[9.755183946488294e-06]
curr_lr =  curr_lr = [9.755183946488294e-06]
 [9.755183946488294e-06]
curr_lr =  [9.755183946488294e-06]
[2024-02-19 11:28:05,638] [INFO] [timer.py:260:stop] epoch=0/micro_step=4400/global_step=220, RunningAvgSamplesPerSec=16.780766210318742, CurrSamplesPerSec=17.200381698719234, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.755183946488294e-06]
curr_lr = curr_lr = curr_lr =    curr_lr = curr_lr = curr_lr =  [9.753846153846154e-06][9.753846153846154e-06][9.753846153846154e-06]Epoch: [0][221/500]        Time 148.723 (148.532)  Loss 12.0466 (8.7274)   VQALoss 0.0000 (0.0000) VGLoss 12.0466 (8.7274)
 


 curr_lr = [9.753846153846154e-06]
[9.753846153846154e-06]
 [9.753846153846154e-06]
[9.753846153846154e-06]
curr_lr =  [9.753846153846154e-06]
curr_lr =  [9.752508361204014e-06]curr_lr = 
 curr_lr = curr_lr =  [9.752508361204014e-06] 
[9.752508361204014e-06]curr_lr = 
curr_lr = [9.752508361204014e-06]
  [9.752508361204014e-06][9.752508361204014e-06]

curr_lr =  [9.752508361204014e-06]
curr_lr =  [9.752508361204014e-06]
curr_lr = curr_lr = curr_lr =   curr_lr =   [9.751170568561874e-06][9.751170568561874e-06]

[9.751170568561874e-06][9.751170568561874e-06]

curr_lr = curr_lr =   [9.751170568561874e-06][9.751170568561874e-06]

curr_lr =  [9.751170568561874e-06]
curr_lr =  [9.751170568561874e-06]
curr_lr =  curr_lr =  [9.749832775919732e-06]
curr_lr = [9.749832775919732e-06] 
[9.749832775919732e-06]
curr_lr =  curr_lr = [9.749832775919732e-06] 
[9.749832775919732e-06]
curr_lr =  [9.749832775919732e-06]
curr_lr =  [9.749832775919732e-06]curr_lr = 
 [9.749832775919732e-06]
curr_lr =  curr_lr = curr_lr = [9.748494983277594e-06]curr_lr =  
  [9.748494983277594e-06][9.748494983277594e-06][9.748494983277594e-06]


curr_lr =  curr_lr =  [9.748494983277594e-06]
[9.748494983277594e-06]
curr_lr =  [9.748494983277594e-06]
curr_lr =  [9.748494983277594e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =        [9.747157190635452e-06][9.747157190635452e-06][9.747157190635452e-06][9.747157190635452e-06][9.747157190635452e-06]
[9.747157190635452e-06]



[9.747157190635452e-06]
Epoch: [0][226/500]     Time 149.370 (148.468)  Loss 9.2152 (8.8491)    VQALoss 0.0000 (0.0000) VGLoss 9.2152 (8.8491)

curr_lr =  [9.747157190635452e-06]
curr_lr = curr_lr =   curr_lr =  [9.745819397993312e-06]
[9.745819397993312e-06]
[9.745819397993312e-06]
curr_lr =  [9.745819397993312e-06]
curr_lr =  [9.745819397993312e-06]
curr_lr =  [9.745819397993312e-06]
curr_lr =  curr_lr = [9.745819397993312e-06]
 [9.745819397993312e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     [9.744481605351172e-06][9.744481605351172e-06]

[9.744481605351172e-06]
[9.744481605351172e-06]
curr_lr =  curr_lr =  [9.744481605351172e-06]
[9.744481605351172e-06]
curr_lr =  [9.744481605351172e-06]
curr_lr =  [9.744481605351172e-06]
curr_lr =  curr_lr =  [9.74314381270903e-06]
curr_lr = [9.74314381270903e-06]
 [9.74314381270903e-06]
curr_lr =  curr_lr = [9.74314381270903e-06]
 [9.74314381270903e-06]
curr_lr = curr_lr =   [9.74314381270903e-06]
[9.74314381270903e-06]
curr_lr =  [9.74314381270903e-06]
[2024-02-19 11:52:50,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=22, lr=[9.74247491638796e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =   [9.74180602006689e-06]
curr_lr = [9.74180602006689e-06] 
curr_lr = [9.74180602006689e-06]
 curr_lr =  [9.74180602006689e-06]
[9.74180602006689e-06]
curr_lr =  [9.74180602006689e-06]
curr_lr =  [9.74180602006689e-06]
[2024-02-19 11:52:50,797] [INFO] [timer.py:260:stop] epoch=0/micro_step=4600/global_step=230, RunningAvgSamplesPerSec=16.80071441051349, CurrSamplesPerSec=17.20041840001949, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.74180602006689e-06]
curr_lr = curr_lr =  curr_lr = curr_lr =  curr_lr = curr_lr = curr_lr = [9.74046822742475e-06]    [9.74046822742475e-06]
Epoch: [0][231/500]     Time 148.629 (148.545)  Loss 8.4539 (8.8802)    VQALoss 0.0000 (0.0000) VGLoss 8.4539 (8.8802) 
[9.74046822742475e-06][9.74046822742475e-06][9.74046822742475e-06][9.74046822742475e-06]



[9.74046822742475e-06]

curr_lr =  [9.74046822742475e-06]
curr_lr =  [9.73913043478261e-06]
curr_lr = curr_lr =   [9.73913043478261e-06]
[9.73913043478261e-06]curr_lr = 
 curr_lr = [9.73913043478261e-06]
 [9.73913043478261e-06]
curr_lr =  [9.73913043478261e-06]
curr_lr =  curr_lr =  [9.73913043478261e-06]
[9.73913043478261e-06]
curr_lr = curr_lr =   [9.737792642140468e-06][9.737792642140468e-06]

curr_lr =  [9.737792642140468e-06]
curr_lr =  [9.737792642140468e-06]
curr_lr =  curr_lr = [9.737792642140468e-06]
 [9.737792642140468e-06]
curr_lr =  [9.737792642140468e-06]
curr_lr =  [9.737792642140468e-06]
curr_lr =  curr_lr =  [9.73645484949833e-06]
curr_lr = [9.73645484949833e-06] 
curr_lr =  [9.73645484949833e-06]
[9.73645484949833e-06]
curr_lr =  [9.73645484949833e-06]
curr_lr =  [9.73645484949833e-06]
curr_lr =  [9.73645484949833e-06]
curr_lr =  [9.73645484949833e-06]
curr_lr =  curr_lr =  [9.735117056856188e-06]
curr_lr = [9.735117056856188e-06]
 [9.735117056856188e-06]
curr_lr =  curr_lr = [9.735117056856188e-06]
 curr_lr = [9.735117056856188e-06]
 [9.735117056856188e-06]
curr_lr =  [9.735117056856188e-06]
curr_lr =  [9.735117056856188e-06]
curr_lr = curr_lr =  curr_lr = curr_lr =  curr_lr = curr_lr = [9.733779264214048e-06]  
curr_lr = [9.733779264214048e-06] 
 [9.733779264214048e-06]Epoch: [0][236/500]     Time 149.133 (148.563)  Loss 6.9271 (9.3474)    VQALoss 0.0000 (0.0000) VGLoss 6.9271 (9.3474) 
[9.733779264214048e-06]
[9.733779264214048e-06]

[9.733779264214048e-06]
[9.733779264214048e-06]
curr_lr =  [9.733779264214048e-06]
curr_lr =  [9.732441471571908e-06]curr_lr = 
curr_lr = curr_lr =    [9.732441471571908e-06]
[9.732441471571908e-06]
[9.732441471571908e-06]
curr_lr =  [9.732441471571908e-06]
curr_lr =  [9.732441471571908e-06]
curr_lr =  [9.732441471571908e-06]
curr_lr =  [9.732441471571908e-06]
curr_lr = curr_lr =   [9.731103678929766e-06][9.731103678929766e-06]curr_lr = 

 curr_lr = [9.731103678929766e-06]
 curr_lr = [9.731103678929766e-06]
 curr_lr =  [9.731103678929766e-06]
[9.731103678929766e-06]
curr_lr =  [9.731103678929766e-06]
curr_lr =  [9.731103678929766e-06]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.729765886287626e-06] [9.729765886287626e-06]
 
[9.729765886287626e-06][9.729765886287626e-06]

curr_lr =  [9.729765886287626e-06]
curr_lr =  [9.729765886287626e-06]
curr_lr =  [9.729765886287626e-06]
curr_lr =  [9.729765886287626e-06]
[2024-02-19 12:17:36,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=22, lr=[9.729096989966556e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =   curr_lr =  curr_lr = [9.728428093645486e-06][9.728428093645486e-06]

 [9.728428093645486e-06]
[9.728428093645486e-06]
curr_lr =  curr_lr =  [9.728428093645486e-06]
[9.728428093645486e-06]
curr_lr =  [9.728428093645486e-06]
[2024-02-19 12:17:36,526] [INFO] [timer.py:260:stop] epoch=0/micro_step=4800/global_step=240, RunningAvgSamplesPerSec=16.81875136997803, CurrSamplesPerSec=17.190859699433794, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.728428093645486e-06]
curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr = curr_lr = curr_lr = Epoch: [0][241/500]      Time 148.838 (148.624)  Loss 5.8401 (8.6079)    VQALoss 0.0000 (0.0000) VGLoss 5.8401 (8.6079)    [9.727090301003346e-06]
  
[9.727090301003346e-06][9.727090301003346e-06][9.727090301003346e-06][9.727090301003346e-06]



[9.727090301003346e-06][9.727090301003346e-06]

curr_lr =  [9.727090301003346e-06]
curr_lr = curr_lr =  curr_lr =   curr_lr = [9.725752508361206e-06]
[9.725752508361206e-06] 
[9.725752508361206e-06]
[9.725752508361206e-06]
curr_lr =  curr_lr =  [9.725752508361206e-06]
[9.725752508361206e-06]
curr_lr =  [9.725752508361206e-06]
curr_lr =  [9.725752508361206e-06]
curr_lr =  [9.724414715719064e-06]
curr_lr = curr_lr =   [9.724414715719064e-06][9.724414715719064e-06]

curr_lr = curr_lr =   curr_lr = [9.724414715719064e-06][9.724414715719064e-06]

 [9.724414715719064e-06]
curr_lr =  [9.724414715719064e-06]
curr_lr =  [9.724414715719064e-06]
curr_lr =  [9.723076923076924e-06]curr_lr = 
curr_lr =   [9.723076923076924e-06]
[9.723076923076924e-06]
curr_lr =  [9.723076923076924e-06]
curr_lr =  curr_lr = [9.723076923076924e-06]
 [9.723076923076924e-06]
curr_lr =  [9.723076923076924e-06]
curr_lr =  [9.723076923076924e-06]
curr_lr =  curr_lr = [9.721739130434784e-06]
curr_lr =   [9.721739130434784e-06]curr_lr = 
 [9.721739130434784e-06]
[9.721739130434784e-06]
curr_lr = curr_lr =   [9.721739130434784e-06]
[9.721739130434784e-06]
curr_lr =  [9.721739130434784e-06]
curr_lr =  [9.721739130434784e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr =  Epoch: [0][246/500]      Time 148.562 (148.354)  Loss 4.5163 (8.9842)    VQALoss 0.0000 (0.0000) VGLoss 4.5163 (8.9842) curr_lr =  [9.720401337792643e-06][9.720401337792643e-06][9.720401337792643e-06] 
 


[9.720401337792643e-06][9.720401337792643e-06]

[9.720401337792643e-06]
[9.720401337792643e-06]
curr_lr =  [9.720401337792643e-06]
curr_lr = curr_lr =   [9.719063545150502e-06][9.719063545150502e-06]

curr_lr = curr_lr =   [9.719063545150502e-06]
[9.719063545150502e-06]
curr_lr = curr_lr =   [9.719063545150502e-06]
[9.719063545150502e-06]
curr_lr =  [9.719063545150502e-06]
curr_lr =  [9.719063545150502e-06]
curr_lr =  curr_lr = [9.717725752508362e-06]
curr_lr =   [9.717725752508362e-06]curr_lr = [9.717725752508362e-06]

curr_lr =   [9.717725752508362e-06]curr_lr = 
[9.717725752508362e-06] 
[9.717725752508362e-06]
curr_lr =  [9.717725752508362e-06]
curr_lr =  [9.717725752508362e-06]
curr_lr =  curr_lr = [9.716387959866222e-06]
 curr_lr =  [9.716387959866222e-06]
[9.716387959866222e-06]
curr_lr = curr_lr =  curr_lr =   [9.716387959866222e-06][9.716387959866222e-06]

[9.716387959866222e-06]
curr_lr =  [9.716387959866222e-06]
curr_lr =  [9.716387959866222e-06]
[2024-02-19 12:42:22,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=22, lr=[9.715719063545151e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = curr_lr = [9.715050167224081e-06]
  curr_lr = [9.715050167224081e-06][9.715050167224081e-06]

 [9.715050167224081e-06]
curr_lr =  [9.715050167224081e-06]
curr_lr =  [9.715050167224081e-06]
curr_lr =  [9.715050167224081e-06]
[2024-02-19 12:42:22,359] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=250, RunningAvgSamplesPerSec=16.835334237082826, CurrSamplesPerSec=17.204552734735874, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.715050167224081e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr =      [9.713712374581941e-06]Epoch: [0][251/500] Time 148.870 (148.819)  Loss 3.2412 (9.0805)    VQALoss 0.0000 (0.0000) VGLoss 3.2412 (9.0805)
[9.713712374581941e-06][9.713712374581941e-06][9.713712374581941e-06][9.713712374581941e-06]
[9.713712374581941e-06]
[9.713712374581941e-06]




curr_lr =  [9.713712374581941e-06]
[2024-02-19 12:44:55,777] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step251 is about to be saved!
[2024-02-19 12:45:08,004] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step251/mp_rank_00_model_states.pt
[2024-02-19 12:45:08,004] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step251/mp_rank_00_model_states.pt...
[2024-02-19 12:47:21,056] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step251/mp_rank_00_model_states.pt.
[2024-02-19 12:47:22,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step251/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-19 12:47:22,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step251/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-19 12:47:22,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step251/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-19 12:47:22,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step251/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-19 12:47:22,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step251/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-19 12:47:22,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step251/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-19 12:47:22,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step251/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-19 12:47:22,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step251/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-19 12:47:26,529] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step251/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-19 12:47:26,529] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step251/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-19 12:47:26,530] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step251 is ready now!
[2024-02-19 12:47:26,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step251/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-19 12:47:26,582] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step251/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-19 12:47:26,582] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step251 is ready now!
[2024-02-19 12:47:26,607] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step251/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-19 12:47:26,607] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step251/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-19 12:47:26,607] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step251 is ready now!
[2024-02-19 12:47:26,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step251/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-19 12:47:26,622] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step251/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-19 12:47:26,622] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step251 is ready now!
[2024-02-19 12:47:26,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step251/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-19 12:47:26,675] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step251/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-19 12:47:26,676] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step251 is ready now!
[2024-02-19 12:47:26,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step251/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-19 12:47:26,728] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step251/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-19 12:47:26,728] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step251 is ready now!
[2024-02-19 12:47:26,730] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step251/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-19 12:47:26,730] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step251/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-19 12:47:26,730] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step251 is ready now!
[2024-02-19 12:47:26,764] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step251/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-19 12:47:26,765] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step251/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-19 12:47:26,765] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step251 is ready now!
curr_lr =  [9.7123745819398e-06]
curr_lr =  curr_lr = curr_lr =  [9.7123745819398e-06] 
[9.7123745819398e-06]
[9.7123745819398e-06]
curr_lr = curr_lr =   [9.7123745819398e-06]
[9.7123745819398e-06]
curr_lr =  [9.7123745819398e-06]
curr_lr =  [9.7123745819398e-06]
curr_lr =  curr_lr =  [9.71103678929766e-06]curr_lr = 
 [9.71103678929766e-06]
curr_lr = [9.71103678929766e-06] 
[9.71103678929766e-06]
curr_lr =  [9.71103678929766e-06]
curr_lr =  [9.71103678929766e-06]
curr_lr =  [9.71103678929766e-06]
curr_lr =  [9.71103678929766e-06]
curr_lr =  curr_lr = curr_lr =   [9.70969899665552e-06]
curr_lr = [9.70969899665552e-06][9.70969899665552e-06]
 
[9.70969899665552e-06]
curr_lr =  curr_lr = [9.70969899665552e-06]
 [9.70969899665552e-06]
curr_lr =  [9.70969899665552e-06]
curr_lr =  [9.70969899665552e-06]
curr_lr =  curr_lr = curr_lr = curr_lr =  [9.70836120401338e-06]
  [9.70836120401338e-06][9.70836120401338e-06]curr_lr = 

[9.70836120401338e-06] 
[9.70836120401338e-06]
curr_lr = curr_lr =   [9.70836120401338e-06]
curr_lr = [9.70836120401338e-06]
 [9.70836120401338e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =  curr_lr =  curr_lr =   [9.707023411371237e-06] 
curr_lr = [9.707023411371237e-06]
[9.707023411371237e-06]
[9.707023411371237e-06][9.707023411371237e-06] [9.707023411371237e-06]

Epoch: [0][256/500]     Time 149.208 (179.538)  Loss 17.0249 (8.7364)   VQALoss 0.0000 (0.0000) VGLoss 17.0249 (8.7364)

[9.707023411371237e-06]
curr_lr =  [9.707023411371237e-06]
curr_lr =  curr_lr = [9.705685618729097e-06]
 [9.705685618729097e-06]
curr_lr = curr_lr =   curr_lr = curr_lr =   [9.705685618729097e-06][9.705685618729097e-06]

[9.705685618729097e-06]
[9.705685618729097e-06]
curr_lr =  [9.705685618729097e-06]
curr_lr =  [9.705685618729097e-06]
curr_lr =  curr_lr = [9.704347826086957e-06]
 curr_lr =  [9.704347826086957e-06]
curr_lr = [9.704347826086957e-06]
 curr_lr = curr_lr =  [9.704347826086957e-06]
 [9.704347826086957e-06]
[9.704347826086957e-06]
curr_lr =  [9.704347826086957e-06]
curr_lr =  [9.704347826086957e-06]
curr_lr =  [9.703010033444817e-06]
curr_lr =  curr_lr =  [9.703010033444817e-06]
[9.703010033444817e-06]
curr_lr =  [9.703010033444817e-06]
curr_lr =  [9.703010033444817e-06]
curr_lr =  [9.703010033444817e-06]
curr_lr =  [9.703010033444817e-06]curr_lr = 
 [9.703010033444817e-06]
[2024-02-19 13:09:43,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=22, lr=[9.702341137123747e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =   curr_lr =  [9.701672240802677e-06]
[9.701672240802677e-06]
[9.701672240802677e-06]
curr_lr =  [9.701672240802677e-06]
curr_lr = curr_lr =   [9.701672240802677e-06]
[9.701672240802677e-06]
curr_lr =  [9.701672240802677e-06]
[2024-02-19 13:09:44,120] [INFO] [timer.py:260:stop] epoch=0/micro_step=5200/global_step=260, RunningAvgSamplesPerSec=16.850503645072703, CurrSamplesPerSec=17.17834916121014, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.701672240802677e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =      Epoch: [0][261/500]  Time 148.504 (148.741)  Loss 3.4438 (8.8384)    VQALoss 0.0000 (0.0000) VGLoss 3.4438 (8.8384)  
[9.700334448160535e-06]
[9.700334448160535e-06][9.700334448160535e-06][9.700334448160535e-06]
[9.700334448160535e-06]
[9.700334448160535e-06][9.700334448160535e-06]



curr_lr =  [9.700334448160535e-06]
curr_lr =  curr_lr = curr_lr =   [9.698996655518395e-06]
[9.698996655518395e-06]
[9.698996655518395e-06]
curr_lr =  curr_lr = [9.698996655518395e-06]
 curr_lr = [9.698996655518395e-06] 
[9.698996655518395e-06]
curr_lr =  [9.698996655518395e-06]
curr_lr =  [9.698996655518395e-06]
curr_lr =  curr_lr = [9.697658862876255e-06] 
curr_lr = [9.697658862876255e-06]curr_lr = curr_lr = 
   [9.697658862876255e-06]
curr_lr = [9.697658862876255e-06]
 [9.697658862876255e-06]
[9.697658862876255e-06]
curr_lr =  [9.697658862876255e-06]
curr_lr =  [9.697658862876255e-06]
curr_lr =  [9.696321070234115e-06]
curr_lr = curr_lr =   [9.696321070234115e-06]
curr_lr = [9.696321070234115e-06]
 [9.696321070234115e-06]
curr_lr =  [9.696321070234115e-06]
curr_lr =  [9.696321070234115e-06]
curr_lr =  [9.696321070234115e-06]
curr_lr =  [9.696321070234115e-06]
curr_lr = curr_lr =  curr_lr =   [9.694983277591973e-06]
[9.694983277591973e-06][9.694983277591973e-06]

curr_lr = curr_lr =  curr_lr =  [9.694983277591973e-06] 
[9.694983277591973e-06]
[9.694983277591973e-06]
curr_lr =  [9.694983277591973e-06]
curr_lr =  [9.694983277591973e-06]
curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr = curr_lr = Epoch: [0][266/500]     Time 148.399 (148.225)  Loss 5.0372 (8.7273)    VQALoss 0.0000 (0.0000) VGLoss 5.0372 (8.7273)    
[9.693645484949835e-06] [9.693645484949835e-06]
[9.693645484949835e-06]

[9.693645484949835e-06]
[9.693645484949835e-06]
[9.693645484949835e-06][9.693645484949835e-06]

curr_lr =  [9.693645484949835e-06]
curr_lr =  [9.692307692307693e-06]
curr_lr =  curr_lr =  [9.692307692307693e-06]
[9.692307692307693e-06]
curr_lr =  [9.692307692307693e-06]
curr_lr =  [9.692307692307693e-06]
curr_lr =  [9.692307692307693e-06]
curr_lr =  [9.692307692307693e-06]
curr_lr =  [9.692307692307693e-06]
curr_lr =  [9.690969899665551e-06]
curr_lr = curr_lr =   [9.690969899665551e-06]
curr_lr = [9.690969899665551e-06]
 [9.690969899665551e-06]
curr_lr =  [9.690969899665551e-06]
curr_lr =  curr_lr = [9.690969899665551e-06] 
[9.690969899665551e-06]
curr_lr =  [9.690969899665551e-06]
curr_lr =  curr_lr = [9.689632107023413e-06]curr_lr = 
curr_lr =    [9.689632107023413e-06][9.689632107023413e-06]

curr_lr = [9.689632107023413e-06]
 curr_lr = [9.689632107023413e-06]
 [9.689632107023413e-06]
curr_lr =  [9.689632107023413e-06]
curr_lr =  [9.689632107023413e-06]
[2024-02-19 13:34:28,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=22, lr=[9.688963210702343e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =   curr_lr = [9.688294314381271e-06]
 [9.688294314381271e-06]
[9.688294314381271e-06]
curr_lr =  [9.688294314381271e-06]
curr_lr =  curr_lr = curr_lr = [9.688294314381271e-06]
  [9.688294314381271e-06]
[9.688294314381271e-06]
[2024-02-19 13:34:28,236] [INFO] [timer.py:260:stop] epoch=0/micro_step=5400/global_step=270, RunningAvgSamplesPerSec=16.865453260280336, CurrSamplesPerSec=17.1905256871404, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.688294314381271e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    Epoch: [0][271/500]      Time 148.761 (148.649)  Loss 5.1042 (8.9333)    VQALoss 0.0000 (0.0000) VGLoss 5.1042 (8.9333)   curr_lr = 
[9.686956521739131e-06][9.686956521739131e-06][9.686956521739131e-06]


[9.686956521739131e-06] [9.686956521739131e-06][9.686956521739131e-06]


[9.686956521739131e-06]
curr_lr =  [9.686956521739131e-06]
curr_lr = curr_lr =   curr_lr = [9.685618729096991e-06] [9.685618729096991e-06]

[9.685618729096991e-06]
curr_lr = curr_lr = curr_lr =    [9.685618729096991e-06][9.685618729096991e-06]

[9.685618729096991e-06]
curr_lr =  [9.685618729096991e-06]
curr_lr =  [9.685618729096991e-06]
curr_lr = curr_lr = curr_lr =    [9.68428093645485e-06]curr_lr = 
[9.68428093645485e-06][9.68428093645485e-06]
 
curr_lr = [9.68428093645485e-06] 
[9.68428093645485e-06]curr_lr = 
 [9.68428093645485e-06]
curr_lr =  [9.68428093645485e-06]
curr_lr =  [9.68428093645485e-06]
curr_lr = curr_lr =   [9.682943143812709e-06][9.682943143812709e-06]

curr_lr =  [9.682943143812709e-06]
curr_lr =  curr_lr =  [9.682943143812709e-06]
[9.682943143812709e-06]
curr_lr =  [9.682943143812709e-06]
curr_lr =  [9.682943143812709e-06]
curr_lr =  [9.682943143812709e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =    [9.68160535117057e-06]
[9.68160535117057e-06]
[9.68160535117057e-06]
[9.68160535117057e-06]
curr_lr = curr_lr =   [9.68160535117057e-06]
[9.68160535117057e-06]
curr_lr =  [9.68160535117057e-06]
curr_lr =  [9.68160535117057e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =        [9.680267558528429e-06][9.680267558528429e-06][9.680267558528429e-06][9.680267558528429e-06]
[9.680267558528429e-06]

[9.680267558528429e-06][9.680267558528429e-06]

Epoch: [0][276/500]     Time 148.483 (148.181)  Loss 6.6401 (8.4789)    VQALoss 0.0000 (0.0000) VGLoss 6.6401 (8.4789)


curr_lr =  [9.680267558528429e-06]
curr_lr =  [9.678929765886289e-06]
curr_lr = curr_lr =   [9.678929765886289e-06][9.678929765886289e-06]

curr_lr = curr_lr =   [9.678929765886289e-06]
curr_lr =  [9.678929765886289e-06]
[9.678929765886289e-06]
curr_lr =  [9.678929765886289e-06]curr_lr = 
 [9.678929765886289e-06]
curr_lr = curr_lr =   curr_lr = [9.677591973244149e-06][9.677591973244149e-06]

 [9.677591973244149e-06]
curr_lr =  [9.677591973244149e-06]
curr_lr =  [9.677591973244149e-06]
curr_lr =  [9.677591973244149e-06]
curr_lr =  [9.677591973244149e-06]
curr_lr =  [9.677591973244149e-06]
curr_lr =  [9.676254180602007e-06]
curr_lr = curr_lr =  curr_lr =   [9.676254180602007e-06]curr_lr = 
[9.676254180602007e-06]
[9.676254180602007e-06]
 [9.676254180602007e-06]
curr_lr =  curr_lr = [9.676254180602007e-06]
 [9.676254180602007e-06]
curr_lr =  [9.676254180602007e-06]
[2024-02-19 13:59:11,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=22, lr=[9.675585284280937e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = curr_lr =  [9.674916387959867e-06]
 curr_lr = [9.674916387959867e-06][9.674916387959867e-06]

 [9.674916387959867e-06]
curr_lr =  curr_lr =  [9.674916387959867e-06]
[9.674916387959867e-06]
curr_lr =  [9.674916387959867e-06]
[2024-02-19 13:59:12,042] [INFO] [timer.py:260:stop] epoch=0/micro_step=5600/global_step=280, RunningAvgSamplesPerSec=16.879460175454984, CurrSamplesPerSec=17.205235068524345, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.674916387959867e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr =     Epoch: [0][281/500] Time 149.033 (148.634)  Loss 3.9707 (8.3992)    VQALoss 0.0000 (0.0000) VGLoss 3.9707 (8.3992) [9.673578595317727e-06]
[9.673578595317727e-06]
[9.673578595317727e-06][9.673578595317727e-06]
[9.673578595317727e-06]
[9.673578595317727e-06]
[9.673578595317727e-06]


curr_lr =  [9.673578595317727e-06]
curr_lr = curr_lr =   curr_lr = [9.672240802675587e-06][9.672240802675587e-06]

 curr_lr =  [9.672240802675587e-06]
[9.672240802675587e-06]
curr_lr =  curr_lr = [9.672240802675587e-06]
 [9.672240802675587e-06]
curr_lr =  [9.672240802675587e-06]
curr_lr =  [9.672240802675587e-06]
curr_lr = curr_lr = curr_lr =    [9.670903010033445e-06]
[9.670903010033445e-06][9.670903010033445e-06]

curr_lr =  curr_lr = [9.670903010033445e-06]
 curr_lr =  [9.670903010033445e-06]
[9.670903010033445e-06]
curr_lr =  [9.670903010033445e-06]
curr_lr =  [9.670903010033445e-06]
curr_lr = curr_lr = curr_lr =    curr_lr =  [9.669565217391305e-06][9.669565217391305e-06]

[9.669565217391305e-06]
[9.669565217391305e-06]
curr_lr =  curr_lr = [9.669565217391305e-06]
 [9.669565217391305e-06]
curr_lr =  [9.669565217391305e-06]
curr_lr =  [9.669565217391305e-06]
curr_lr =  curr_lr =  [9.668227424749165e-06]
[9.668227424749165e-06]
curr_lr = curr_lr =   [9.668227424749165e-06]
[9.668227424749165e-06]
curr_lr =  [9.668227424749165e-06]
curr_lr =  [9.668227424749165e-06]
curr_lr =  [9.668227424749165e-06]
curr_lr =  [9.668227424749165e-06]
curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr =  curr_lr = Epoch: [0][286/500]      Time 148.457 (148.206)  Loss 6.7937 (8.3623)    VQALoss 0.0000 (0.0000) VGLoss 6.7937 (8.3623)   
[9.666889632107025e-06][9.666889632107025e-06][9.666889632107025e-06]


[9.666889632107025e-06]
[9.666889632107025e-06][9.666889632107025e-06]

curr_lr =  [9.666889632107025e-06]
curr_lr =  [9.666889632107025e-06]
curr_lr =  curr_lr =  curr_lr = [9.665551839464884e-06]
[9.665551839464884e-06]
 [9.665551839464884e-06]
curr_lr =  curr_lr =  [9.665551839464884e-06]
curr_lr = [9.665551839464884e-06] 
[9.665551839464884e-06]
curr_lr =  [9.665551839464884e-06]
curr_lr =  [9.665551839464884e-06]
curr_lr =  curr_lr =  [9.664214046822743e-06]
[9.664214046822743e-06]
curr_lr = curr_lr = curr_lr =   [9.664214046822743e-06] 
curr_lr = [9.664214046822743e-06]
 [9.664214046822743e-06]
[9.664214046822743e-06]
curr_lr =  [9.664214046822743e-06]
curr_lr =  [9.664214046822743e-06]
curr_lr = curr_lr =   [9.662876254180603e-06][9.662876254180603e-06]

curr_lr = curr_lr =   [9.662876254180603e-06]
curr_lr =  [9.662876254180603e-06]
[9.662876254180603e-06]
curr_lr = curr_lr =   [9.662876254180603e-06]
[9.662876254180603e-06]
curr_lr =  [9.662876254180603e-06]
[2024-02-19 14:23:55,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=22, lr=[9.662207357859533e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.661538461538462e-06]
curr_lr =  curr_lr = [9.661538461538462e-06]
 curr_lr = curr_lr = curr_lr = [9.661538461538462e-06] 
  [9.661538461538462e-06][9.661538461538462e-06]
[9.661538461538462e-06]

curr_lr =  [9.661538461538462e-06]
[2024-02-19 14:23:55,606] [INFO] [timer.py:260:stop] epoch=0/micro_step=5800/global_step=290, RunningAvgSamplesPerSec=16.892624900296124, CurrSamplesPerSec=17.230146918116475, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.661538461538462e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    Epoch: [0][291/500]    Time 148.407 (148.382)  Loss 5.0565 (8.5682)    VQALoss 0.0000 (0.0000) VGLoss 5.0565 (8.5682)    
[9.660200668896322e-06][9.660200668896322e-06][9.660200668896322e-06]


[9.660200668896322e-06][9.660200668896322e-06][9.660200668896322e-06]


[9.660200668896322e-06]
curr_lr =  [9.660200668896322e-06]
curr_lr = curr_lr =   [9.65886287625418e-06]
curr_lr = [9.65886287625418e-06]
curr_lr =   [9.65886287625418e-06]
[9.65886287625418e-06]
curr_lr = curr_lr =   [9.65886287625418e-06][9.65886287625418e-06]

curr_lr =  [9.65886287625418e-06]
curr_lr =  [9.65886287625418e-06]
curr_lr =  curr_lr = [9.65752508361204e-06]
 [9.65752508361204e-06]
curr_lr = curr_lr =   [9.65752508361204e-06]curr_lr = 
[9.65752508361204e-06]
 [9.65752508361204e-06]
curr_lr = curr_lr =   [9.65752508361204e-06]
[9.65752508361204e-06]
curr_lr =  [9.65752508361204e-06]
curr_lr = curr_lr =   [9.6561872909699e-06]
[9.6561872909699e-06]
curr_lr = curr_lr =   curr_lr = [9.6561872909699e-06]
 [9.6561872909699e-06]
[9.6561872909699e-06]
curr_lr =  [9.6561872909699e-06]
curr_lr =  [9.6561872909699e-06]
curr_lr =  [9.6561872909699e-06]
curr_lr = curr_lr =   curr_lr = [9.65484949832776e-06]
curr_lr =  [9.65484949832776e-06]
 [9.65484949832776e-06]
[9.65484949832776e-06]
curr_lr =  curr_lr =  [9.65484949832776e-06]
[9.65484949832776e-06]
curr_lr =  [9.65484949832776e-06]
curr_lr =  [9.65484949832776e-06]
curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =   curr_lr =  [9.65351170568562e-06]Epoch: [0][296/500]       Time 148.613 (148.232)  Loss 3.6936 (8.2619)    VQALoss 0.0000 (0.0000) VGLoss 3.6936 (8.2619) 
 [9.65351170568562e-06]curr_lr = 
[9.65351170568562e-06][9.65351170568562e-06]

 
[9.65351170568562e-06][9.65351170568562e-06]

[9.65351170568562e-06]
curr_lr =  [9.65351170568562e-06]
curr_lr = curr_lr =   curr_lr = [9.652173913043478e-06][9.652173913043478e-06]

curr_lr =   [9.652173913043478e-06]
[9.652173913043478e-06]
curr_lr =  curr_lr =  [9.652173913043478e-06]
[9.652173913043478e-06]
curr_lr =  [9.652173913043478e-06]
curr_lr =  [9.652173913043478e-06]
curr_lr = curr_lr = curr_lr =    curr_lr =  [9.650836120401338e-06][9.650836120401338e-06]

[9.650836120401338e-06]
[9.650836120401338e-06]
curr_lr =  curr_lr = [9.650836120401338e-06] 
[9.650836120401338e-06]
curr_lr =  [9.650836120401338e-06]
curr_lr =  [9.650836120401338e-06]
curr_lr = curr_lr =  curr_lr =  [9.649498327759198e-06]
 [9.649498327759198e-06]
[9.649498327759198e-06]
curr_lr =  curr_lr = [9.649498327759198e-06]
 curr_lr =  [9.649498327759198e-06]
[9.649498327759198e-06]
curr_lr = curr_lr =   [9.649498327759198e-06][9.649498327759198e-06]

[2024-02-19 14:48:38,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=22, lr=[9.648829431438128e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr =  [9.648160535117058e-06]
[9.648160535117058e-06]
curr_lr =  [9.648160535117058e-06]
curr_lr =  [9.648160535117058e-06]
curr_lr =  [9.648160535117058e-06]
curr_lr =  [9.648160535117058e-06]
curr_lr =  [9.648160535117058e-06]
[2024-02-19 14:48:38,800] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=300, RunningAvgSamplesPerSec=16.90504019601811, CurrSamplesPerSec=17.181991393612194, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.648160535117058e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =      Epoch: [0][301/500]  Time 148.889 (148.503)  Loss 27.0273 (8.3228)   VQALoss 0.0000 (0.0000) VGLoss 27.0273 (8.3228)  
[9.646822742474918e-06][9.646822742474918e-06][9.646822742474918e-06]
[9.646822742474918e-06][9.646822742474918e-06]

[9.646822742474918e-06]


[9.646822742474918e-06]
curr_lr =  [9.646822742474918e-06]
[2024-02-19 14:51:12,063] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step301 is about to be saved!
[2024-02-19 14:51:24,085] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step301/mp_rank_00_model_states.pt
[2024-02-19 14:51:24,085] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step301/mp_rank_00_model_states.pt...
[2024-02-19 14:53:40,827] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step301/mp_rank_00_model_states.pt.
[2024-02-19 14:53:42,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step301/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-19 14:53:42,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step301/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-19 14:53:42,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step301/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-19 14:53:42,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step301/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-19 14:53:42,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step301/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-19 14:53:42,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step301/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-19 14:53:42,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step301/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-19 14:53:42,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step301/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-19 14:53:46,267] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step301/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-19 14:53:46,268] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step301/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-19 14:53:46,268] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step301 is ready now!
[2024-02-19 14:53:46,404] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step301/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-19 14:53:46,404] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step301/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-19 14:53:46,404] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step301 is ready now!
[2024-02-19 14:53:46,409] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step301/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-19 14:53:46,410] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step301/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-19 14:53:46,410] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step301 is ready now!
[2024-02-19 14:53:46,418] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step301/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-19 14:53:46,418] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step301/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-19 14:53:46,419] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step301 is ready now!
[2024-02-19 14:53:46,473] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step301/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-19 14:53:46,473] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step301/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-19 14:53:46,473] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step301 is ready now!
[2024-02-19 14:53:46,504] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step301/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-19 14:53:46,504] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step301/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-19 14:53:46,504] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step301 is ready now!
[2024-02-19 14:53:46,507] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step301/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-19 14:53:46,507] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step301/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-19 14:53:46,507] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step301 is ready now!
[2024-02-19 14:53:46,600] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step301/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-19 14:53:46,612] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step301/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-19 14:53:46,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step301 is ready now!
curr_lr = curr_lr =   curr_lr =  curr_lr = [9.645484949832776e-06][9.645484949832776e-06]

[9.645484949832776e-06]
 curr_lr =  [9.645484949832776e-06]
[9.645484949832776e-06]
curr_lr =  [9.645484949832776e-06]
curr_lr =  [9.645484949832776e-06]
curr_lr =  [9.645484949832776e-06]
curr_lr =  curr_lr = curr_lr = curr_lr = [9.644147157190636e-06] 
  [9.644147157190636e-06][9.644147157190636e-06]

[9.644147157190636e-06]
curr_lr = curr_lr =   [9.644147157190636e-06][9.644147157190636e-06]

curr_lr =  [9.644147157190636e-06]
curr_lr =  [9.644147157190636e-06]
curr_lr = curr_lr =   curr_lr = curr_lr =  [9.642809364548496e-06]
 [9.642809364548496e-06]
[9.642809364548496e-06]
[9.642809364548496e-06]
curr_lr =  curr_lr =  [9.642809364548496e-06]
[9.642809364548496e-06]
curr_lr =  [9.642809364548496e-06]
curr_lr =  [9.642809364548496e-06]
curr_lr = curr_lr = curr_lr =    curr_lr = [9.641471571906356e-06][9.641471571906356e-06][9.641471571906356e-06]


 [9.641471571906356e-06]
curr_lr = curr_lr =   [9.641471571906356e-06]
[9.641471571906356e-06]
curr_lr =  [9.641471571906356e-06]
curr_lr =  [9.641471571906356e-06]
curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr =  curr_lr =    [9.640133779264214e-06] [9.640133779264214e-06]
Epoch: [0][306/500]     Time 148.907 (180.075)  Loss 5.4743 (8.3914)    VQALoss 0.0000 (0.0000) VGLoss 5.4743 (8.3914)
[9.640133779264214e-06][9.640133779264214e-06]

[9.640133779264214e-06][9.640133779264214e-06]

[9.640133779264214e-06]

curr_lr =  [9.640133779264214e-06]
curr_lr = curr_lr =   curr_lr = curr_lr =  [9.638795986622074e-06][9.638795986622074e-06]
 
[9.638795986622074e-06]
[9.638795986622074e-06]
curr_lr =  [9.638795986622074e-06]
curr_lr =  [9.638795986622074e-06]
curr_lr =  curr_lr = [9.638795986622074e-06] 
[9.638795986622074e-06]
curr_lr = curr_lr =  curr_lr =   [9.637458193979934e-06]
[9.637458193979934e-06]
[9.637458193979934e-06]
curr_lr =  [9.637458193979934e-06]curr_lr = 
 [9.637458193979934e-06]
curr_lr =  [9.637458193979934e-06]
curr_lr =  [9.637458193979934e-06]
curr_lr =  [9.637458193979934e-06]
curr_lr = curr_lr =   curr_lr = [9.636120401337792e-06][9.636120401337792e-06]curr_lr = 

  [9.636120401337792e-06]
[9.636120401337792e-06]
curr_lr = curr_lr =   [9.636120401337792e-06]
[9.636120401337792e-06]
curr_lr =  [9.636120401337792e-06]
curr_lr =  [9.636120401337792e-06]
[2024-02-19 15:16:01,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=22, lr=[9.635451505016722e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr = curr_lr =    [9.634782608695654e-06]
[9.634782608695654e-06]
[9.634782608695654e-06]
curr_lr =  [9.634782608695654e-06]
curr_lr =  [9.634782608695654e-06]
curr_lr =  [9.634782608695654e-06]
curr_lr =  [9.634782608695654e-06]
[2024-02-19 15:16:02,085] [INFO] [timer.py:260:stop] epoch=0/micro_step=6200/global_step=310, RunningAvgSamplesPerSec=16.916282303129247, CurrSamplesPerSec=17.226814375516813, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.634782608695654e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =     curr_lr =  curr_lr = [9.633444816053512e-06][9.633444816053512e-06]  [9.633444816053512e-06]
[9.633444816053512e-06]
[9.633444816053512e-06]


[9.633444816053512e-06][9.633444816053512e-06]Epoch: [0][311/500]       Time 148.891 (148.582)  Loss 5.1884 (8.2717)    VQALoss 0.0000 (0.0000) VGLoss 5.1884 (8.2717)


curr_lr =  [9.633444816053512e-06]
curr_lr = curr_lr =   curr_lr = [9.632107023411372e-06]
[9.632107023411372e-06] 
curr_lr = curr_lr =   [9.632107023411372e-06]
curr_lr = [9.632107023411372e-06][9.632107023411372e-06]

 [9.632107023411372e-06]
curr_lr =  [9.632107023411372e-06]
curr_lr =  [9.632107023411372e-06]
curr_lr =  curr_lr = [9.630769230769232e-06]
curr_lr =   [9.630769230769232e-06]
[9.630769230769232e-06]
curr_lr = curr_lr =  curr_lr =   [9.630769230769232e-06][9.630769230769232e-06][9.630769230769232e-06]


curr_lr =  [9.630769230769232e-06]
curr_lr =  [9.630769230769232e-06]
curr_lr =  curr_lr =  [9.629431438127092e-06]curr_lr = 
curr_lr =  [9.629431438127092e-06] 
[9.629431438127092e-06][9.629431438127092e-06]

curr_lr =  [9.629431438127092e-06]
curr_lr =  [9.629431438127092e-06]
curr_lr =  [9.629431438127092e-06]
curr_lr =  [9.629431438127092e-06]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.62809364548495e-06][9.62809364548495e-06]

curr_lr =    curr_lr = [9.62809364548495e-06]
[9.62809364548495e-06]
 [9.62809364548495e-06]
[9.62809364548495e-06]
curr_lr =  [9.62809364548495e-06]
curr_lr =  [9.62809364548495e-06]
curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =   Epoch: [0][316/500]        Time 148.796 (148.153)  Loss 5.2936 (8.7739)    VQALoss 0.0000 (0.0000) VGLoss 5.2936 (8.7739) [9.62675585284281e-06]curr_lr = 

curr_lr =  [9.62675585284281e-06] [9.62675585284281e-06][9.62675585284281e-06]


 [9.62675585284281e-06]
[9.62675585284281e-06]
[9.62675585284281e-06]
curr_lr =  [9.62675585284281e-06]
curr_lr =  curr_lr = curr_lr =  [9.62541806020067e-06] curr_lr = 
 [9.62541806020067e-06]
[9.62541806020067e-06][9.62541806020067e-06]

curr_lr = curr_lr =   [9.62541806020067e-06]
[9.62541806020067e-06]
curr_lr =  [9.62541806020067e-06]
curr_lr =  [9.62541806020067e-06]
curr_lr =  curr_lr =  [9.624080267558528e-06]
[9.624080267558528e-06]
curr_lr =  [9.624080267558528e-06]
curr_lr = curr_lr =   curr_lr =  [9.624080267558528e-06][9.624080267558528e-06]

[9.624080267558528e-06]
curr_lr =  [9.624080267558528e-06]
curr_lr =  [9.624080267558528e-06]
curr_lr = curr_lr =   [9.62274247491639e-06]
curr_lr = [9.62274247491639e-06]
 curr_lr =  [9.62274247491639e-06]
[9.62274247491639e-06]
curr_lr =  curr_lr = [9.62274247491639e-06]
 [9.62274247491639e-06]
curr_lr =  [9.62274247491639e-06]
curr_lr =  [9.62274247491639e-06]
[2024-02-19 15:40:45,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=22, lr=[9.62207357859532e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.621404682274248e-06]
curr_lr =  curr_lr = [9.621404682274248e-06]curr_lr = 
  [9.621404682274248e-06]
[9.621404682274248e-06]
curr_lr =  [9.621404682274248e-06]
curr_lr =  [9.621404682274248e-06]
curr_lr =  [9.621404682274248e-06]
[2024-02-19 15:40:45,523] [INFO] [timer.py:260:stop] epoch=0/micro_step=6400/global_step=320, RunningAvgSamplesPerSec=16.92712642416368, CurrSamplesPerSec=17.217585779475282, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.621404682274248e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr =     Epoch: [0][321/500] Time 148.960 (148.548)  Loss 5.1051 (8.5442)    VQALoss 0.0000 (0.0000) VGLoss 5.1051 (8.5442) [9.620066889632108e-06][9.620066889632108e-06][9.620066889632108e-06][9.620066889632108e-06]
[9.620066889632108e-06]
[9.620066889632108e-06]


[9.620066889632108e-06]


curr_lr =  [9.620066889632108e-06]
curr_lr = curr_lr = curr_lr =    curr_lr = [9.618729096989968e-06]
 [9.618729096989968e-06][9.618729096989968e-06]

[9.618729096989968e-06]
curr_lr =  curr_lr = [9.618729096989968e-06] 
[9.618729096989968e-06]
curr_lr =  [9.618729096989968e-06]
curr_lr =  [9.618729096989968e-06]
curr_lr = curr_lr =  curr_lr =   [9.617391304347828e-06][9.617391304347828e-06]

[9.617391304347828e-06]
curr_lr =  curr_lr =  [9.617391304347828e-06]curr_lr = [9.617391304347828e-06]

 [9.617391304347828e-06]
curr_lr =  [9.617391304347828e-06]
curr_lr =  [9.617391304347828e-06]
curr_lr =  curr_lr = [9.616053511705686e-06]
 curr_lr =  [9.616053511705686e-06]
[9.616053511705686e-06]
curr_lr =  curr_lr =  curr_lr = [9.616053511705686e-06]
 [9.616053511705686e-06]
[9.616053511705686e-06]
curr_lr =  [9.616053511705686e-06]
curr_lr =  [9.616053511705686e-06]
curr_lr =  curr_lr = curr_lr =  [9.614715719063546e-06] 
[9.614715719063546e-06]
[9.614715719063546e-06]curr_lr = 
 [9.614715719063546e-06]
curr_lr = curr_lr =   [9.614715719063546e-06]
[9.614715719063546e-06]
curr_lr =  [9.614715719063546e-06]
curr_lr =  [9.614715719063546e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = Epoch: [0][326/500] Time 148.420 (148.208)  Loss 22.5529 (8.0709)   VQALoss 0.0000 (0.0000) VGLoss 22.5529 (8.0709)curr_lr = 
       [9.613377926421406e-06][9.613377926421406e-06][9.613377926421406e-06][9.613377926421406e-06][9.613377926421406e-06]


[9.613377926421406e-06]


[9.613377926421406e-06]
curr_lr =  [9.613377926421406e-06]
curr_lr =  [9.612040133779264e-06]
curr_lr = curr_lr =   [9.612040133779264e-06]
[9.612040133779264e-06]
curr_lr =  curr_lr =  [9.612040133779264e-06]
[9.612040133779264e-06]
curr_lr =  [9.612040133779264e-06]
curr_lr =  curr_lr = [9.612040133779264e-06] 
[9.612040133779264e-06]
curr_lr =  [9.610702341137125e-06]curr_lr = 
 [9.610702341137125e-06]
curr_lr = curr_lr =   [9.610702341137125e-06]
[9.610702341137125e-06]
curr_lr =  curr_lr =  [9.610702341137125e-06]
[9.610702341137125e-06]
curr_lr =  [9.610702341137125e-06]
curr_lr =  [9.610702341137125e-06]
curr_lr =  curr_lr =  curr_lr = [9.609364548494984e-06]
 [9.609364548494984e-06]
[9.609364548494984e-06]
curr_lr =  curr_lr = curr_lr = [9.609364548494984e-06] 
 [9.609364548494984e-06]
[9.609364548494984e-06]
curr_lr =  [9.609364548494984e-06]
curr_lr =  [9.609364548494984e-06]
[2024-02-19 16:05:28,208] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=22, lr=[9.608695652173914e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.608026755852844e-06]curr_lr = 
 curr_lr =  [9.608026755852844e-06]
[9.608026755852844e-06]
curr_lr = curr_lr =   curr_lr = [9.608026755852844e-06]
 [9.608026755852844e-06]
[9.608026755852844e-06]
curr_lr =  [9.608026755852844e-06]
[2024-02-19 16:05:28,408] [INFO] [timer.py:260:stop] epoch=0/micro_step=6600/global_step=330, RunningAvgSamplesPerSec=16.93752779242076, CurrSamplesPerSec=17.241781026483597, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.608026755852844e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    curr_lr =     Epoch: [0][331/500]        Time 148.570 (148.291)  Loss 6.1066 (8.0707)    VQALoss 0.0000 (0.0000) VGLoss 6.1066 (8.0707)[9.606688963210703e-06][9.606688963210703e-06][9.606688963210703e-06][9.606688963210703e-06][9.606688963210703e-06]

[9.606688963210703e-06]




[9.606688963210703e-06]
curr_lr =  [9.606688963210703e-06]
curr_lr =  curr_lr = [9.605351170568563e-06]
 [9.605351170568563e-06]
curr_lr =  [9.605351170568563e-06]
curr_lr =  curr_lr = curr_lr =   [9.605351170568563e-06]
[9.605351170568563e-06][9.605351170568563e-06]

curr_lr =  curr_lr =  [9.605351170568563e-06]
[9.605351170568563e-06]
curr_lr = curr_lr = curr_lr =    curr_lr =  [9.604013377926422e-06][9.604013377926422e-06]

[9.604013377926422e-06][9.604013377926422e-06]

curr_lr =  curr_lr = [9.604013377926422e-06] 
[9.604013377926422e-06]
curr_lr =  [9.604013377926422e-06]
curr_lr =  [9.604013377926422e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =    [9.602675585284281e-06]
[9.602675585284281e-06][9.602675585284281e-06]

[9.602675585284281e-06]
curr_lr =  [9.602675585284281e-06]
curr_lr = curr_lr =   [9.602675585284281e-06]
[9.602675585284281e-06]
curr_lr =  [9.602675585284281e-06]
curr_lr = curr_lr =  curr_lr =  curr_lr =  [9.601337792642141e-06]
 [9.601337792642141e-06]
[9.601337792642141e-06]
[9.601337792642141e-06]
curr_lr =  curr_lr =  [9.601337792642141e-06]
[9.601337792642141e-06]
curr_lr =  [9.601337792642141e-06]
curr_lr =  [9.601337792642141e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =   curr_lr =  curr_lr = Epoch: [0][336/500]   Time 150.116 (147.989)  Loss 3.2141 (8.1092)    VQALoss 0.0000 (0.0000) VGLoss 3.2141 (8.1092) [9.600000000000001e-06] 
 [9.600000000000001e-06][9.600000000000001e-06][9.600000000000001e-06]


[9.600000000000001e-06]
[9.600000000000001e-06]

[9.600000000000001e-06]
curr_lr =  [9.600000000000001e-06]
curr_lr =  [9.598662207357861e-06]curr_lr = 
 curr_lr = [9.598662207357861e-06]
 curr_lr = [9.598662207357861e-06]
 [9.598662207357861e-06]curr_lr = 
curr_lr =   [9.598662207357861e-06]
[9.598662207357861e-06]
curr_lr =  [9.598662207357861e-06]
curr_lr =  [9.598662207357861e-06]
curr_lr =  curr_lr = curr_lr =   [9.59732441471572e-06]
curr_lr =  [9.59732441471572e-06][9.59732441471572e-06]

[9.59732441471572e-06]
curr_lr = curr_lr =   [9.59732441471572e-06]
[9.59732441471572e-06]
curr_lr =  [9.59732441471572e-06]
curr_lr =  [9.59732441471572e-06]
curr_lr = curr_lr =   [9.59598662207358e-06]
[9.59598662207358e-06]
curr_lr = curr_lr =   [9.59598662207358e-06]
[9.59598662207358e-06]
curr_lr = curr_lr =   [9.59598662207358e-06]
[9.59598662207358e-06]
curr_lr =  curr_lr =  [9.59598662207358e-06]
[9.59598662207358e-06]
[2024-02-19 16:30:10,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=22, lr=[9.59531772575251e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr =  [9.59464882943144e-06]
curr_lr =  [9.59464882943144e-06]
[9.59464882943144e-06]
curr_lr =  curr_lr = [9.59464882943144e-06] 
curr_lr = [9.59464882943144e-06]
 [9.59464882943144e-06]
curr_lr =  [9.59464882943144e-06]
[2024-02-19 16:30:10,483] [INFO] [timer.py:260:stop] epoch=0/micro_step=6800/global_step=340, RunningAvgSamplesPerSec=16.947586070485126, CurrSamplesPerSec=17.23666728456409, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.59464882943144e-06]
curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =  curr_lr = curr_lr =    [9.593311036789299e-06] [9.593311036789299e-06]

 [9.593311036789299e-06][9.593311036789299e-06]

[9.593311036789299e-06]
[9.593311036789299e-06][9.593311036789299e-06]

Epoch: [0][341/500]     Time 148.581 (148.428)  Loss 5.8518 (7.9202)    VQALoss 0.0000 (0.0000) VGLoss 5.8518 (7.9202)
curr_lr =  [9.593311036789299e-06]
curr_lr = curr_lr =   curr_lr =  [9.591973244147157e-06]curr_lr = 
[9.591973244147157e-06]
 [9.591973244147157e-06]
[9.591973244147157e-06]
curr_lr = curr_lr =   [9.591973244147157e-06][9.591973244147157e-06]

curr_lr =  [9.591973244147157e-06]
curr_lr =  [9.591973244147157e-06]
curr_lr =  [9.590635451505017e-06]
curr_lr = curr_lr =   curr_lr =  [9.590635451505017e-06][9.590635451505017e-06]

[9.590635451505017e-06]
curr_lr = curr_lr =   [9.590635451505017e-06][9.590635451505017e-06]

curr_lr =  [9.590635451505017e-06]
curr_lr =  [9.590635451505017e-06]
curr_lr =  curr_lr = curr_lr =  [9.589297658862877e-06]
 [9.589297658862877e-06]
curr_lr =  [9.589297658862877e-06]
[9.589297658862877e-06]
curr_lr = curr_lr =   [9.589297658862877e-06][9.589297658862877e-06]

curr_lr =  curr_lr =  [9.589297658862877e-06]
[9.589297658862877e-06]
curr_lr =  curr_lr = curr_lr = [9.587959866220737e-06]
  curr_lr = [9.587959866220737e-06][9.587959866220737e-06]
 
[9.587959866220737e-06]
curr_lr = curr_lr =   [9.587959866220737e-06]
[9.587959866220737e-06]
curr_lr =  [9.587959866220737e-06]
curr_lr =  [9.587959866220737e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =       Epoch: [0][346/500]        Time 147.496 (148.213)  Loss 10.8258 (8.2108)   VQALoss 0.0000 (0.0000) VGLoss 10.8258 (8.2108)[9.586622073578597e-06][9.586622073578597e-06]
[9.586622073578597e-06][9.586622073578597e-06]
[9.586622073578597e-06]



[9.586622073578597e-06]
[9.586622073578597e-06]
curr_lr =  [9.586622073578597e-06]
curr_lr =  curr_lr = [9.585284280936455e-06]
 [9.585284280936455e-06]
curr_lr =  curr_lr = curr_lr =   [9.585284280936455e-06]curr_lr = 
[9.585284280936455e-06]
 [9.585284280936455e-06]
[9.585284280936455e-06]
curr_lr =  [9.585284280936455e-06]
curr_lr =  [9.585284280936455e-06]
curr_lr =  curr_lr = curr_lr =  [9.583946488294315e-06]
 [9.583946488294315e-06][9.583946488294315e-06]

curr_lr =  [9.583946488294315e-06]
curr_lr = curr_lr =   [9.583946488294315e-06]
[9.583946488294315e-06]
curr_lr =  [9.583946488294315e-06]
curr_lr =  [9.583946488294315e-06]
curr_lr =  curr_lr =  curr_lr = [9.582608695652175e-06]
 [9.582608695652175e-06]
[9.582608695652175e-06]
curr_lr =  [9.582608695652175e-06]
curr_lr =  [9.582608695652175e-06]
curr_lr =  [9.582608695652175e-06]
curr_lr =  [9.582608695652175e-06]
curr_lr =  [9.582608695652175e-06]
[2024-02-19 16:54:53,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=22, lr=[9.581939799331105e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = curr_lr =   [9.581270903010033e-06]
[9.581270903010033e-06]
[9.581270903010033e-06]
curr_lr =  [9.581270903010033e-06]
curr_lr =  [9.581270903010033e-06]
curr_lr =  [9.581270903010033e-06]
curr_lr =  [9.581270903010033e-06]
[2024-02-19 16:54:53,330] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=350, RunningAvgSamplesPerSec=16.95681857723389, CurrSamplesPerSec=17.23207572876977, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.581270903010033e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = Epoch: [0][351/500]        Time 149.278 (148.496)  Loss 3.5195 (8.6683)    VQALoss 0.0000 (0.0000) VGLoss 3.5195 (8.6683)    
 curr_lr = [9.579933110367893e-06][9.579933110367893e-06] 

[9.579933110367893e-06][9.579933110367893e-06][9.579933110367893e-06][9.579933110367893e-06]



[9.579933110367893e-06]
curr_lr =  [9.579933110367893e-06]
[2024-02-19 16:57:27,062] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step351 is about to be saved!
[2024-02-19 16:57:39,123] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step351/mp_rank_00_model_states.pt
[2024-02-19 16:57:39,123] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step351/mp_rank_00_model_states.pt...
[2024-02-19 16:59:56,543] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step351/mp_rank_00_model_states.pt.
[2024-02-19 16:59:57,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step351/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-19 16:59:57,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step351/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-19 16:59:57,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step351/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-19 16:59:57,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step351/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-19 16:59:57,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step351/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-19 16:59:57,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step351/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-19 16:59:57,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step351/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-19 16:59:57,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step351/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-19 17:00:01,889] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step351/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-19 17:00:01,889] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step351/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-19 17:00:01,889] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step351 is ready now!
[2024-02-19 17:00:01,922] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step351/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-19 17:00:01,922] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step351/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-19 17:00:01,922] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step351 is ready now!
[2024-02-19 17:00:01,953] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step351/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-19 17:00:01,953] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step351/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-19 17:00:01,953] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step351 is ready now!
[2024-02-19 17:00:02,002] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step351/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-19 17:00:02,003] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step351/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-19 17:00:02,003] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step351 is ready now!
[2024-02-19 17:00:02,130] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step351/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-19 17:00:02,130] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step351/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-19 17:00:02,130] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step351 is ready now!
[2024-02-19 17:00:02,189] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step351/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-19 17:00:02,189] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step351/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-19 17:00:02,189] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step351 is ready now!
[2024-02-19 17:00:02,276] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step351/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-19 17:00:02,276] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step351/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-19 17:00:02,276] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step351 is ready now!
[2024-02-19 17:00:02,288] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step351/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-19 17:00:02,300] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step351/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-19 17:00:02,300] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step351 is ready now!
curr_lr = curr_lr = curr_lr = curr_lr =     [9.578595317725753e-06][9.578595317725753e-06][9.578595317725753e-06]


[9.578595317725753e-06]
curr_lr =  curr_lr = [9.578595317725753e-06]
 [9.578595317725753e-06]
curr_lr =  [9.578595317725753e-06]
curr_lr =  [9.578595317725753e-06]
curr_lr = curr_lr = curr_lr =    [9.577257525083613e-06][9.577257525083613e-06][9.577257525083613e-06]


curr_lr =  curr_lr = [9.577257525083613e-06]
 [9.577257525083613e-06]
curr_lr =  [9.577257525083613e-06]
curr_lr =  [9.577257525083613e-06]
curr_lr =  [9.577257525083613e-06]
curr_lr = curr_lr = curr_lr =    [9.575919732441473e-06][9.575919732441473e-06]

[9.575919732441473e-06]
curr_lr = curr_lr =   curr_lr = [9.575919732441473e-06]
[9.575919732441473e-06]
 [9.575919732441473e-06]
curr_lr =  [9.575919732441473e-06]
curr_lr =  [9.575919732441473e-06]
curr_lr =  curr_lr = [9.574581939799333e-06]curr_lr = 
  curr_lr = [9.574581939799333e-06] [9.574581939799333e-06]

[9.574581939799333e-06]
curr_lr =  [9.574581939799333e-06]curr_lr = 
 [9.574581939799333e-06]
curr_lr =  [9.574581939799333e-06]
curr_lr =  [9.574581939799333e-06]
curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr = curr_lr =  curr_lr =     [9.573244147157191e-06]Epoch: [0][356/500]  Time 148.112 (180.205)  Loss 4.3735 (8.5737)    VQALoss 0.0000 (0.0000) VGLoss 4.3735 (8.5737)
 
[9.573244147157191e-06]
[9.573244147157191e-06][9.573244147157191e-06][9.573244147157191e-06]


[9.573244147157191e-06][9.573244147157191e-06]

curr_lr =  [9.573244147157191e-06]
curr_lr = curr_lr =   [9.57190635451505e-06]
[9.57190635451505e-06]
curr_lr =  curr_lr =  [9.57190635451505e-06]
[9.57190635451505e-06]
curr_lr =  curr_lr = [9.57190635451505e-06]
 [9.57190635451505e-06]
curr_lr =  [9.57190635451505e-06]
curr_lr =  [9.57190635451505e-06]
curr_lr =  curr_lr = curr_lr = [9.57056856187291e-06] 
 [9.57056856187291e-06]
[9.57056856187291e-06]
curr_lr =  [9.57056856187291e-06]
curr_lr =  curr_lr = [9.57056856187291e-06]
 [9.57056856187291e-06]
curr_lr =  [9.57056856187291e-06]
curr_lr =  [9.57056856187291e-06]
curr_lr = curr_lr =   [9.569230769230769e-06][9.569230769230769e-06]

curr_lr = curr_lr =   [9.569230769230769e-06][9.569230769230769e-06]

curr_lr =  [9.569230769230769e-06]
curr_lr =  [9.569230769230769e-06]
curr_lr =  [9.569230769230769e-06]
curr_lr =  [9.569230769230769e-06]
[2024-02-19 17:22:16,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=22, lr=[9.568561872909699e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr =  [9.567892976588629e-06]
[9.567892976588629e-06]
curr_lr = curr_lr =   [9.567892976588629e-06]
[9.567892976588629e-06]curr_lr = 
 curr_lr =  [9.567892976588629e-06]
[9.567892976588629e-06]
curr_lr =  [9.567892976588629e-06]
[2024-02-19 17:22:16,940] [INFO] [timer.py:260:stop] epoch=0/micro_step=7200/global_step=360, RunningAvgSamplesPerSec=16.96522338484258, CurrSamplesPerSec=17.265656207433853, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.567892976588629e-06]
curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =    Epoch: [0][361/500]       Time 149.391 (148.540)  Loss 23.7298 (7.8947)   VQALoss 0.0000 (0.0000) VGLoss 23.7298 (7.8947) [9.566555183946489e-06]
curr_lr = 
[9.566555183946489e-06][9.566555183946489e-06][9.566555183946489e-06]


curr_lr =  [9.566555183946489e-06]
 [9.566555183946489e-06]
[9.566555183946489e-06]
curr_lr =  [9.566555183946489e-06]
curr_lr =  curr_lr = curr_lr =  [9.565217391304349e-06]
 curr_lr = [9.565217391304349e-06] [9.565217391304349e-06]

[9.565217391304349e-06]
curr_lr =  curr_lr = [9.565217391304349e-06]
 [9.565217391304349e-06]
curr_lr =  [9.565217391304349e-06]
curr_lr =  [9.565217391304349e-06]
curr_lr = curr_lr = curr_lr =    curr_lr = [9.563879598662209e-06][9.563879598662209e-06][9.563879598662209e-06]
 

[9.563879598662209e-06]
curr_lr =  [9.563879598662209e-06]
curr_lr =  [9.563879598662209e-06]
curr_lr =  [9.563879598662209e-06]
curr_lr =  [9.563879598662209e-06]
curr_lr = curr_lr = curr_lr =   curr_lr =   [9.562541806020068e-06][9.562541806020068e-06]

[9.562541806020068e-06][9.562541806020068e-06]

curr_lr =  [9.562541806020068e-06]
curr_lr =  curr_lr = [9.562541806020068e-06]
 [9.562541806020068e-06]
curr_lr =  [9.562541806020068e-06]
curr_lr =  [9.561204013377927e-06]
curr_lr =  curr_lr = curr_lr =  [9.561204013377927e-06] 
[9.561204013377927e-06]curr_lr = 
 [9.561204013377927e-06]
[9.561204013377927e-06]
curr_lr =  [9.561204013377927e-06]
curr_lr =  curr_lr = [9.561204013377927e-06]
 [9.561204013377927e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =      curr_lr =   [9.559866220735787e-06][9.559866220735787e-06][9.559866220735787e-06][9.559866220735787e-06]Epoch: [0][366/500]    Time 147.602 (148.326)  Loss 2.5288 (8.5062)    VQALoss 0.0000 (0.0000) VGLoss 2.5288 (8.5062)
[9.559866220735787e-06]
[9.559866220735787e-06]


[9.559866220735787e-06]


curr_lr =  [9.559866220735787e-06]
curr_lr =  curr_lr = curr_lr = [9.558528428093647e-06]
 curr_lr =  curr_lr =   [9.558528428093647e-06]
[9.558528428093647e-06]
[9.558528428093647e-06][9.558528428093647e-06]

curr_lr =  [9.558528428093647e-06]
curr_lr =  [9.558528428093647e-06]
curr_lr =  [9.558528428093647e-06]
curr_lr = curr_lr =   [9.557190635451505e-06]
[9.557190635451505e-06]curr_lr = 
curr_lr =   [9.557190635451505e-06]
[9.557190635451505e-06]
curr_lr =  curr_lr = [9.557190635451505e-06]
 [9.557190635451505e-06]
curr_lr =  [9.557190635451505e-06]
curr_lr =  [9.557190635451505e-06]
curr_lr = curr_lr =   [9.555852842809366e-06]
curr_lr = [9.555852842809366e-06]
 curr_lr = [9.555852842809366e-06]
 [9.555852842809366e-06]
curr_lr =  [9.555852842809366e-06]
curr_lr =  curr_lr = [9.555852842809366e-06]
 [9.555852842809366e-06]
curr_lr =  [9.555852842809366e-06]
[2024-02-19 17:47:01,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=22, lr=[9.555183946488296e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr = curr_lr =    curr_lr = [9.554515050167225e-06][9.554515050167225e-06][9.554515050167225e-06]


 [9.554515050167225e-06]
curr_lr =  [9.554515050167225e-06]
curr_lr =  [9.554515050167225e-06]
curr_lr =  [9.554515050167225e-06]
[2024-02-19 17:47:01,375] [INFO] [timer.py:260:stop] epoch=0/micro_step=7400/global_step=370, RunningAvgSamplesPerSec=16.97303792940301, CurrSamplesPerSec=17.188625785021724, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.554515050167225e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =    curr_lr = Epoch: [0][371/500]     Time 148.941 (148.474)  Loss 18.7332 (8.2973)   VQALoss 0.0000 (0.0000) VGLoss 18.7332 (8.2973) curr_lr = [9.553177257525084e-06][9.553177257525084e-06] 


[9.553177257525084e-06][9.553177257525084e-06] [9.553177257525084e-06]


[9.553177257525084e-06]
[9.553177257525084e-06]
curr_lr =  [9.553177257525084e-06]
curr_lr = curr_lr = curr_lr =    [9.551839464882944e-06]
[9.551839464882944e-06]
[9.551839464882944e-06]
curr_lr = curr_lr =  curr_lr =  [9.551839464882944e-06] 
[9.551839464882944e-06]
[9.551839464882944e-06]
curr_lr =  [9.551839464882944e-06]
curr_lr =  [9.551839464882944e-06]
curr_lr = curr_lr =  curr_lr =   [9.550501672240804e-06][9.550501672240804e-06]

[9.550501672240804e-06]
curr_lr = curr_lr =   [9.550501672240804e-06]curr_lr = 
[9.550501672240804e-06]
 [9.550501672240804e-06]
curr_lr =  [9.550501672240804e-06]
curr_lr =  [9.550501672240804e-06]
curr_lr = curr_lr =   [9.549163879598662e-06]
[9.549163879598662e-06]
curr_lr = curr_lr =   [9.549163879598662e-06][9.549163879598662e-06]

curr_lr = curr_lr =   [9.549163879598662e-06]
[9.549163879598662e-06]
curr_lr =  [9.549163879598662e-06]
curr_lr =  [9.549163879598662e-06]
curr_lr = curr_lr =   curr_lr = [9.547826086956522e-06][9.547826086956522e-06]
 
[9.547826086956522e-06]
curr_lr =  curr_lr = [9.547826086956522e-06]
curr_lr =   [9.547826086956522e-06]
[9.547826086956522e-06]
curr_lr =  [9.547826086956522e-06]
curr_lr =  [9.547826086956522e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr =      [9.546488294314382e-06][9.546488294314382e-06][9.546488294314382e-06]


[9.546488294314382e-06][9.546488294314382e-06][9.546488294314382e-06]


[9.546488294314382e-06]
Epoch: [0][376/500]     Time 147.318 (148.202)  Loss 3.0157 (7.8005)    VQALoss 0.0000 (0.0000) VGLoss 3.0157 (7.8005)
curr_lr =  [9.546488294314382e-06]
curr_lr =  [9.54515050167224e-06]
curr_lr = curr_lr = curr_lr =    [9.54515050167224e-06][9.54515050167224e-06]

[9.54515050167224e-06]
curr_lr =  [9.54515050167224e-06]
curr_lr =  [9.54515050167224e-06]
curr_lr =  [9.54515050167224e-06]
curr_lr =  [9.54515050167224e-06]
curr_lr = curr_lr = curr_lr =    [9.543812709030102e-06]
[9.543812709030102e-06][9.543812709030102e-06]

curr_lr =  curr_lr =  [9.543812709030102e-06]
[9.543812709030102e-06]
curr_lr =  [9.543812709030102e-06]
curr_lr =  [9.543812709030102e-06]
curr_lr =  [9.543812709030102e-06]
curr_lr = curr_lr =   curr_lr = [9.54247491638796e-06][9.54247491638796e-06]

 curr_lr =  [9.54247491638796e-06]
[9.54247491638796e-06]
curr_lr = curr_lr =   [9.54247491638796e-06]
[9.54247491638796e-06]
curr_lr =  [9.54247491638796e-06]
curr_lr =  [9.54247491638796e-06]
[2024-02-19 18:11:44,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=22, lr=[9.54180602006689e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.54113712374582e-06]curr_lr = 
 curr_lr = [9.54113712374582e-06] 
[9.54113712374582e-06]
curr_lr =  curr_lr = [9.54113712374582e-06]curr_lr = 
  [9.54113712374582e-06]
[9.54113712374582e-06]
curr_lr =  [9.54113712374582e-06]
[2024-02-19 18:11:44,993] [INFO] [timer.py:260:stop] epoch=0/micro_step=7600/global_step=380, RunningAvgSamplesPerSec=16.980731328844623, CurrSamplesPerSec=17.200774261964543, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.54113712374582e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr =   curr_lr =  Epoch: [0][381/500]  Time 148.856 (148.502)  Loss 24.4246 (7.4565)   VQALoss 0.0000 (0.0000) VGLoss 24.4246 (7.4565) [9.53979933110368e-06] [9.53979933110368e-06]
[9.53979933110368e-06]
[9.53979933110368e-06]


[9.53979933110368e-06][9.53979933110368e-06]

[9.53979933110368e-06]
curr_lr =  [9.53979933110368e-06]
curr_lr = curr_lr =   curr_lr = [9.53846153846154e-06][9.53846153846154e-06] 

curr_lr = [9.53846153846154e-06]
 curr_lr = [9.53846153846154e-06] curr_lr = 
 [9.53846153846154e-06]
[9.53846153846154e-06]
curr_lr =  [9.53846153846154e-06]
curr_lr =  [9.53846153846154e-06]
curr_lr =  curr_lr = curr_lr =   [9.537123745819398e-06]
[9.537123745819398e-06]
[9.537123745819398e-06]curr_lr = 
 [9.537123745819398e-06]
curr_lr = curr_lr =   [9.537123745819398e-06][9.537123745819398e-06]

curr_lr =  [9.537123745819398e-06]
curr_lr =  [9.537123745819398e-06]
curr_lr = curr_lr =   [9.535785953177258e-06][9.535785953177258e-06]

curr_lr = curr_lr =   [9.535785953177258e-06][9.535785953177258e-06]

curr_lr =  curr_lr =  [9.535785953177258e-06]
[9.535785953177258e-06]
curr_lr =  [9.535785953177258e-06]
curr_lr =  [9.535785953177258e-06]
curr_lr =  curr_lr = [9.534448160535118e-06]
 curr_lr = [9.534448160535118e-06] 
[9.534448160535118e-06]
curr_lr =  [9.534448160535118e-06]
curr_lr =  [9.534448160535118e-06]
curr_lr =  [9.534448160535118e-06]
curr_lr =  [9.534448160535118e-06]
curr_lr =  [9.534448160535118e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr =      [9.533110367892976e-06][9.533110367892976e-06]

[9.533110367892976e-06][9.533110367892976e-06][9.533110367892976e-06]
[9.533110367892976e-06]

[9.533110367892976e-06]

Epoch: [0][386/500]     Time 148.182 (148.478)  Loss 4.3530 (8.1811)    VQALoss 0.0000 (0.0000) VGLoss 4.3530 (8.1811)
curr_lr =  [9.533110367892976e-06]
curr_lr = curr_lr =  curr_lr =   [9.531772575250838e-06]
[9.531772575250838e-06]
[9.531772575250838e-06]
curr_lr =  curr_lr = [9.531772575250838e-06] 
[9.531772575250838e-06]
curr_lr =  curr_lr = [9.531772575250838e-06]
 [9.531772575250838e-06]
curr_lr =  [9.531772575250838e-06]
curr_lr = curr_lr =   [9.530434782608696e-06][9.530434782608696e-06]curr_lr = curr_lr = 

  [9.530434782608696e-06]curr_lr = [9.530434782608696e-06]

 [9.530434782608696e-06]
curr_lr =  [9.530434782608696e-06]
curr_lr =  [9.530434782608696e-06]
curr_lr =  [9.530434782608696e-06]
curr_lr = curr_lr =   [9.529096989966556e-06]
[9.529096989966556e-06]
curr_lr =  [9.529096989966556e-06]
curr_lr = curr_lr = curr_lr =    [9.529096989966556e-06][9.529096989966556e-06]

[9.529096989966556e-06]
curr_lr =  [9.529096989966556e-06]
curr_lr =  [9.529096989966556e-06]
[2024-02-19 18:36:30,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=22, lr=[9.528428093645486e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = [9.527759197324416e-06]
curr_lr =   [9.527759197324416e-06]
[9.527759197324416e-06]curr_lr = 
curr_lr =   curr_lr = [9.527759197324416e-06][9.527759197324416e-06]

 [9.527759197324416e-06]
curr_lr =  [9.527759197324416e-06]
[2024-02-19 18:36:30,257] [INFO] [timer.py:260:stop] epoch=0/micro_step=7800/global_step=390, RunningAvgSamplesPerSec=16.987502340842454, CurrSamplesPerSec=17.195345985839182, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.527759197324416e-06]
curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr = curr_lr =     [9.526421404682274e-06]
 [9.526421404682274e-06][9.526421404682274e-06]
[9.526421404682274e-06][9.526421404682274e-06]

curr_lr = 
[9.526421404682274e-06]
 Epoch: [0][391/500]    Time 148.614 (148.526)  Loss 5.1217 (8.1084)    VQALoss 0.0000 (0.0000) VGLoss 5.1217 (8.1084)[9.526421404682274e-06]

curr_lr =  [9.526421404682274e-06]
curr_lr = curr_lr =   curr_lr = [9.525083612040134e-06][9.525083612040134e-06] 

[9.525083612040134e-06]
curr_lr =  [9.525083612040134e-06]curr_lr = 
 [9.525083612040134e-06]
curr_lr =  [9.525083612040134e-06]
curr_lr =  [9.525083612040134e-06]
curr_lr =  [9.525083612040134e-06]
curr_lr =  [9.523745819397994e-06]
curr_lr =  curr_lr = [9.523745819397994e-06] 
[9.523745819397994e-06]
curr_lr = curr_lr =   curr_lr = [9.523745819397994e-06][9.523745819397994e-06] 

[9.523745819397994e-06]
curr_lr =  [9.523745819397994e-06]
curr_lr =  [9.523745819397994e-06]
curr_lr =  curr_lr =  [9.522408026755854e-06]curr_lr = 
curr_lr =   [9.522408026755854e-06]
[9.522408026755854e-06][9.522408026755854e-06]

curr_lr =  curr_lr =  [9.522408026755854e-06]
[9.522408026755854e-06]
curr_lr =  [9.522408026755854e-06]
curr_lr =  [9.522408026755854e-06]
curr_lr =  [9.521070234113712e-06]
curr_lr = curr_lr =   [9.521070234113712e-06][9.521070234113712e-06]

curr_lr =  curr_lr = [9.521070234113712e-06] 
[9.521070234113712e-06]
curr_lr =  [9.521070234113712e-06]
curr_lr =  [9.521070234113712e-06]
curr_lr =  [9.521070234113712e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr = curr_lr =  Epoch: [0][396/500]     Time 147.590 (148.200)  Loss 5.7870 (8.2545)    VQALoss 0.0000 (0.0000) VGLoss 5.7870 (8.2545)   [9.519732441471574e-06] 
 
[9.519732441471574e-06]
[9.519732441471574e-06]
[9.519732441471574e-06][9.519732441471574e-06]

[9.519732441471574e-06]
[9.519732441471574e-06]
curr_lr =  [9.519732441471574e-06]
curr_lr = curr_lr =   [9.518394648829432e-06]
curr_lr = [9.518394648829432e-06]
 [9.518394648829432e-06]
curr_lr = curr_lr =   [9.518394648829432e-06][9.518394648829432e-06]

curr_lr =  [9.518394648829432e-06]
curr_lr =  [9.518394648829432e-06]
curr_lr =  [9.518394648829432e-06]
curr_lr =  curr_lr =  [9.517056856187292e-06]
[9.517056856187292e-06]curr_lr = 
 [9.517056856187292e-06]
curr_lr =  curr_lr = [9.517056856187292e-06]
 [9.517056856187292e-06]
curr_lr =  [9.517056856187292e-06]
curr_lr =  [9.517056856187292e-06]
curr_lr =  [9.517056856187292e-06]
curr_lr = curr_lr =   curr_lr = curr_lr =  [9.515719063545152e-06][9.515719063545152e-06]

[9.515719063545152e-06]
 [9.515719063545152e-06]
curr_lr = curr_lr =   [9.515719063545152e-06][9.515719063545152e-06]

curr_lr =  [9.515719063545152e-06]
curr_lr =  [9.515719063545152e-06]
[2024-02-19 19:01:13,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=22, lr=[9.515050167224082e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.51438127090301e-06]
curr_lr =  [9.51438127090301e-06]
curr_lr =  [9.51438127090301e-06]
curr_lr = curr_lr =   [9.51438127090301e-06][9.51438127090301e-06]

curr_lr =  curr_lr = [9.51438127090301e-06] 
[9.51438127090301e-06]
[2024-02-19 19:01:13,924] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=400, RunningAvgSamplesPerSec=16.994419754128714, CurrSamplesPerSec=17.196359199305924, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.51438127090301e-06]
curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr =    [9.51304347826087e-06][9.51304347826087e-06][9.51304347826087e-06]curr_lr =  

Epoch: [0][401/500]     Time 148.795 (148.572)  Loss 4.3197 (8.4915)    VQALoss 0.0000 (0.0000) VGLoss 4.3197 (8.4915)
[9.51304347826087e-06][9.51304347826087e-06] 


[9.51304347826087e-06]
[9.51304347826087e-06]
curr_lr =  [9.51304347826087e-06]
[2024-02-19 19:03:47,300] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step401 is about to be saved!
[2024-02-19 19:03:59,533] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step401/mp_rank_00_model_states.pt
[2024-02-19 19:03:59,534] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step401/mp_rank_00_model_states.pt...
[2024-02-19 19:06:16,026] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step401/mp_rank_00_model_states.pt.
[2024-02-19 19:06:17,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step401/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-19 19:06:17,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step401/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-19 19:06:17,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step401/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-19 19:06:17,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step401/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-19 19:06:17,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step401/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-19 19:06:17,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step401/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-19 19:06:17,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step401/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-19 19:06:17,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step401/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-19 19:06:21,385] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step401/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-19 19:06:21,385] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step401/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-19 19:06:21,385] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step401 is ready now!
[2024-02-19 19:06:21,388] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step401/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-19 19:06:21,389] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step401/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-19 19:06:21,389] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step401 is ready now!
[2024-02-19 19:06:21,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step401/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-19 19:06:21,415] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step401/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-19 19:06:21,415] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step401 is ready now!
[2024-02-19 19:06:21,430] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step401/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-19 19:06:21,430] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step401/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-19 19:06:21,430] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step401 is ready now!
[2024-02-19 19:06:21,679] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step401/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-19 19:06:21,679] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step401/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-19 19:06:21,679] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step401 is ready now!
[2024-02-19 19:06:21,717] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step401/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-19 19:06:21,717] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step401/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-19 19:06:21,717] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step401 is ready now!
[2024-02-19 19:06:21,720] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step401/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-19 19:06:21,722] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step401/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-19 19:06:21,722] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step401/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-19 19:06:21,723] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step401 is ready now!
[2024-02-19 19:06:21,729] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step401/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-19 19:06:21,729] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step401 is ready now!
curr_lr = curr_lr =   curr_lr = curr_lr =  [9.51170568561873e-06] 
[9.51170568561873e-06]
[9.51170568561873e-06]
[9.51170568561873e-06]
curr_lr =  [9.51170568561873e-06]
curr_lr =  [9.51170568561873e-06]
curr_lr =  [9.51170568561873e-06]
curr_lr =  [9.51170568561873e-06]
curr_lr =  curr_lr = [9.51036789297659e-06]curr_lr = 
  [9.51036789297659e-06][9.51036789297659e-06]curr_lr = 

 [9.51036789297659e-06]
curr_lr =  curr_lr = [9.51036789297659e-06] 
[9.51036789297659e-06]
curr_lr =  [9.51036789297659e-06]
curr_lr =  [9.51036789297659e-06]
curr_lr = curr_lr =  curr_lr =   [9.50903010033445e-06]curr_lr = 
 [9.50903010033445e-06]
[9.50903010033445e-06]
[9.50903010033445e-06]
curr_lr =  [9.50903010033445e-06]
curr_lr =  [9.50903010033445e-06]
curr_lr =  [9.50903010033445e-06]
curr_lr =  [9.50903010033445e-06]
curr_lr =  curr_lr = curr_lr = curr_lr =  [9.50769230769231e-06]  
[9.50769230769231e-06]
[9.50769230769231e-06]
[9.50769230769231e-06]
curr_lr =  curr_lr =  [9.50769230769231e-06]
[9.50769230769231e-06]
curr_lr =  [9.50769230769231e-06]
curr_lr =  [9.50769230769231e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =  curr_lr =   Epoch: [0][406/500]   Time 147.732 (180.121)  Loss 6.6816 (7.7056)    VQALoss 0.0000 (0.0000) VGLoss 6.6816 (7.7056) [9.506354515050168e-06] 

[9.506354515050168e-06] 
[9.506354515050168e-06][9.506354515050168e-06]

[9.506354515050168e-06]
[9.506354515050168e-06][9.506354515050168e-06]

curr_lr =  [9.506354515050168e-06]
curr_lr = curr_lr =  curr_lr =  curr_lr =  [9.505016722408028e-06]
 [9.505016722408028e-06]
[9.505016722408028e-06][9.505016722408028e-06]

curr_lr =  [9.505016722408028e-06]
curr_lr =  [9.505016722408028e-06]
curr_lr =  [9.505016722408028e-06]
curr_lr =  [9.505016722408028e-06]
curr_lr = curr_lr =  curr_lr =  curr_lr =  [9.503678929765887e-06][9.503678929765887e-06]

 [9.503678929765887e-06]
[9.503678929765887e-06]
curr_lr = curr_lr =   [9.503678929765887e-06][9.503678929765887e-06]

curr_lr =  [9.503678929765887e-06]
curr_lr =  [9.503678929765887e-06]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.502341137123746e-06] 
 [9.502341137123746e-06]
[9.502341137123746e-06][9.502341137123746e-06]

curr_lr =  curr_lr = [9.502341137123746e-06]
 [9.502341137123746e-06]
curr_lr =  [9.502341137123746e-06]
curr_lr =  [9.502341137123746e-06]
[2024-02-19 19:28:37,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=22, lr=[9.501672240802676e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr =  curr_lr = [9.501003344481606e-06]
 [9.501003344481606e-06]curr_lr = 
[9.501003344481606e-06] 
[9.501003344481606e-06]
curr_lr =  [9.501003344481606e-06]curr_lr = 
 [9.501003344481606e-06]
curr_lr =  [9.501003344481606e-06]
[2024-02-19 19:28:37,333] [INFO] [timer.py:260:stop] epoch=0/micro_step=8200/global_step=410, RunningAvgSamplesPerSec=17.00076124024685, CurrSamplesPerSec=17.1988977693096, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.501003344481606e-06]
[2024-02-19 19:31:05,736] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512
curr_lr = Epoch: [0][411/500]   Time 148.405 (148.479)  Loss 5.3951 (8.3353)    VQALoss 0.0000 (0.0000) VGLoss 5.3951 (8.3353)curr_lr = curr_lr = curr_lr = 
 curr_lr =    curr_lr = curr_lr = [9.500334448160537e-06] [9.500334448160537e-06][9.500334448160537e-06] 
 [9.500334448160537e-06]

[9.500334448160537e-06]

[9.500334448160537e-06][9.500334448160537e-06]

curr_lr =  [9.500334448160537e-06]
curr_lr =  curr_lr =  [9.498996655518395e-06]
curr_lr = [9.498996655518395e-06]
 curr_lr = [9.498996655518395e-06]
 [9.498996655518395e-06]
curr_lr =  curr_lr =  [9.498996655518395e-06]
[9.498996655518395e-06]
curr_lr =  [9.498996655518395e-06]
curr_lr =  [9.498996655518395e-06]
curr_lr = curr_lr = curr_lr =    [9.497658862876254e-06][9.497658862876254e-06][9.497658862876254e-06]curr_lr = 


 [9.497658862876254e-06]
curr_lr =  [9.497658862876254e-06]
curr_lr =  curr_lr = [9.497658862876254e-06]
 [9.497658862876254e-06]
curr_lr =  [9.497658862876254e-06]
curr_lr = curr_lr =   curr_lr =  curr_lr = [9.496321070234115e-06][9.496321070234115e-06][9.496321070234115e-06]
 

[9.496321070234115e-06]
curr_lr = curr_lr =   [9.496321070234115e-06][9.496321070234115e-06]

curr_lr =  [9.496321070234115e-06]
curr_lr =  [9.496321070234115e-06]
curr_lr =  curr_lr = curr_lr = [9.494983277591973e-06]
 curr_lr =  [9.494983277591973e-06] 
[9.494983277591973e-06][9.494983277591973e-06]

curr_lr =  [9.494983277591973e-06]
curr_lr =  [9.494983277591973e-06]
curr_lr =  [9.494983277591973e-06]
curr_lr =  [9.494983277591973e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =       [9.493645484949833e-06][9.493645484949833e-06]
curr_lr = 
[9.493645484949833e-06]Epoch: [0][416/500]      Time 147.406 (148.005)  Loss 3.7076 (8.3807)    VQALoss 0.0000 (0.0000) VGLoss 3.7076 (8.3807)
[9.493645484949833e-06][9.493645484949833e-06]
[9.493645484949833e-06]


 [9.493645484949833e-06]
curr_lr =  [9.493645484949833e-06]
curr_lr = curr_lr =   [9.492307692307693e-06]
[9.492307692307693e-06]
curr_lr = curr_lr =  [9.492307692307693e-06]
 [9.492307692307693e-06]
curr_lr =  [9.492307692307693e-06]
curr_lr =  [9.492307692307693e-06]
curr_lr =  [9.492307692307693e-06]
curr_lr =  [9.492307692307693e-06]
curr_lr = curr_lr =   curr_lr =  [9.490969899665553e-06][9.490969899665553e-06]

[9.490969899665553e-06]curr_lr = 
 curr_lr = curr_lr = [9.490969899665553e-06] 
 [9.490969899665553e-06][9.490969899665553e-06]

curr_lr =  [9.490969899665553e-06]
curr_lr =  [9.490969899665553e-06]
curr_lr = curr_lr =   curr_lr = [9.489632107023411e-06]
[9.489632107023411e-06] 
[9.489632107023411e-06]
curr_lr =  [9.489632107023411e-06]
curr_lr = curr_lr =   [9.489632107023411e-06][9.489632107023411e-06]

curr_lr =  [9.489632107023411e-06]
curr_lr =  [9.489632107023411e-06]
[2024-02-19 19:53:18,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=23, lr=[9.488963210702341e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr = curr_lr =    curr_lr = [9.488294314381271e-06][9.488294314381271e-06]

 [9.488294314381271e-06]
[9.488294314381271e-06]
curr_lr = curr_lr =   [9.488294314381271e-06]
[9.488294314381271e-06]
curr_lr =  [9.488294314381271e-06]
[2024-02-19 19:53:18,952] [INFO] [timer.py:260:stop] epoch=0/micro_step=8400/global_step=420, RunningAvgSamplesPerSec=17.00755730491566, CurrSamplesPerSec=17.23562245154333, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.488294314381271e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =   curr_lr =   [9.486956521739131e-06] Epoch: [0][421/500]    Time 148.317 (148.302)  Loss 5.6336 (7.3476)    VQALoss 0.0000 (0.0000) VGLoss 5.6336 (7.3476)
[9.486956521739131e-06][9.486956521739131e-06][9.486956521739131e-06]



[9.486956521739131e-06][9.486956521739131e-06]curr_lr = 

 [9.486956521739131e-06]
curr_lr =  [9.486956521739131e-06]
curr_lr =  [9.485618729096991e-06]
curr_lr =  curr_lr = curr_lr =  [9.485618729096991e-06] 
[9.485618729096991e-06]
[9.485618729096991e-06]
curr_lr =  curr_lr =  [9.485618729096991e-06]
[9.485618729096991e-06]
curr_lr =  [9.485618729096991e-06]
curr_lr =  [9.485618729096991e-06]
curr_lr = curr_lr =   [9.484280936454851e-06][9.484280936454851e-06]

curr_lr = curr_lr =   [9.484280936454851e-06]
[9.484280936454851e-06]
curr_lr = curr_lr =   [9.484280936454851e-06]
[9.484280936454851e-06]
curr_lr =  curr_lr = [9.484280936454851e-06]
 [9.484280936454851e-06]
curr_lr =  curr_lr =  [9.48294314381271e-06]
curr_lr = [9.48294314381271e-06]
 curr_lr = curr_lr = [9.48294314381271e-06] 
 curr_lr = [9.48294314381271e-06]
 [9.48294314381271e-06]
[9.48294314381271e-06]
curr_lr =  [9.48294314381271e-06]
curr_lr =  [9.48294314381271e-06]
curr_lr =  [9.48160535117057e-06]curr_lr = curr_lr = 
  [9.48160535117057e-06]
[9.48160535117057e-06]
curr_lr =  [9.48160535117057e-06]
curr_lr =  [9.48160535117057e-06]
curr_lr =  [9.48160535117057e-06]
curr_lr =  [9.48160535117057e-06]
curr_lr =  [9.48160535117057e-06]
curr_lr = curr_lr = curr_lr = curr_lr =    Epoch: [0][426/500]  Time 147.993 (148.273)  Loss 5.7240 (8.2277)    VQALoss 0.0000 (0.0000) VGLoss 5.7240 (8.2277)curr_lr =  
curr_lr = [9.480267558528429e-06][9.480267558528429e-06]
[9.480267558528429e-06]
 
[9.480267558528429e-06]
 curr_lr = [9.480267558528429e-06]
[9.480267558528429e-06]
 [9.480267558528429e-06]
curr_lr =  [9.480267558528429e-06]
curr_lr =  curr_lr = curr_lr = [9.478929765886289e-06]curr_lr =  
  [9.478929765886289e-06]
[9.478929765886289e-06]
[9.478929765886289e-06]
curr_lr =  [9.478929765886289e-06]
curr_lr = curr_lr =   [9.478929765886289e-06]
[9.478929765886289e-06]
curr_lr =  [9.478929765886289e-06]
curr_lr = curr_lr =   [9.477591973244147e-06][9.477591973244147e-06]

curr_lr =  curr_lr = [9.477591973244147e-06]
 [9.477591973244147e-06]
curr_lr =  curr_lr = [9.477591973244147e-06] 
[9.477591973244147e-06]
curr_lr =  [9.477591973244147e-06]
curr_lr =  [9.477591973244147e-06]
curr_lr = curr_lr =   curr_lr = [9.476254180602007e-06] 
[9.476254180602007e-06]
[9.476254180602007e-06]
curr_lr =  [9.476254180602007e-06]
curr_lr = curr_lr =   [9.476254180602007e-06][9.476254180602007e-06]

curr_lr =  [9.476254180602007e-06]
curr_lr =  [9.476254180602007e-06]
[2024-02-19 20:18:01,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=23, lr=[9.475585284280937e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr =  [9.474916387959867e-06]
curr_lr =  [9.474916387959867e-06]
[9.474916387959867e-06]
curr_lr =  [9.474916387959867e-06]
curr_lr =  curr_lr = [9.474916387959867e-06] 
[9.474916387959867e-06]
curr_lr =  [9.474916387959867e-06]
[2024-02-19 20:18:01,624] [INFO] [timer.py:260:stop] epoch=0/micro_step=8600/global_step=430, RunningAvgSamplesPerSec=17.01375936956451, CurrSamplesPerSec=17.2426897652991, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.474916387959867e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     curr_lr = curr_lr = curr_lr = Epoch: [0][431/500]   Time 148.696 (148.337)  Loss 6.8457 (7.8505)    VQALoss 0.0000 (0.0000) VGLoss 6.8457 (7.8505) [9.473578595317727e-06][9.473578595317727e-06][9.473578595317727e-06][9.473578595317727e-06]

 
 

[9.473578595317727e-06]
[9.473578595317727e-06][9.473578595317727e-06]

curr_lr =  [9.473578595317727e-06]
curr_lr = curr_lr =   curr_lr =  curr_lr = [9.472240802675587e-06][9.472240802675587e-06]

 [9.472240802675587e-06]
[9.472240802675587e-06]
curr_lr = curr_lr =   [9.472240802675587e-06][9.472240802675587e-06]

curr_lr =  [9.472240802675587e-06]
curr_lr =  [9.472240802675587e-06]
curr_lr = curr_lr =   curr_lr = [9.470903010033445e-06]
curr_lr =  [9.470903010033445e-06]
[9.470903010033445e-06] 
curr_lr = curr_lr =  [9.470903010033445e-06] 
[9.470903010033445e-06][9.470903010033445e-06]

curr_lr =  [9.470903010033445e-06]
curr_lr =  [9.470903010033445e-06]
curr_lr =  curr_lr = curr_lr = [9.469565217391305e-06] 
 [9.469565217391305e-06]
[9.469565217391305e-06]
curr_lr =  [9.469565217391305e-06]
curr_lr =  [9.469565217391305e-06]
curr_lr =  [9.469565217391305e-06]
curr_lr =  [9.469565217391305e-06]
curr_lr =  [9.469565217391305e-06]
curr_lr = curr_lr =   [9.468227424749165e-06]
[9.468227424749165e-06]
curr_lr =  [9.468227424749165e-06]
curr_lr =  [9.468227424749165e-06]curr_lr = 
curr_lr =   [9.468227424749165e-06]
[9.468227424749165e-06]
curr_lr =  [9.468227424749165e-06]
curr_lr =  [9.468227424749165e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   Epoch: [0][436/500]     Time 147.652 (148.127)  Loss 4.3547 (8.0939)    VQALoss 0.0000 (0.0000) VGLoss 4.3547 (8.0939)     
[9.466889632107025e-06]
[9.466889632107025e-06][9.466889632107025e-06]
[9.466889632107025e-06]
[9.466889632107025e-06][9.466889632107025e-06]
[9.466889632107025e-06]


curr_lr =  [9.466889632107025e-06]
curr_lr =  curr_lr =  [9.465551839464883e-06]
[9.465551839464883e-06]
curr_lr =  curr_lr = [9.465551839464883e-06]
 curr_lr = [9.465551839464883e-06]
 curr_lr = [9.465551839464883e-06] 
[9.465551839464883e-06]
curr_lr =  [9.465551839464883e-06]
curr_lr =  [9.465551839464883e-06]
curr_lr =  curr_lr = [9.464214046822743e-06]
 curr_lr =  [9.464214046822743e-06]
curr_lr = [9.464214046822743e-06]
 [9.464214046822743e-06]
curr_lr =  curr_lr = [9.464214046822743e-06]
 [9.464214046822743e-06]
curr_lr =  curr_lr = [9.464214046822743e-06]
 [9.464214046822743e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     [9.462876254180603e-06][9.462876254180603e-06][9.462876254180603e-06][9.462876254180603e-06]



curr_lr =  [9.462876254180603e-06]
curr_lr =  [9.462876254180603e-06]
curr_lr =  [9.462876254180603e-06]
curr_lr =  [9.462876254180603e-06]
[2024-02-19 20:42:44,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=23, lr=[9.462207357859533e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = [9.461538461538463e-06] 
[9.461538461538463e-06]
curr_lr = curr_lr =   curr_lr = [9.461538461538463e-06] [9.461538461538463e-06]

[9.461538461538463e-06]
curr_lr =  [9.461538461538463e-06]
curr_lr =  [9.461538461538463e-06]
[2024-02-19 20:42:44,505] [INFO] [timer.py:260:stop] epoch=0/micro_step=8800/global_step=440, RunningAvgSamplesPerSec=17.019629829231796, CurrSamplesPerSec=17.26525576328796, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.461538461538463e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =  curr_lr =    [9.460200668896323e-06]Epoch: [0][441/500]     Time 148.797 (148.470)  Loss 5.7769 (7.8414)    VQALoss 0.0000 (0.0000) VGLoss 5.7769 (7.8414) curr_lr = [9.460200668896323e-06]
[9.460200668896323e-06][9.460200668896323e-06]


[9.460200668896323e-06]
[9.460200668896323e-06] 

[9.460200668896323e-06]
curr_lr =  [9.460200668896323e-06]
curr_lr =  curr_lr = [9.45886287625418e-06] 
[9.45886287625418e-06]
curr_lr =  curr_lr =  curr_lr = [9.45886287625418e-06]
 [9.45886287625418e-06]
[9.45886287625418e-06]
curr_lr =  [9.45886287625418e-06]
curr_lr =  [9.45886287625418e-06]
curr_lr =  [9.45886287625418e-06]
curr_lr =  curr_lr = [9.45752508361204e-06]curr_lr = curr_lr = 
   [9.45752508361204e-06]
[9.45752508361204e-06]
[9.45752508361204e-06]
curr_lr =  [9.45752508361204e-06]curr_lr = 
 [9.45752508361204e-06]
curr_lr =  [9.45752508361204e-06]
curr_lr =  [9.45752508361204e-06]
curr_lr =  curr_lr = [9.4561872909699e-06]
 [9.4561872909699e-06]
curr_lr =  curr_lr = curr_lr =   [9.4561872909699e-06]curr_lr = 
[9.4561872909699e-06]
 [9.4561872909699e-06]
[9.4561872909699e-06]
curr_lr =  [9.4561872909699e-06]
curr_lr =  [9.4561872909699e-06]
curr_lr = curr_lr =   curr_lr =  [9.454849498327759e-06][9.454849498327759e-06]

[9.454849498327759e-06]
curr_lr =  [9.454849498327759e-06]
curr_lr =  [9.454849498327759e-06]
curr_lr =  [9.454849498327759e-06]
curr_lr =  [9.454849498327759e-06]
curr_lr =  [9.454849498327759e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = Epoch: [0][446/500] Time 147.063 (148.078)  Loss 4.1720 (7.9098)    VQALoss 0.0000 (0.0000) VGLoss 4.1720 (7.9098)  curr_lr =    
 [9.45351170568562e-06] [9.45351170568562e-06][9.45351170568562e-06]
[9.45351170568562e-06][9.45351170568562e-06][9.45351170568562e-06]

[9.45351170568562e-06]



curr_lr =  [9.45351170568562e-06]
curr_lr =  curr_lr = curr_lr =  [9.452173913043479e-06]
 [9.452173913043479e-06]
curr_lr = [9.452173913043479e-06] 
[9.452173913043479e-06]
curr_lr =  curr_lr = [9.452173913043479e-06]
 [9.452173913043479e-06]
curr_lr =  [9.452173913043479e-06]
curr_lr =  [9.452173913043479e-06]
curr_lr = curr_lr =   [9.450836120401339e-06][9.450836120401339e-06]curr_lr = 

 curr_lr = [9.450836120401339e-06] 
[9.450836120401339e-06]
curr_lr =  [9.450836120401339e-06]
curr_lr =  curr_lr =  [9.450836120401339e-06]
[9.450836120401339e-06]
curr_lr =  [9.450836120401339e-06]
curr_lr = curr_lr =   curr_lr = [9.449498327759198e-06]
 [9.449498327759198e-06]
[9.449498327759198e-06]
curr_lr = curr_lr = curr_lr =    [9.449498327759198e-06][9.449498327759198e-06]

[9.449498327759198e-06]
curr_lr =  [9.449498327759198e-06]
curr_lr =  [9.449498327759198e-06]
[2024-02-19 21:07:26,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=23, lr=[9.448829431438128e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr = curr_lr =    [9.448160535117058e-06]
[9.448160535117058e-06][9.448160535117058e-06]curr_lr = 

 [9.448160535117058e-06]
curr_lr =  [9.448160535117058e-06]
curr_lr =  [9.448160535117058e-06]
curr_lr =  [9.448160535117058e-06]
[2024-02-19 21:07:27,105] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=450, RunningAvgSamplesPerSec=17.025308431295425, CurrSamplesPerSec=17.23013674332181, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.448160535117058e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =      [9.446822742474917e-06]
[9.446822742474917e-06][9.446822742474917e-06][9.446822742474917e-06][9.446822742474917e-06]



[9.446822742474917e-06]curr_lr = 
 [9.446822742474917e-06]
Epoch: [0][451/500]     Time 148.713 (148.425)  Loss 3.6469 (7.7394)    VQALoss 0.0000 (0.0000) VGLoss 3.6469 (7.7394)
curr_lr =  [9.446822742474917e-06]
[2024-02-19 21:10:00,837] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step451 is about to be saved!
[2024-02-19 21:10:13,505] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step451/mp_rank_00_model_states.pt
[2024-02-19 21:10:13,505] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step451/mp_rank_00_model_states.pt...
[2024-02-19 21:12:33,909] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step451/mp_rank_00_model_states.pt.
[2024-02-19 21:12:35,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step451/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-19 21:12:35,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step451/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-19 21:12:35,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step451/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-19 21:12:35,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step451/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-19 21:12:35,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step451/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-19 21:12:35,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step451/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-19 21:12:35,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step451/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-19 21:12:35,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step451/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-19 21:12:39,507] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step451/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-19 21:12:39,508] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step451/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-19 21:12:39,508] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step451 is ready now!
[2024-02-19 21:12:39,589] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step451/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-19 21:12:39,590] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step451/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-19 21:12:39,590] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step451 is ready now!
[2024-02-19 21:12:39,603] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step451/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-19 21:12:39,604] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step451/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-19 21:12:39,604] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step451 is ready now!
[2024-02-19 21:12:39,606] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step451/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-19 21:12:39,606] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step451/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-19 21:12:39,606] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step451 is ready now!
[2024-02-19 21:12:39,611] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step451/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-19 21:12:39,611] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step451/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-19 21:12:39,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step451 is ready now!
[2024-02-19 21:12:39,679] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step451/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-19 21:12:39,680] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step451/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-19 21:12:39,680] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step451 is ready now!
[2024-02-19 21:12:39,720] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step451/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-19 21:12:39,720] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step451/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-19 21:12:39,721] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step451 is ready now!
[2024-02-19 21:12:39,866] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step451/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-19 21:12:39,876] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step451/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-19 21:12:39,876] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step451 is ready now!
curr_lr = curr_lr = curr_lr =    [9.445484949832776e-06][9.445484949832776e-06]
[9.445484949832776e-06]

curr_lr =  [9.445484949832776e-06]
curr_lr =  curr_lr =  [9.445484949832776e-06]
[9.445484949832776e-06]
curr_lr =  [9.445484949832776e-06]
curr_lr =  [9.445484949832776e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =   [9.444147157190636e-06][9.444147157190636e-06]

 [9.444147157190636e-06]
curr_lr = [9.444147157190636e-06]
 [9.444147157190636e-06]
curr_lr =  [9.444147157190636e-06]
curr_lr = curr_lr =   [9.444147157190636e-06]
[9.444147157190636e-06]
curr_lr =  curr_lr =  curr_lr = [9.442809364548495e-06]curr_lr = 
 [9.442809364548495e-06]
 [9.442809364548495e-06]
[9.442809364548495e-06]
curr_lr =  [9.442809364548495e-06]
curr_lr = curr_lr =   [9.442809364548495e-06]
[9.442809364548495e-06]
curr_lr =  [9.442809364548495e-06]
curr_lr =  curr_lr = curr_lr = curr_lr = [9.441471571906356e-06] 
  [9.441471571906356e-06]
[9.441471571906356e-06]
[9.441471571906356e-06]
curr_lr =  curr_lr =  [9.441471571906356e-06]
[9.441471571906356e-06]
curr_lr =  [9.441471571906356e-06]
curr_lr =  [9.441471571906356e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =       Epoch: [0][456/500]   Time 147.278 (181.023)  Loss 25.1552 (7.7430)   VQALoss 0.0000 (0.0000) VGLoss 25.1552 (7.7430)
[9.440133779264214e-06][9.440133779264214e-06]curr_lr = [9.440133779264214e-06]
[9.440133779264214e-06]
[9.440133779264214e-06][9.440133779264214e-06]



 [9.440133779264214e-06]
curr_lr =  [9.440133779264214e-06]
curr_lr =  curr_lr = [9.438795986622074e-06]
curr_lr =   [9.438795986622074e-06]
[9.438795986622074e-06]
curr_lr =  curr_lr = curr_lr =  [9.438795986622074e-06]
 [9.438795986622074e-06]
[9.438795986622074e-06]
curr_lr =  [9.438795986622074e-06]
curr_lr =  [9.438795986622074e-06]
curr_lr =  curr_lr =  [9.437458193979934e-06]
curr_lr =  [9.437458193979934e-06]curr_lr = 
 [9.437458193979934e-06]
[9.437458193979934e-06]
curr_lr = curr_lr =   [9.437458193979934e-06]
[9.437458193979934e-06]
curr_lr =  [9.437458193979934e-06]
curr_lr =  [9.437458193979934e-06]
curr_lr =  curr_lr =  [9.436120401337794e-06]
curr_lr = curr_lr =  [9.436120401337794e-06]
 curr_lr = curr_lr = [9.436120401337794e-06][9.436120401337794e-06]  

[9.436120401337794e-06][9.436120401337794e-06]

curr_lr =  [9.436120401337794e-06]
curr_lr =  [9.436120401337794e-06]
[2024-02-19 21:34:53,033] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=23, lr=[9.435451505016722e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.434782608695652e-06]
curr_lr =  curr_lr =  [9.434782608695652e-06]
curr_lr = [9.434782608695652e-06] 
[9.434782608695652e-06]
curr_lr =  curr_lr = [9.434782608695652e-06] 
[9.434782608695652e-06]
curr_lr =  [9.434782608695652e-06]
[2024-02-19 21:34:53,235] [INFO] [timer.py:260:stop] epoch=0/micro_step=9200/global_step=460, RunningAvgSamplesPerSec=17.03090032208203, CurrSamplesPerSec=17.299948709269316, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.434782608695652e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =   curr_lr =   Epoch: [0][461/500]  Time 148.497 (148.159)  Loss 5.7026 (7.6233)    VQALoss 0.0000 (0.0000) VGLoss 5.7026 (7.6233) [9.433444816053512e-06] [9.433444816053512e-06]

[9.433444816053512e-06][9.433444816053512e-06]

[9.433444816053512e-06]
[9.433444816053512e-06]
[9.433444816053512e-06]

curr_lr =  [9.433444816053512e-06]
curr_lr =  curr_lr = [9.432107023411372e-06] 
curr_lr = curr_lr =  [9.432107023411372e-06] 
[9.432107023411372e-06]
[9.432107023411372e-06]
curr_lr =  curr_lr =  [9.432107023411372e-06]
[9.432107023411372e-06]
curr_lr =  [9.432107023411372e-06]
curr_lr =  [9.432107023411372e-06]
curr_lr =  curr_lr = [9.43076923076923e-06]
 curr_lr = curr_lr =  [9.43076923076923e-06]
 [9.43076923076923e-06]
[9.43076923076923e-06]
curr_lr =  [9.43076923076923e-06]
curr_lr =  [9.43076923076923e-06]
curr_lr =  [9.43076923076923e-06]
curr_lr =  [9.43076923076923e-06]
curr_lr =  curr_lr = [9.429431438127092e-06]curr_lr = 
 curr_lr =   [9.429431438127092e-06]
[9.429431438127092e-06]
[9.429431438127092e-06]
curr_lr =  [9.429431438127092e-06]
curr_lr =  [9.429431438127092e-06]
curr_lr =  [9.429431438127092e-06]
curr_lr =  [9.429431438127092e-06]
curr_lr =  [9.42809364548495e-06]curr_lr = 
 curr_lr = curr_lr =   [9.42809364548495e-06]
[9.42809364548495e-06]
[9.42809364548495e-06]
curr_lr =  [9.42809364548495e-06]
curr_lr =  [9.42809364548495e-06]
curr_lr =  [9.42809364548495e-06]
curr_lr =  [9.42809364548495e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =        [9.42675585284281e-06]
[9.42675585284281e-06][9.42675585284281e-06]

Epoch: [0][466/500]     Time 147.895 (148.282)  Loss 21.7696 (7.7176)   VQALoss 0.0000 (0.0000) VGLoss 21.7696 (7.7176)[9.42675585284281e-06][9.42675585284281e-06][9.42675585284281e-06][9.42675585284281e-06]




curr_lr =  [9.42675585284281e-06]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.42541806020067e-06]
  [9.42541806020067e-06]
[9.42541806020067e-06]
[9.42541806020067e-06]
curr_lr =  curr_lr = [9.42541806020067e-06]
 [9.42541806020067e-06]
curr_lr =  [9.42541806020067e-06]
curr_lr =  [9.42541806020067e-06]
curr_lr = curr_lr =   curr_lr =  curr_lr = [9.42408026755853e-06][9.42408026755853e-06]
 
[9.42408026755853e-06]
[9.42408026755853e-06]
curr_lr =  [9.42408026755853e-06]
curr_lr =  curr_lr = [9.42408026755853e-06]
 [9.42408026755853e-06]
curr_lr =  [9.42408026755853e-06]
curr_lr = curr_lr = curr_lr =   curr_lr =   [9.422742474916388e-06]
[9.422742474916388e-06][9.422742474916388e-06]

[9.422742474916388e-06]
curr_lr =  [9.422742474916388e-06]
curr_lr =  [9.422742474916388e-06]
curr_lr =  [9.422742474916388e-06]
curr_lr =  [9.422742474916388e-06]
[2024-02-19 21:59:35,208] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=23, lr=[9.422073578595318e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr = curr_lr =    [9.421404682274248e-06][9.421404682274248e-06]

[9.421404682274248e-06]
curr_lr = curr_lr =   [9.421404682274248e-06]
[9.421404682274248e-06]
curr_lr =  [9.421404682274248e-06]
curr_lr =  [9.421404682274248e-06]
[2024-02-19 21:59:35,409] [INFO] [timer.py:260:stop] epoch=0/micro_step=9400/global_step=470, RunningAvgSamplesPerSec=17.036206994379107, CurrSamplesPerSec=17.107778429067075, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.421404682274248e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =   curr_lr =     [9.420066889632108e-06][9.420066889632108e-06][9.420066889632108e-06]

[9.420066889632108e-06][9.420066889632108e-06][9.420066889632108e-06]
Epoch: [0][471/500]     Time 148.756 (148.205)  Loss 6.6376 (8.0140)    VQALoss 0.0000 (0.0000) VGLoss 6.6376 (8.0140)[9.420066889632108e-06]




curr_lr =  [9.420066889632108e-06]
curr_lr =  curr_lr = [9.418729096989966e-06]curr_lr = 
  curr_lr = [9.418729096989966e-06][9.418729096989966e-06]

 [9.418729096989966e-06]
curr_lr =  [9.418729096989966e-06]
curr_lr =  [9.418729096989966e-06]
curr_lr =  [9.418729096989966e-06]
curr_lr =  [9.418729096989966e-06]
curr_lr = curr_lr = curr_lr =    curr_lr = [9.417391304347828e-06][9.417391304347828e-06]

 [9.417391304347828e-06]
[9.417391304347828e-06]curr_lr = 
curr_lr =   [9.417391304347828e-06]
[9.417391304347828e-06]
curr_lr =  [9.417391304347828e-06]
curr_lr =  [9.417391304347828e-06]
curr_lr =  curr_lr =  curr_lr = [9.416053511705686e-06]
[9.416053511705686e-06] 
[9.416053511705686e-06]curr_lr = 
 curr_lr = [9.416053511705686e-06]
 [9.416053511705686e-06]
curr_lr = curr_lr =   [9.416053511705686e-06]
[9.416053511705686e-06]
curr_lr =  [9.416053511705686e-06]
curr_lr = curr_lr =  curr_lr =   [9.414715719063546e-06]
curr_lr = [9.414715719063546e-06][9.414715719063546e-06]

 [9.414715719063546e-06]
curr_lr =  curr_lr = [9.414715719063546e-06]
 [9.414715719063546e-06]
curr_lr =  [9.414715719063546e-06]
curr_lr =  [9.414715719063546e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   Epoch: [0][476/500]       Time 147.252 (148.074)  Loss 5.2284 (7.9727)    VQALoss 0.0000 (0.0000) VGLoss 5.2284 (7.9727)  
  [9.413377926421406e-06][9.413377926421406e-06]

[9.413377926421406e-06][9.413377926421406e-06]

[9.413377926421406e-06][9.413377926421406e-06]

curr_lr =  [9.413377926421406e-06]
curr_lr =  [9.413377926421406e-06]
curr_lr =  [9.412040133779266e-06]
curr_lr =  curr_lr =  [9.412040133779266e-06]
[9.412040133779266e-06]
curr_lr =  [9.412040133779266e-06]
curr_lr = curr_lr =   [9.412040133779266e-06]
[9.412040133779266e-06]
curr_lr =  curr_lr =  [9.412040133779266e-06]
[9.412040133779266e-06]
curr_lr =  [9.410702341137124e-06]
curr_lr =  curr_lr = [9.410702341137124e-06] 
[9.410702341137124e-06]
curr_lr = curr_lr =   [9.410702341137124e-06]
[9.410702341137124e-06]
curr_lr =  [9.410702341137124e-06]
curr_lr =  [9.410702341137124e-06]
curr_lr =  [9.410702341137124e-06]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.409364548494984e-06]
 [9.409364548494984e-06] 
[9.409364548494984e-06]
[9.409364548494984e-06]
curr_lr =  [9.409364548494984e-06]
curr_lr = curr_lr =   [9.409364548494984e-06]
[9.409364548494984e-06]
curr_lr =  [9.409364548494984e-06]
[2024-02-19 22:24:17,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=23, lr=[9.408695652173914e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.408026755852844e-06][9.408026755852844e-06] 

 [9.408026755852844e-06]
[9.408026755852844e-06]
curr_lr =  curr_lr = [9.408026755852844e-06]
 [9.408026755852844e-06]
curr_lr =  [9.408026755852844e-06]
[2024-02-19 22:24:17,589] [INFO] [timer.py:260:stop] epoch=0/micro_step=9600/global_step=480, RunningAvgSamplesPerSec=17.041300651594547, CurrSamplesPerSec=17.403537893652224, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.408026755852844e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    Epoch: [0][481/500]      Time 148.473 (148.305)  Loss 5.4849 (7.9668)    VQALoss 0.0000 (0.0000) VGLoss 5.4849 (7.9668)  
curr_lr =  [9.406688963210704e-06][9.406688963210704e-06][9.406688963210704e-06]

[9.406688963210704e-06]

 [9.406688963210704e-06]
[9.406688963210704e-06]
[9.406688963210704e-06]
curr_lr =  [9.406688963210704e-06]
curr_lr = curr_lr =   curr_lr = [9.405351170568564e-06][9.405351170568564e-06]
curr_lr = 
  [9.405351170568564e-06]
[9.405351170568564e-06]
curr_lr =  [9.405351170568564e-06]
curr_lr =  [9.405351170568564e-06]
curr_lr =  [9.405351170568564e-06]
curr_lr =  [9.405351170568564e-06]
curr_lr = curr_lr = curr_lr =    [9.404013377926422e-06][9.404013377926422e-06]
[9.404013377926422e-06]

curr_lr =  curr_lr = [9.404013377926422e-06] 
curr_lr = [9.404013377926422e-06]
 [9.404013377926422e-06]
curr_lr =  [9.404013377926422e-06]
curr_lr =  [9.404013377926422e-06]
curr_lr =  curr_lr = curr_lr =  [9.402675585284282e-06]
 [9.402675585284282e-06]
[9.402675585284282e-06]
curr_lr =  [9.402675585284282e-06]
curr_lr = curr_lr =   [9.402675585284282e-06][9.402675585284282e-06]

curr_lr =  curr_lr =  [9.402675585284282e-06]
[9.402675585284282e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     [9.401337792642142e-06]
[9.401337792642142e-06][9.401337792642142e-06][9.401337792642142e-06]


curr_lr =  [9.401337792642142e-06]
curr_lr =  [9.401337792642142e-06]
curr_lr =  [9.401337792642142e-06]
curr_lr =  [9.401337792642142e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr =      [9.4e-06][9.4e-06][9.4e-06]


[9.4e-06][9.4e-06]

Epoch: [0][486/500]     Time 148.070 (148.210)  Loss 4.4062 (7.5844)    VQALoss 0.0000 (0.0000) VGLoss 4.4062 (7.5844)[9.4e-06][9.4e-06]


curr_lr =  [9.4e-06]
curr_lr = curr_lr =   curr_lr = [9.39866220735786e-06]
 curr_lr =  [9.39866220735786e-06][9.39866220735786e-06]

[9.39866220735786e-06]
curr_lr =  [9.39866220735786e-06]
curr_lr =  [9.39866220735786e-06]
curr_lr =  [9.39866220735786e-06]
curr_lr =  [9.39866220735786e-06]
curr_lr = curr_lr =  curr_lr =   [9.39732441471572e-06]
[9.39732441471572e-06]
[9.39732441471572e-06]
curr_lr =  curr_lr = [9.39732441471572e-06]
 curr_lr = [9.39732441471572e-06] 
[9.39732441471572e-06]
curr_lr =  [9.39732441471572e-06]
curr_lr =  [9.39732441471572e-06]
curr_lr =  curr_lr = [9.39598662207358e-06]
 [9.39598662207358e-06]curr_lr = 
 [9.39598662207358e-06]
curr_lr =  curr_lr = [9.39598662207358e-06]
 [9.39598662207358e-06]
curr_lr =  [9.39598662207358e-06]
curr_lr =  [9.39598662207358e-06]
curr_lr =  [9.39598662207358e-06]
[2024-02-19 22:49:00,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=23, lr=[9.39531772575251e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = [9.39464882943144e-06] 
[9.39464882943144e-06]
curr_lr =  curr_lr =  [9.39464882943144e-06]
[9.39464882943144e-06]
curr_lr =  [9.39464882943144e-06]
curr_lr =  [9.39464882943144e-06]
curr_lr =  [9.39464882943144e-06]
[2024-02-19 22:49:00,268] [INFO] [timer.py:260:stop] epoch=0/micro_step=9800/global_step=490, RunningAvgSamplesPerSec=17.04605979787952, CurrSamplesPerSec=17.384896387956733, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.39464882943144e-06]
curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr = curr_lr = curr_lr =   [9.3933110367893e-06]   
 [9.3933110367893e-06][9.3933110367893e-06]
[9.3933110367893e-06]
Epoch: [0][491/500]     Time 148.451 (148.321)  Loss 4.3167 (7.4417)    VQALoss 0.0000 (0.0000) VGLoss 4.3167 (7.4417)
[9.3933110367893e-06][9.3933110367893e-06][9.3933110367893e-06]



curr_lr =  [9.3933110367893e-06]
curr_lr =  curr_lr = [9.391973244147158e-06]
 curr_lr =  [9.391973244147158e-06]
[9.391973244147158e-06]
curr_lr =  [9.391973244147158e-06]
curr_lr = curr_lr =   [9.391973244147158e-06][9.391973244147158e-06]

curr_lr =  [9.391973244147158e-06]
curr_lr =  [9.391973244147158e-06]
curr_lr =  [9.390635451505017e-06]
curr_lr =  [9.390635451505017e-06]
curr_lr = curr_lr =   [9.390635451505017e-06]curr_lr = 
[9.390635451505017e-06]
curr_lr =   [9.390635451505017e-06]
[9.390635451505017e-06]
curr_lr =  [9.390635451505017e-06]
curr_lr =  [9.390635451505017e-06]
curr_lr =  [9.389297658862877e-06]
curr_lr = curr_lr =   [9.389297658862877e-06]curr_lr = 
 [9.389297658862877e-06]curr_lr = 
 [9.389297658862877e-06][9.389297658862877e-06]
curr_lr = 
 [9.389297658862877e-06]
curr_lr =  [9.389297658862877e-06]
curr_lr =  [9.389297658862877e-06]
curr_lr = curr_lr =   curr_lr =  [9.387959866220736e-06][9.387959866220736e-06]

curr_lr = [9.387959866220736e-06]
 [9.387959866220736e-06]
curr_lr =  [9.387959866220736e-06]
curr_lr =  [9.387959866220736e-06]
curr_lr =  [9.387959866220736e-06]
curr_lr =  [9.387959866220736e-06]
curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr =  curr_lr =   [9.386622073578595e-06] 
 [9.386622073578595e-06]
[9.386622073578595e-06]Epoch: [0][496/500]      Time 147.320 (148.103)  Loss 3.9980 (7.8372)    VQALoss 0.0000 (0.0000) VGLoss 3.9980 (7.8372)[9.386622073578595e-06]
[9.386622073578595e-06][9.386622073578595e-06]

[9.386622073578595e-06]


curr_lr =  [9.386622073578595e-06]
curr_lr =  curr_lr = curr_lr = [9.385284280936455e-06] 
 [9.385284280936455e-06]
curr_lr = [9.385284280936455e-06] 
[9.385284280936455e-06]
curr_lr =  [9.385284280936455e-06]
curr_lr =  curr_lr = [9.385284280936455e-06]
 [9.385284280936455e-06]
curr_lr =  [9.385284280936455e-06]
curr_lr =  curr_lr = curr_lr =  [9.383946488294315e-06]
 [9.383946488294315e-06]
[9.383946488294315e-06]
curr_lr = curr_lr =   [9.383946488294315e-06]
[9.383946488294315e-06]
curr_lr =  [9.383946488294315e-06]
curr_lr =  [9.383946488294315e-06]
curr_lr =  [9.383946488294315e-06]
curr_lr = curr_lr =  curr_lr =   [9.382608695652175e-06]
[9.382608695652175e-06][9.382608695652175e-06]

curr_lr =  [9.382608695652175e-06]
curr_lr =  [9.382608695652175e-06]
curr_lr =  [9.382608695652175e-06]
curr_lr =  [9.382608695652175e-06]
curr_lr =  [9.382608695652175e-06]
[2024-02-19 23:13:42,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=23, lr=[9.381939799331105e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = curr_lr = [9.381270903010035e-06] 
 curr_lr = [9.381270903010035e-06]
[9.381270903010035e-06] 
[9.381270903010035e-06]
curr_lr =  curr_lr =  [9.381270903010035e-06]
[9.381270903010035e-06]
curr_lr =  [9.381270903010035e-06]
[2024-02-19 23:13:43,099] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=500, RunningAvgSamplesPerSec=17.05060997272541, CurrSamplesPerSec=17.388489239029433, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.381270903010035e-06]
==> Epoch 1/30
==> Epoch 1/30
==> Epoch 1/30
==> Epoch 1/30
==> Epoch 1/30
==> Epoch 1/30
==> Epoch 1/30
==> Epoch 1/30
Epoch: [1][  1/500]     Time 148.616 (148.616)  Loss 20.8977 (7.2053)   VQALoss 0.0000 (0.0000) VGLoss 20.8977 (7.2053)
[2024-02-19 23:16:11,738] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step501 is about to be saved!
[2024-02-19 23:16:23,436] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_1/global_step501/mp_rank_00_model_states.pt
[2024-02-19 23:16:23,436] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step501/mp_rank_00_model_states.pt...
[2024-02-19 23:18:39,946] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step501/mp_rank_00_model_states.pt.
[2024-02-19 23:18:41,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step501/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-19 23:18:41,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step501/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-19 23:18:41,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step501/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-19 23:18:41,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step501/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-19 23:18:41,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step501/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-19 23:18:41,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step501/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-19 23:18:41,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step501/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-19 23:18:41,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step501/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-19 23:18:45,386] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step501/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-19 23:18:45,387] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step501/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-19 23:18:45,387] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step501 is ready now!
[2024-02-19 23:18:45,397] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step501/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-19 23:18:45,397] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step501/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-19 23:18:45,397] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step501 is ready now!
[2024-02-19 23:18:45,412] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step501/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-19 23:18:45,413] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step501/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-19 23:18:45,413] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step501 is ready now!
[2024-02-19 23:18:45,504] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step501/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-19 23:18:45,505] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step501/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-19 23:18:45,505] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step501 is ready now!
[2024-02-19 23:18:45,516] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step501/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-19 23:18:45,517] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step501/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-19 23:18:45,517] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step501 is ready now!
[2024-02-19 23:18:45,548] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step501/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-19 23:18:45,558] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step501/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-19 23:18:45,558] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step501 is ready now!
[2024-02-19 23:18:45,581] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step501/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-19 23:18:45,581] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step501/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-19 23:18:45,581] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step501 is ready now!
[2024-02-19 23:18:45,588] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step501/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-19 23:18:45,588] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step501/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-19 23:18:45,588] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step501 is ready now!
curr_lr =  curr_lr =  [9.379264214046823e-06]
curr_lr = curr_lr = [9.379264214046823e-06] 
 [9.379264214046823e-06][9.379264214046823e-06]

curr_lr =  curr_lr = [9.379264214046823e-06]
 [9.379264214046823e-06]
curr_lr =  [9.379264214046823e-06]
curr_lr =  [9.379264214046823e-06]
curr_lr = curr_lr =   [9.377926421404683e-06]
[9.377926421404683e-06]
curr_lr =  curr_lr = [9.377926421404683e-06]
 [9.377926421404683e-06]
curr_lr =  [9.377926421404683e-06]
curr_lr =  [9.377926421404683e-06]
curr_lr =  [9.377926421404683e-06]
curr_lr =  [9.377926421404683e-06]
curr_lr = curr_lr =   curr_lr =  curr_lr = [9.376588628762543e-06]
[9.376588628762543e-06] 
[9.376588628762543e-06]
[9.376588628762543e-06]
curr_lr =  [9.376588628762543e-06]
curr_lr =  [9.376588628762543e-06]
curr_lr =  [9.376588628762543e-06]
curr_lr =  [9.376588628762543e-06]
curr_lr = curr_lr =   curr_lr = [9.375250836120401e-06] 
[9.375250836120401e-06]
[9.375250836120401e-06]
curr_lr =  [9.375250836120401e-06]
curr_lr =  [9.375250836120401e-06]
curr_lr =  [9.375250836120401e-06]
curr_lr =  [9.375250836120401e-06]
curr_lr =  [9.375250836120401e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =       [9.373913043478263e-06]
[9.373913043478263e-06]curr_lr = 
[9.373913043478263e-06][9.373913043478263e-06][9.373913043478263e-06]


[9.373913043478263e-06]
 [9.373913043478263e-06]
Epoch: [1][  6/500]     Time 147.850 (178.965)  Loss 3.9801 (7.4050)    VQALoss 0.0000 (0.0000) VGLoss 3.9801 (7.4050)
curr_lr =  [9.373913043478263e-06]
curr_lr =  curr_lr = [9.372575250836121e-06]
 [9.372575250836121e-06]curr_lr = 
 [9.372575250836121e-06]
curr_lr =  [9.372575250836121e-06]curr_lr = 
 [9.372575250836121e-06]
curr_lr =  [9.372575250836121e-06]
curr_lr =  [9.372575250836121e-06]
curr_lr =  [9.372575250836121e-06]
curr_lr = curr_lr = curr_lr =    curr_lr =  [9.371237458193981e-06][9.371237458193981e-06][9.371237458193981e-06]

[9.371237458193981e-06]

curr_lr = curr_lr =   [9.371237458193981e-06][9.371237458193981e-06]

curr_lr =  [9.371237458193981e-06]
curr_lr =  [9.371237458193981e-06]
curr_lr = curr_lr =   [9.369899665551841e-06]
curr_lr = curr_lr = [9.369899665551841e-06]
  [9.369899665551841e-06]
[9.369899665551841e-06]
curr_lr =  curr_lr = [9.369899665551841e-06]
 [9.369899665551841e-06]
curr_lr =  [9.369899665551841e-06]
curr_lr =  [9.369899665551841e-06]
[2024-02-19 23:41:00,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=23, lr=[9.369230769230771e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr = curr_lr =    curr_lr = [9.368561872909699e-06][9.368561872909699e-06] 
[9.368561872909699e-06]

[9.368561872909699e-06]
curr_lr =  [9.368561872909699e-06]
curr_lr =  [9.368561872909699e-06]
curr_lr =  [9.368561872909699e-06]
[2024-02-19 23:41:00,375] [INFO] [timer.py:260:stop] epoch=0/micro_step=10200/global_step=510, RunningAvgSamplesPerSec=17.054871128637362, CurrSamplesPerSec=17.370580340875616, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.368561872909699e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =  curr_lr =      [9.367224080267559e-06][9.367224080267559e-06]
[9.367224080267559e-06]
[9.367224080267559e-06]

[9.367224080267559e-06]
[9.367224080267559e-06][9.367224080267559e-06]

Epoch: [1][ 11/500]     Time 148.783 (148.521)  Loss 4.1447 (7.8055)    VQALoss 0.0000 (0.0000) VGLoss 4.1447 (7.8055)
curr_lr =  [9.367224080267559e-06]
curr_lr =  curr_lr =  [9.365886287625419e-06]
curr_lr = [9.365886287625419e-06]
 [9.365886287625419e-06]
curr_lr =  [9.365886287625419e-06]
curr_lr =  [9.365886287625419e-06]
curr_lr =  [9.365886287625419e-06]
curr_lr =  [9.365886287625419e-06]
curr_lr =  [9.365886287625419e-06]
curr_lr =  [9.364548494983279e-06]
curr_lr =  [9.364548494983279e-06]
curr_lr = curr_lr =   [9.364548494983279e-06]curr_lr = 
[9.364548494983279e-06]
 [9.364548494983279e-06]
curr_lr =  [9.364548494983279e-06]
curr_lr =  [9.364548494983279e-06]
curr_lr =  [9.364548494983279e-06]
curr_lr = curr_lr =   curr_lr =  [9.363210702341137e-06]
curr_lr = [9.363210702341137e-06][9.363210702341137e-06]

 [9.363210702341137e-06]
curr_lr =  curr_lr = [9.363210702341137e-06]
 [9.363210702341137e-06]
curr_lr =  [9.363210702341137e-06]
curr_lr =  [9.363210702341137e-06]
curr_lr =  [9.361872909698997e-06]
curr_lr = curr_lr =   curr_lr = [9.361872909698997e-06] 
[9.361872909698997e-06]
[9.361872909698997e-06]
curr_lr =  [9.361872909698997e-06]
curr_lr =  [9.361872909698997e-06]
curr_lr =  [9.361872909698997e-06]
curr_lr =  [9.361872909698997e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =     curr_lr =   Epoch: [1][ 16/500] Time 147.185 (147.931)  Loss 7.1483 (7.9139)    VQALoss 0.0000 (0.0000) VGLoss 7.1483 (7.9139)[9.360535117056857e-06]
[9.360535117056857e-06][9.360535117056857e-06][9.360535117056857e-06][9.360535117056857e-06] 
[9.360535117056857e-06]




[9.360535117056857e-06]
curr_lr =  [9.360535117056857e-06]
curr_lr =  curr_lr =  [9.359197324414717e-06]
curr_lr = [9.359197324414717e-06]
 [9.359197324414717e-06]
curr_lr = curr_lr =   [9.359197324414717e-06]
[9.359197324414717e-06]
curr_lr =  [9.359197324414717e-06]
curr_lr =  [9.359197324414717e-06]
curr_lr =  [9.359197324414717e-06]
curr_lr =  curr_lr = curr_lr = curr_lr =  [9.357859531772577e-06] 
 [9.357859531772577e-06][9.357859531772577e-06]

[9.357859531772577e-06]
curr_lr =  curr_lr = [9.357859531772577e-06] 
[9.357859531772577e-06]
curr_lr =  [9.357859531772577e-06]
curr_lr =  [9.357859531772577e-06]
curr_lr = curr_lr =   curr_lr = [9.356521739130435e-06][9.356521739130435e-06]
 
[9.356521739130435e-06]
curr_lr =  [9.356521739130435e-06]
curr_lr =  curr_lr = [9.356521739130435e-06] curr_lr = 
 [9.356521739130435e-06]
[9.356521739130435e-06]
curr_lr =  [9.356521739130435e-06]
[2024-02-20 00:05:41,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=23, lr=[9.355852842809365e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.355183946488295e-06]
curr_lr = curr_lr =   curr_lr = [9.355183946488295e-06] 
[9.355183946488295e-06]
[9.355183946488295e-06]
curr_lr =  curr_lr = [9.355183946488295e-06]
 [9.355183946488295e-06]
curr_lr =  [9.355183946488295e-06]
[2024-02-20 00:05:42,038] [INFO] [timer.py:260:stop] epoch=0/micro_step=10400/global_step=520, RunningAvgSamplesPerSec=17.05932168115571, CurrSamplesPerSec=17.418249013693366, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.355183946488295e-06]
curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr =     curr_lr = [9.353846153846155e-06][9.353846153846155e-06]

[9.353846153846155e-06][9.353846153846155e-06][9.353846153846155e-06]Epoch: [1][ 21/500]        Time 148.740 (148.393)  Loss 6.0479 (7.6085)    VQALoss 0.0000 (0.0000) VGLoss 6.0479 (7.6085)[9.353846153846155e-06]

 


[9.353846153846155e-06]
curr_lr =  [9.353846153846155e-06]
curr_lr = curr_lr =   curr_lr = [9.352508361204015e-06][9.352508361204015e-06]

 [9.352508361204015e-06]curr_lr = 
 curr_lr =  [9.352508361204015e-06]
[9.352508361204015e-06]
curr_lr =  [9.352508361204015e-06]
curr_lr =  [9.352508361204015e-06]
curr_lr =  [9.352508361204015e-06]
curr_lr = curr_lr = curr_lr =   curr_lr =   [9.351170568561873e-06][9.351170568561873e-06]

[9.351170568561873e-06]
[9.351170568561873e-06]
curr_lr = curr_lr =   [9.351170568561873e-06]
[9.351170568561873e-06]
curr_lr =  [9.351170568561873e-06]
curr_lr =  [9.351170568561873e-06]
curr_lr = curr_lr =   [9.349832775919733e-06]
[9.349832775919733e-06]
curr_lr =  curr_lr = [9.349832775919733e-06]
 curr_lr =  [9.349832775919733e-06]
[9.349832775919733e-06]
curr_lr =  [9.349832775919733e-06]
curr_lr =  [9.349832775919733e-06]
curr_lr =  [9.349832775919733e-06]
curr_lr =  curr_lr = [9.348494983277593e-06] 
curr_lr = [9.348494983277593e-06] 
curr_lr =  [9.348494983277593e-06]
[9.348494983277593e-06]
curr_lr = curr_lr =   [9.348494983277593e-06][9.348494983277593e-06]

curr_lr =  [9.348494983277593e-06]
curr_lr =  [9.348494983277593e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  Epoch: [1][ 26/500]  Time 147.590 (148.274)  Loss 4.4359 (7.7124)    VQALoss 0.0000 (0.0000) VGLoss 4.4359 (7.7124) curr_lr = 
  curr_lr =  [9.347157190635453e-06][9.347157190635453e-06]

[9.347157190635453e-06][9.347157190635453e-06] 

 [9.347157190635453e-06]
[9.347157190635453e-06]
[9.347157190635453e-06]
curr_lr =  [9.347157190635453e-06]
curr_lr = curr_lr =   curr_lr = [9.345819397993312e-06]
[9.345819397993312e-06]
 [9.345819397993312e-06]
curr_lr = curr_lr =   [9.345819397993312e-06]
[9.345819397993312e-06]
curr_lr =  [9.345819397993312e-06]
curr_lr = curr_lr =   [9.345819397993312e-06]
[9.345819397993312e-06]
curr_lr =  curr_lr = curr_lr =   curr_lr = [9.34448160535117e-06]
 [9.34448160535117e-06]
[9.34448160535117e-06]
[9.34448160535117e-06]
curr_lr = curr_lr =   [9.34448160535117e-06][9.34448160535117e-06]

curr_lr =  [9.34448160535117e-06]
curr_lr =  [9.34448160535117e-06]
curr_lr = curr_lr =  curr_lr =  [9.34314381270903e-06] 
[9.34314381270903e-06]
curr_lr = [9.34314381270903e-06]
 curr_lr = [9.34314381270903e-06]
 curr_lr = [9.34314381270903e-06]
 [9.34314381270903e-06]
curr_lr =  [9.34314381270903e-06]
curr_lr =  [9.34314381270903e-06]
[2024-02-20 00:30:24,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=23, lr=[9.34247491638796e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.34180602006689e-06]
curr_lr = curr_lr =  curr_lr =   [9.34180602006689e-06]
[9.34180602006689e-06]
[9.34180602006689e-06]
curr_lr =  curr_lr = [9.34180602006689e-06] 
[9.34180602006689e-06]
curr_lr =  [9.34180602006689e-06]
[2024-02-20 00:30:24,516] [INFO] [timer.py:260:stop] epoch=0/micro_step=10600/global_step=530, RunningAvgSamplesPerSec=17.06343948056194, CurrSamplesPerSec=17.42747410738054, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.34180602006689e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =      curr_lr =  [9.34046822742475e-06][9.34046822742475e-06]Epoch: [1][ 31/500]     Time 148.663 (148.206)  Loss 6.0106 (7.4442)    VQALoss 0.0000 (0.0000) VGLoss 6.0106 (7.4442)[9.34046822742475e-06][9.34046822742475e-06]

[9.34046822742475e-06][9.34046822742475e-06]
 



[9.34046822742475e-06]
curr_lr =  [9.34046822742475e-06]
curr_lr =  curr_lr =  curr_lr = [9.33913043478261e-06]
 [9.33913043478261e-06]
[9.33913043478261e-06]
curr_lr = curr_lr =   curr_lr = [9.33913043478261e-06][9.33913043478261e-06]

 [9.33913043478261e-06]
curr_lr =  [9.33913043478261e-06]
curr_lr =  [9.33913043478261e-06]
curr_lr = curr_lr =   [9.337792642140469e-06][9.337792642140469e-06]

curr_lr = curr_lr =  curr_lr =  curr_lr = [9.337792642140469e-06] 
 [9.337792642140469e-06][9.337792642140469e-06]

[9.337792642140469e-06]
curr_lr =  [9.337792642140469e-06]
curr_lr =  [9.337792642140469e-06]
curr_lr = curr_lr =   curr_lr = [9.336454849498328e-06]
[9.336454849498328e-06]
 [9.336454849498328e-06]
curr_lr =  [9.336454849498328e-06]
curr_lr =  [9.336454849498328e-06]curr_lr = 
 curr_lr = [9.336454849498328e-06] 
[9.336454849498328e-06]
curr_lr =  [9.336454849498328e-06]
curr_lr =  curr_lr =  [9.335117056856188e-06]
[9.335117056856188e-06]
curr_lr =  curr_lr = [9.335117056856188e-06]
 [9.335117056856188e-06]curr_lr = 
curr_lr =   [9.335117056856188e-06]
[9.335117056856188e-06]
curr_lr = curr_lr =   [9.335117056856188e-06]
[9.335117056856188e-06]
curr_lr = curr_lr =  curr_lr =  curr_lr = curr_lr = curr_lr = curr_lr =    [9.333779264214048e-06]  [9.333779264214048e-06]

[9.333779264214048e-06]
[9.333779264214048e-06][9.333779264214048e-06][9.333779264214048e-06]
[9.333779264214048e-06]


Epoch: [1][ 36/500]     Time 145.655 (148.007)  Loss 5.2391 (7.8436)    VQALoss 0.0000 (0.0000) VGLoss 5.2391 (7.8436)
curr_lr =  [9.333779264214048e-06]
curr_lr =  curr_lr = [9.332441471571906e-06]
 curr_lr =  [9.332441471571906e-06]
[9.332441471571906e-06]
curr_lr =  curr_lr = [9.332441471571906e-06]
 [9.332441471571906e-06]
curr_lr =  [9.332441471571906e-06]
curr_lr =  [9.332441471571906e-06]
curr_lr =  [9.332441471571906e-06]
curr_lr =  [9.331103678929766e-06]
curr_lr =  curr_lr = curr_lr = [9.331103678929766e-06] 
 [9.331103678929766e-06][9.331103678929766e-06]

curr_lr =  curr_lr =  [9.331103678929766e-06]
[9.331103678929766e-06]
curr_lr =  [9.331103678929766e-06]
curr_lr =  [9.331103678929766e-06]
curr_lr = curr_lr =   curr_lr =  [9.329765886287626e-06][9.329765886287626e-06]

[9.329765886287626e-06]
curr_lr = curr_lr = curr_lr =    [9.329765886287626e-06][9.329765886287626e-06][9.329765886287626e-06]


curr_lr =  [9.329765886287626e-06]
curr_lr =  [9.329765886287626e-06]
[2024-02-20 00:55:08,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=23, lr=[9.329096989966556e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =  curr_lr =   curr_lr = [9.328428093645484e-06]
[9.328428093645484e-06]
 [9.328428093645484e-06]
[9.328428093645484e-06]
curr_lr =  curr_lr = [9.328428093645484e-06]
 [9.328428093645484e-06]
curr_lr =  [9.328428093645484e-06]
[2024-02-20 00:55:08,881] [INFO] [timer.py:260:stop] epoch=0/micro_step=10800/global_step=540, RunningAvgSamplesPerSec=17.067013521760543, CurrSamplesPerSec=17.38966100363435, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.328428093645484e-06]
curr_lr = curr_lr =  curr_lr = curr_lr = curr_lr =   curr_lr =   curr_lr = [9.327090301003346e-06]Epoch: [1][ 41/500]   Time 148.648 (148.863)  Loss 3.7963 (7.7496)    VQALoss 0.0000 (0.0000) VGLoss 3.7963 (7.7496) 
[9.327090301003346e-06] 
[9.327090301003346e-06][9.327090301003346e-06]

[9.327090301003346e-06]
[9.327090301003346e-06]
[9.327090301003346e-06]

curr_lr =  [9.327090301003346e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =   [9.325752508361204e-06] [9.325752508361204e-06]

[9.325752508361204e-06][9.325752508361204e-06]

curr_lr = curr_lr =   [9.325752508361204e-06][9.325752508361204e-06]

curr_lr =  [9.325752508361204e-06]
curr_lr =  [9.325752508361204e-06]
curr_lr =  curr_lr = curr_lr = curr_lr = [9.324414715719064e-06]
   [9.324414715719064e-06]
[9.324414715719064e-06][9.324414715719064e-06]

curr_lr = curr_lr =   [9.324414715719064e-06]
[9.324414715719064e-06]
curr_lr =  [9.324414715719064e-06]
curr_lr =  [9.324414715719064e-06]
curr_lr =  curr_lr = [9.323076923076924e-06]curr_lr = 
  [9.323076923076924e-06][9.323076923076924e-06]

curr_lr =  curr_lr = [9.323076923076924e-06]
 [9.323076923076924e-06]
curr_lr =  [9.323076923076924e-06]
curr_lr =  curr_lr =  [9.323076923076924e-06]
[9.323076923076924e-06]
curr_lr =  curr_lr = [9.321739130434784e-06]
curr_lr =   [9.321739130434784e-06]
[9.321739130434784e-06]
curr_lr =  [9.321739130434784e-06]
curr_lr =  curr_lr = [9.321739130434784e-06] 
[9.321739130434784e-06]
curr_lr =  [9.321739130434784e-06]
curr_lr =  [9.321739130434784e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr =     curr_lr = [9.320401337792642e-06][9.320401337792642e-06][9.320401337792642e-06][9.320401337792642e-06][9.320401337792642e-06][9.320401337792642e-06]


 


[9.320401337792642e-06]
Epoch: [1][ 46/500]     Time 148.740 (148.366)  Loss 5.5285 (7.8026)    VQALoss 0.0000 (0.0000) VGLoss 5.5285 (7.8026)
curr_lr =  [9.320401337792642e-06]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.319063545150502e-06] [9.319063545150502e-06]

 [9.319063545150502e-06]
[9.319063545150502e-06]
curr_lr = curr_lr =   [9.319063545150502e-06]
[9.319063545150502e-06]
curr_lr =  [9.319063545150502e-06]
curr_lr =  [9.319063545150502e-06]
curr_lr = curr_lr =   [9.317725752508362e-06][9.317725752508362e-06]

curr_lr = curr_lr =   curr_lr = curr_lr = [9.317725752508362e-06] 
 [9.317725752508362e-06]
[9.317725752508362e-06][9.317725752508362e-06]

curr_lr =  [9.317725752508362e-06]
curr_lr =  [9.317725752508362e-06]
curr_lr = curr_lr =  curr_lr =  curr_lr =   [9.31638795986622e-06]
[9.31638795986622e-06][9.31638795986622e-06]

[9.31638795986622e-06]
curr_lr = curr_lr =   [9.31638795986622e-06]
[9.31638795986622e-06]
curr_lr =  [9.31638795986622e-06]
curr_lr =  [9.31638795986622e-06]
[2024-02-20 01:19:51,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=23, lr=[9.315719063545152e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =  curr_lr =   [9.315050167224082e-06]
[9.315050167224082e-06][9.315050167224082e-06]

curr_lr =  [9.315050167224082e-06]
curr_lr = curr_lr =   [9.315050167224082e-06]
curr_lr = [9.315050167224082e-06]
 [9.315050167224082e-06]
[2024-02-20 01:19:51,344] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=550, RunningAvgSamplesPerSec=17.070865938073073, CurrSamplesPerSec=17.37511811225959, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.315050167224082e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr =    curr_lr =  [9.31371237458194e-06]Epoch: [1][ 51/500]   Time 148.713 (148.139)  Loss 4.0794 (7.9839)    VQALoss 0.0000 (0.0000) VGLoss 4.0794 (7.9839)[9.31371237458194e-06]
[9.31371237458194e-06][9.31371237458194e-06]
 [9.31371237458194e-06]

[9.31371237458194e-06]


[9.31371237458194e-06]
curr_lr =  [9.31371237458194e-06]
[2024-02-20 01:22:24,165] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step551 is about to be saved!
[2024-02-20 01:22:36,259] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_1/global_step551/mp_rank_00_model_states.pt
[2024-02-20 01:22:36,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step551/mp_rank_00_model_states.pt...
[2024-02-20 01:24:57,351] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step551/mp_rank_00_model_states.pt.
[2024-02-20 01:24:58,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step551/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-20 01:24:58,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step551/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-20 01:24:58,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step551/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-20 01:24:58,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step551/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-20 01:24:58,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step551/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-20 01:24:58,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step551/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-20 01:24:58,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step551/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-20 01:24:58,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step551/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-20 01:25:02,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step551/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-20 01:25:02,905] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step551/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-20 01:25:02,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step551 is ready now!
[2024-02-20 01:25:02,939] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step551/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-20 01:25:02,940] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step551/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-20 01:25:02,940] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step551 is ready now!
[2024-02-20 01:25:02,948] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step551/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-20 01:25:02,948] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step551/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-20 01:25:02,949] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step551 is ready now!
[2024-02-20 01:25:02,982] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step551/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-20 01:25:02,983] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step551/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-20 01:25:02,983] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step551 is ready now!
[2024-02-20 01:25:03,138] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step551/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-20 01:25:03,149] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step551/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-20 01:25:03,149] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step551 is ready now!
[2024-02-20 01:25:03,170] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step551/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-20 01:25:03,170] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step551/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-20 01:25:03,170] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step551 is ready now!
[2024-02-20 01:25:03,172] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step551/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-20 01:25:03,172] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step551/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-20 01:25:03,172] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step551 is ready now!
[2024-02-20 01:25:03,219] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step551/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-20 01:25:03,219] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step551/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-20 01:25:03,219] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step551 is ready now!
curr_lr = curr_lr =  curr_lr =  curr_lr =  [9.3123745819398e-06] 
[9.3123745819398e-06][9.3123745819398e-06]

[9.3123745819398e-06]
curr_lr = curr_lr =   [9.3123745819398e-06]
[9.3123745819398e-06]
curr_lr =  [9.3123745819398e-06]
curr_lr =  [9.3123745819398e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     [9.31103678929766e-06][9.31103678929766e-06]
[9.31103678929766e-06]

[9.31103678929766e-06]
curr_lr =  curr_lr =  [9.31103678929766e-06]
[9.31103678929766e-06]
curr_lr =  [9.31103678929766e-06]
curr_lr =  [9.31103678929766e-06]
curr_lr =  curr_lr =  [9.30969899665552e-06]
[9.30969899665552e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =    [9.30969899665552e-06]
[9.30969899665552e-06][9.30969899665552e-06][9.30969899665552e-06]


curr_lr =  [9.30969899665552e-06]
curr_lr =  [9.30969899665552e-06]
curr_lr =  [9.308361204013378e-06]
curr_lr =  [9.308361204013378e-06]
curr_lr =  [9.308361204013378e-06]
curr_lr =  [9.308361204013378e-06]
curr_lr = curr_lr =   [9.308361204013378e-06][9.308361204013378e-06]

curr_lr =  [9.308361204013378e-06]
curr_lr =  [9.308361204013378e-06]
curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr =  curr_lr =  curr_lr =   [9.307023411371238e-06][9.307023411371238e-06] 

[9.307023411371238e-06][9.307023411371238e-06]
[9.307023411371238e-06]

Epoch: [1][ 56/500]     Time 148.541 (180.821)  Loss 21.5748 (7.3477)   VQALoss 0.0000 (0.0000) VGLoss 21.5748 (7.3477)[9.307023411371238e-06][9.307023411371238e-06]


curr_lr =  [9.307023411371238e-06]
curr_lr = curr_lr =   [9.305685618729098e-06]
[9.305685618729098e-06]
curr_lr =  [9.305685618729098e-06]
curr_lr =  curr_lr = curr_lr = [9.305685618729098e-06] 
 [9.305685618729098e-06]
[9.305685618729098e-06]
curr_lr =  [9.305685618729098e-06]
curr_lr =  [9.305685618729098e-06]
curr_lr = curr_lr =  curr_lr =   [9.304347826086956e-06]
[9.304347826086956e-06]
[9.304347826086956e-06]
curr_lr =  [9.304347826086956e-06]
curr_lr =  curr_lr =  [9.304347826086956e-06]
[9.304347826086956e-06]
curr_lr =  [9.304347826086956e-06]
curr_lr =  [9.304347826086956e-06]
curr_lr =  curr_lr = [9.303010033444818e-06]curr_lr = 
  [9.303010033444818e-06]
curr_lr = [9.303010033444818e-06]
 curr_lr =  [9.303010033444818e-06]
[9.303010033444818e-06]
curr_lr =  [9.303010033444818e-06]
curr_lr =  [9.303010033444818e-06]
curr_lr =  [9.303010033444818e-06]
[2024-02-20 01:47:16,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=23, lr=[9.302341137123748e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =   curr_lr =  [9.301672240802676e-06]curr_lr = 
[9.301672240802676e-06]
[9.301672240802676e-06]
 [9.301672240802676e-06]
curr_lr = curr_lr =   [9.301672240802676e-06]
[9.301672240802676e-06]
curr_lr =  [9.301672240802676e-06]
[2024-02-20 01:47:16,660] [INFO] [timer.py:260:stop] epoch=0/micro_step=11200/global_step=560, RunningAvgSamplesPerSec=17.07461873507823, CurrSamplesPerSec=17.269330810868684, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.301672240802676e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =  curr_lr =     [9.300334448160536e-06]
[9.300334448160536e-06]
[9.300334448160536e-06][9.300334448160536e-06] 
[9.300334448160536e-06][9.300334448160536e-06]


Epoch: [1][ 61/500]     Time 148.826 (148.264)  Loss 5.8659 (7.4372)    VQALoss 0.0000 (0.0000) VGLoss 5.8659 (7.4372)[9.300334448160536e-06]

curr_lr =  [9.300334448160536e-06]
curr_lr =  curr_lr = curr_lr = [9.298996655518396e-06]curr_lr = 
   [9.298996655518396e-06][9.298996655518396e-06]

[9.298996655518396e-06]
curr_lr =  [9.298996655518396e-06]
curr_lr =  [9.298996655518396e-06]
curr_lr =  [9.298996655518396e-06]
curr_lr =  [9.298996655518396e-06]
curr_lr =  curr_lr =  [9.297658862876256e-06]
curr_lr = [9.297658862876256e-06] 
[9.297658862876256e-06]
curr_lr =  curr_lr = curr_lr = [9.297658862876256e-06] 
 [9.297658862876256e-06]
[9.297658862876256e-06]
curr_lr =  [9.297658862876256e-06]
curr_lr =  [9.297658862876256e-06]
curr_lr =  curr_lr =  [9.296321070234114e-06]curr_lr = 
curr_lr =  [9.296321070234114e-06] 
[9.296321070234114e-06]
[9.296321070234114e-06]
curr_lr =  [9.296321070234114e-06]
curr_lr =  [9.296321070234114e-06]
curr_lr =  [9.296321070234114e-06]
curr_lr =  [9.296321070234114e-06]
curr_lr =  curr_lr = curr_lr =   [9.294983277591974e-06]
[9.294983277591974e-06][9.294983277591974e-06]

curr_lr =  curr_lr = [9.294983277591974e-06]
 curr_lr = [9.294983277591974e-06] 
[9.294983277591974e-06]
curr_lr =  [9.294983277591974e-06]
curr_lr =  [9.294983277591974e-06]
curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr = curr_lr =      [9.293645484949834e-06]
[9.293645484949834e-06]
[9.293645484949834e-06]curr_lr = [9.293645484949834e-06][9.293645484949834e-06]
Epoch: [1][ 66/500]     Time 148.471 (148.365)  Loss 23.1435 (7.7000)   VQALoss 0.0000 (0.0000) VGLoss 23.1435 (7.7000)

[9.293645484949834e-06]
 
[9.293645484949834e-06]
curr_lr =  [9.293645484949834e-06]
curr_lr =  curr_lr = curr_lr = [9.292307692307694e-06] 
curr_lr =   [9.292307692307694e-06]
[9.292307692307694e-06]
[9.292307692307694e-06]
curr_lr = curr_lr =   [9.292307692307694e-06]
[9.292307692307694e-06]
curr_lr =  [9.292307692307694e-06]
curr_lr =  [9.292307692307694e-06]
curr_lr =  [9.290969899665553e-06]
curr_lr = curr_lr =   [9.290969899665553e-06][9.290969899665553e-06]

curr_lr =  curr_lr = [9.290969899665553e-06]curr_lr = 
  [9.290969899665553e-06]
[9.290969899665553e-06]
curr_lr =  [9.290969899665553e-06]
curr_lr =  [9.290969899665553e-06]
curr_lr =  [9.289632107023412e-06]
curr_lr =  curr_lr = [9.289632107023412e-06]
 curr_lr = [9.289632107023412e-06] 
[9.289632107023412e-06]
curr_lr =  [9.289632107023412e-06]
curr_lr =  [9.289632107023412e-06]
curr_lr =  [9.289632107023412e-06]
curr_lr =  [9.289632107023412e-06]
[2024-02-20 02:11:58,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=23, lr=[9.288963210702342e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =  curr_lr =   [9.288294314381272e-06]
[9.288294314381272e-06][9.288294314381272e-06]

curr_lr =  [9.288294314381272e-06]
curr_lr = curr_lr =   [9.288294314381272e-06]
[9.288294314381272e-06]
curr_lr =  [9.288294314381272e-06]
[2024-02-20 02:11:59,060] [INFO] [timer.py:260:stop] epoch=0/micro_step=11400/global_step=570, RunningAvgSamplesPerSec=17.078210705128427, CurrSamplesPerSec=17.380838761296626, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.288294314381272e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =       Epoch: [1][ 71/500] Time 148.485 (148.047)  Loss 24.2181 (7.7292)   VQALoss 0.0000 (0.0000) VGLoss 24.2181 (7.7292) [9.286956521739131e-06][9.286956521739131e-06]

[9.286956521739131e-06][9.286956521739131e-06][9.286956521739131e-06]
[9.286956521739131e-06]


[9.286956521739131e-06]

curr_lr =  [9.286956521739131e-06]
curr_lr = curr_lr =   [9.285618729096991e-06]
[9.285618729096991e-06]
curr_lr =  curr_lr = [9.285618729096991e-06]
 [9.285618729096991e-06]
curr_lr =  curr_lr = [9.285618729096991e-06]
 [9.285618729096991e-06]
curr_lr =  [9.285618729096991e-06]
curr_lr =  [9.285618729096991e-06]
curr_lr = curr_lr =   [9.28428093645485e-06]
curr_lr = [9.28428093645485e-06]
 [9.28428093645485e-06]
curr_lr =  [9.28428093645485e-06]
curr_lr =  [9.28428093645485e-06]curr_lr = 
 [9.28428093645485e-06]
curr_lr =  [9.28428093645485e-06]
curr_lr =  [9.28428093645485e-06]
curr_lr = curr_lr =   [9.28294314381271e-06][9.28294314381271e-06]

curr_lr =  curr_lr = [9.28294314381271e-06] curr_lr = 
 curr_lr =  [9.28294314381271e-06]
[9.28294314381271e-06]
[9.28294314381271e-06]
curr_lr =  [9.28294314381271e-06]
curr_lr =  [9.28294314381271e-06]
curr_lr = curr_lr =   [9.28160535117057e-06]
[9.28160535117057e-06]curr_lr = 
 curr_lr = [9.28160535117057e-06] 
[9.28160535117057e-06]
curr_lr =  [9.28160535117057e-06]
curr_lr =  [9.28160535117057e-06]
curr_lr =  [9.28160535117057e-06]
curr_lr =  [9.28160535117057e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =       curr_lr = [9.28026755852843e-06][9.28026755852843e-06]Epoch: [1][ 76/500]     Time 148.610 (148.478)  Loss 4.2109 (7.6746)    VQALoss 0.0000 (0.0000) VGLoss 4.2109 (7.6746)[9.28026755852843e-06] [9.28026755852843e-06]

[9.28026755852843e-06]
[9.28026755852843e-06]



[9.28026755852843e-06]
curr_lr =  [9.28026755852843e-06]
curr_lr =  curr_lr = curr_lr =  [9.27892976588629e-06]
curr_lr =  [9.27892976588629e-06] 
[9.27892976588629e-06]
[9.27892976588629e-06]
curr_lr =  [9.27892976588629e-06]
curr_lr =  curr_lr = [9.27892976588629e-06] 
[9.27892976588629e-06]
curr_lr =  [9.27892976588629e-06]
curr_lr = curr_lr = curr_lr =    [9.277591973244147e-06]curr_lr = [9.277591973244147e-06][9.277591973244147e-06]


 [9.277591973244147e-06]
curr_lr = curr_lr =   [9.277591973244147e-06]
[9.277591973244147e-06]
curr_lr =  [9.277591973244147e-06]
curr_lr =  [9.277591973244147e-06]
curr_lr = curr_lr = curr_lr =    [9.276254180602007e-06]
[9.276254180602007e-06]
curr_lr = [9.276254180602007e-06]
 [9.276254180602007e-06]
curr_lr = curr_lr =   [9.276254180602007e-06]
[9.276254180602007e-06]
curr_lr =  [9.276254180602007e-06]
curr_lr =  [9.276254180602007e-06]
[2024-02-20 02:36:42,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=23, lr=[9.275585284280937e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = curr_lr = [9.274916387959867e-06]
  [9.274916387959867e-06]
[9.274916387959867e-06]
curr_lr =  curr_lr =  [9.274916387959867e-06]
[9.274916387959867e-06]
curr_lr =  [9.274916387959867e-06]
curr_lr =  [9.274916387959867e-06]
[2024-02-20 02:36:42,352] [INFO] [timer.py:260:stop] epoch=0/micro_step=11600/global_step=580, RunningAvgSamplesPerSec=17.081508069796485, CurrSamplesPerSec=17.42007894487838, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.274916387959867e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     curr_lr = [9.273578595317725e-06][9.273578595317725e-06][9.273578595317725e-06]
Epoch: [1][ 81/500]     Time 148.942 (148.272)  Loss 15.6324 (7.7204)   VQALoss 0.0000 (0.0000) VGLoss 15.6324 (7.7204)[9.273578595317725e-06]


curr_lr = 
 curr_lr =   [9.273578595317725e-06][9.273578595317725e-06]

[9.273578595317725e-06]
curr_lr =  [9.273578595317725e-06]
curr_lr =  curr_lr = curr_lr = [9.272240802675585e-06]  
[9.272240802675585e-06][9.272240802675585e-06]

curr_lr =  [9.272240802675585e-06]
curr_lr =  curr_lr =  [9.272240802675585e-06]
[9.272240802675585e-06]
curr_lr =  [9.272240802675585e-06]
curr_lr =  [9.272240802675585e-06]
curr_lr =  curr_lr = [9.270903010033445e-06]
 curr_lr = [9.270903010033445e-06]curr_lr = 
curr_lr =    [9.270903010033445e-06]
[9.270903010033445e-06]
[9.270903010033445e-06]
curr_lr =  [9.270903010033445e-06]
curr_lr =  [9.270903010033445e-06]
curr_lr =  [9.270903010033445e-06]
curr_lr = curr_lr =   curr_lr =  [9.269565217391305e-06][9.269565217391305e-06]

curr_lr = [9.269565217391305e-06]
 [9.269565217391305e-06]
curr_lr =  [9.269565217391305e-06]
curr_lr =  curr_lr = [9.269565217391305e-06]
 [9.269565217391305e-06]
curr_lr =  [9.269565217391305e-06]
curr_lr =  [9.268227424749165e-06]
curr_lr = curr_lr =   curr_lr = [9.268227424749165e-06][9.268227424749165e-06]
 
[9.268227424749165e-06]
curr_lr =  [9.268227424749165e-06]
curr_lr =  [9.268227424749165e-06]
curr_lr =  [9.268227424749165e-06]
curr_lr =  [9.268227424749165e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =      [9.266889632107025e-06]
 [9.266889632107025e-06][9.266889632107025e-06][9.266889632107025e-06][9.266889632107025e-06]
[9.266889632107025e-06]



[9.266889632107025e-06]
Epoch: [1][ 86/500]     Time 148.812 (148.355)  Loss 3.4336 (8.2674)    VQALoss 0.0000 (0.0000) VGLoss 3.4336 (8.2674)
curr_lr =  [9.266889632107025e-06]
curr_lr =  curr_lr =  [9.265551839464883e-06]
[9.265551839464883e-06]
curr_lr =  [9.265551839464883e-06]
curr_lr =  [9.265551839464883e-06]
curr_lr =  curr_lr = [9.265551839464883e-06]
 [9.265551839464883e-06]
curr_lr =  [9.265551839464883e-06]
curr_lr =  [9.265551839464883e-06]
curr_lr = curr_lr =  curr_lr =   [9.264214046822743e-06]
curr_lr = [9.264214046822743e-06][9.264214046822743e-06]

 [9.264214046822743e-06]
curr_lr =  [9.264214046822743e-06]
curr_lr = curr_lr =   [9.264214046822743e-06]
[9.264214046822743e-06]
curr_lr =  [9.264214046822743e-06]
curr_lr = curr_lr =  curr_lr =   curr_lr =  [9.262876254180603e-06][9.262876254180603e-06][9.262876254180603e-06]


[9.262876254180603e-06]
curr_lr =  [9.262876254180603e-06]
curr_lr =  [9.262876254180603e-06]
curr_lr = curr_lr =   [9.262876254180603e-06]
[9.262876254180603e-06]
[2024-02-20 03:01:25,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=23, lr=[9.262207357859533e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.261538461538461e-06]
curr_lr =  [9.261538461538461e-06]
curr_lr =  [9.261538461538461e-06]
curr_lr =  [9.261538461538461e-06]
curr_lr =  [9.261538461538461e-06]
curr_lr =  [9.261538461538461e-06]
curr_lr =  [9.261538461538461e-06]
[2024-02-20 03:01:25,611] [INFO] [timer.py:260:stop] epoch=0/micro_step=11800/global_step=590, RunningAvgSamplesPerSec=17.084682487454955, CurrSamplesPerSec=17.387190341639606, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.261538461538461e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =    curr_lr =    [9.260200668896321e-06][9.260200668896321e-06][9.260200668896321e-06]


[9.260200668896321e-06][9.260200668896321e-06][9.260200668896321e-06]


[9.260200668896321e-06]
Epoch: [1][ 91/500]     Time 148.720 (148.252)  Loss 24.7149 (8.3255)   VQALoss 0.0000 (0.0000) VGLoss 24.7149 (8.3255)
curr_lr =  [9.260200668896321e-06]
curr_lr = curr_lr =   curr_lr = [9.258862876254181e-06]
 [9.258862876254181e-06]
curr_lr =  [9.258862876254181e-06]
[9.258862876254181e-06]
curr_lr =  curr_lr =  [9.258862876254181e-06]
[9.258862876254181e-06]
curr_lr =  [9.258862876254181e-06]
curr_lr =  [9.258862876254181e-06]
curr_lr = curr_lr =  curr_lr =  [9.257525083612041e-06] 
curr_lr = [9.257525083612041e-06]
[9.257525083612041e-06]curr_lr =  
 curr_lr =  [9.257525083612041e-06][9.257525083612041e-06]

[9.257525083612041e-06]
curr_lr =  [9.257525083612041e-06]
curr_lr =  [9.257525083612041e-06]
curr_lr = curr_lr = curr_lr =    [9.2561872909699e-06]
[9.2561872909699e-06]
curr_lr = [9.2561872909699e-06]
curr_lr =   [9.2561872909699e-06][9.2561872909699e-06]

curr_lr =  [9.2561872909699e-06]
curr_lr =  [9.2561872909699e-06]
curr_lr =  [9.2561872909699e-06]
curr_lr =  curr_lr = curr_lr =  [9.25484949832776e-06]
 [9.25484949832776e-06]
[9.25484949832776e-06]
curr_lr = curr_lr =   [9.25484949832776e-06][9.25484949832776e-06]

curr_lr =  [9.25484949832776e-06]
curr_lr =  [9.25484949832776e-06]
curr_lr =  [9.25484949832776e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =        [9.253511705685619e-06][9.253511705685619e-06][9.253511705685619e-06][9.253511705685619e-06]


[9.253511705685619e-06][9.253511705685619e-06][9.253511705685619e-06]



Epoch: [1][ 96/500]     Time 148.198 (148.377)  Loss 6.0554 (7.8416)    VQALoss 0.0000 (0.0000) VGLoss 6.0554 (7.8416)
curr_lr =  [9.253511705685619e-06]
curr_lr = curr_lr =   curr_lr =  [9.252173913043479e-06][9.252173913043479e-06]

[9.252173913043479e-06]
curr_lr = curr_lr =   curr_lr = [9.252173913043479e-06][9.252173913043479e-06]

 [9.252173913043479e-06]
curr_lr =  [9.252173913043479e-06]
curr_lr =  [9.252173913043479e-06]
curr_lr = curr_lr =   [9.250836120401339e-06][9.250836120401339e-06]

curr_lr =  curr_lr =  [9.250836120401339e-06]
[9.250836120401339e-06]
curr_lr = curr_lr =   curr_lr = [9.250836120401339e-06]
 [9.250836120401339e-06]
[9.250836120401339e-06]
curr_lr =  [9.250836120401339e-06]
curr_lr =  curr_lr =  [9.249498327759197e-06]
curr_lr = [9.249498327759197e-06] 
[9.249498327759197e-06]
curr_lr =  curr_lr =  [9.249498327759197e-06]curr_lr = 
[9.249498327759197e-06] 
[9.249498327759197e-06]
curr_lr =  [9.249498327759197e-06]
curr_lr =  [9.249498327759197e-06]
[2024-02-20 03:26:08,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=23, lr=[9.248829431438127e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr =  [9.248160535117059e-06]
curr_lr = [9.248160535117059e-06]
 [9.248160535117059e-06]
curr_lr =  curr_lr = [9.248160535117059e-06] 
[9.248160535117059e-06]
curr_lr = curr_lr =   [9.248160535117059e-06]
[9.248160535117059e-06]
[2024-02-20 03:26:08,784] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=600, RunningAvgSamplesPerSec=17.08777246084904, CurrSamplesPerSec=17.341087974852385, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.248160535117059e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr = curr_lr =     Epoch: [1][101/500] Time 148.763 (148.266)  Loss 25.9377 (7.7656)   VQALoss 0.0000 (0.0000) VGLoss 25.9377 (7.7656) [9.246822742474917e-06][9.246822742474917e-06]


[9.246822742474917e-06][9.246822742474917e-06][9.246822742474917e-06]


[9.246822742474917e-06][9.246822742474917e-06]

curr_lr =  [9.246822742474917e-06]
[2024-02-20 03:28:41,840] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step601 is about to be saved!
[2024-02-20 03:28:53,741] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_1/global_step601/mp_rank_00_model_states.pt
[2024-02-20 03:28:53,741] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step601/mp_rank_00_model_states.pt...
[2024-02-20 03:31:11,498] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step601/mp_rank_00_model_states.pt.
[2024-02-20 03:31:12,809] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step601/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-20 03:31:12,809] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step601/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-20 03:31:12,809] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step601/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-20 03:31:12,809] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step601/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-20 03:31:12,809] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step601/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-20 03:31:12,809] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step601/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-20 03:31:12,809] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step601/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-20 03:31:12,809] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step601/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-20 03:31:16,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step601/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-20 03:31:16,819] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step601/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-20 03:31:16,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step601 is ready now!
[2024-02-20 03:31:16,839] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step601/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-20 03:31:16,839] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step601/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-20 03:31:16,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step601 is ready now!
[2024-02-20 03:31:16,882] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step601/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-20 03:31:16,882] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step601/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-20 03:31:16,882] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step601 is ready now!
[2024-02-20 03:31:16,923] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step601/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-20 03:31:16,923] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step601/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-20 03:31:16,923] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step601 is ready now!
[2024-02-20 03:31:16,964] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step601/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-20 03:31:16,964] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step601/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-20 03:31:16,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step601 is ready now!
[2024-02-20 03:31:17,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step601/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-20 03:31:17,105] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step601/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-20 03:31:17,105] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step601 is ready now!
[2024-02-20 03:31:17,136] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step601/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-20 03:31:17,136] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step601/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-20 03:31:17,136] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step601 is ready now!
[2024-02-20 03:31:17,197] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step601/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-20 03:31:17,210] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step601/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-20 03:31:17,210] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step601 is ready now!
curr_lr = curr_lr =  curr_lr = curr_lr =   [9.245484949832777e-06]
 [9.245484949832777e-06]
[9.245484949832777e-06][9.245484949832777e-06]

curr_lr =  curr_lr =  [9.245484949832777e-06]
[9.245484949832777e-06]
curr_lr =  [9.245484949832777e-06]curr_lr = 
 [9.245484949832777e-06]
curr_lr =  curr_lr = [9.244147157190637e-06] 
[9.244147157190637e-06]curr_lr = 
 curr_lr = [9.244147157190637e-06]
 [9.244147157190637e-06]
curr_lr =  [9.244147157190637e-06]
curr_lr = curr_lr =   [9.244147157190637e-06]
[9.244147157190637e-06]
curr_lr =  [9.244147157190637e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =    [9.242809364548497e-06][9.242809364548497e-06]
[9.242809364548497e-06][9.242809364548497e-06]


curr_lr =  [9.242809364548497e-06]
curr_lr =  [9.242809364548497e-06]
curr_lr =  [9.242809364548497e-06]
curr_lr =  [9.242809364548497e-06]
curr_lr = curr_lr =   [9.241471571906355e-06]
[9.241471571906355e-06]
curr_lr =  curr_lr =  [9.241471571906355e-06]
[9.241471571906355e-06]
curr_lr =  curr_lr =  [9.241471571906355e-06]
[9.241471571906355e-06]
curr_lr =  [9.241471571906355e-06]
curr_lr =  [9.241471571906355e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =  curr_lr =  Epoch: [1][106/500]      Time 148.538 (180.103)  Loss 4.5666 (7.7706)    VQALoss 0.0000 (0.0000) VGLoss 4.5666 (7.7706)  
[9.240133779264215e-06][9.240133779264215e-06]

 [9.240133779264215e-06]
[9.240133779264215e-06]
[9.240133779264215e-06]
curr_lr = [9.240133779264215e-06]
 [9.240133779264215e-06]
curr_lr =  [9.240133779264215e-06]
curr_lr =  curr_lr = [9.238795986622075e-06] 
curr_lr = [9.238795986622075e-06]
 [9.238795986622075e-06]
curr_lr =  [9.238795986622075e-06]
curr_lr =  curr_lr = [9.238795986622075e-06]
 curr_lr = [9.238795986622075e-06] 
[9.238795986622075e-06]
curr_lr =  [9.238795986622075e-06]
curr_lr =  [9.237458193979933e-06]curr_lr = 
 curr_lr = curr_lr = [9.237458193979933e-06]
  [9.237458193979933e-06]
[9.237458193979933e-06]
curr_lr =  [9.237458193979933e-06]
curr_lr =  curr_lr = [9.237458193979933e-06]
 [9.237458193979933e-06]
curr_lr =  [9.237458193979933e-06]
curr_lr =  curr_lr = [9.236120401337794e-06]
 [9.236120401337794e-06]
curr_lr =  [9.236120401337794e-06]
curr_lr = curr_lr =   curr_lr =  [9.236120401337794e-06][9.236120401337794e-06]

[9.236120401337794e-06]
curr_lr =  [9.236120401337794e-06]
curr_lr =  [9.236120401337794e-06]
[2024-02-20 03:53:29,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=23, lr=[9.235451505016724e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = curr_lr =  [9.234782608695653e-06] 
curr_lr = [9.234782608695653e-06]
 [9.234782608695653e-06]
[9.234782608695653e-06]
curr_lr =  curr_lr = [9.234782608695653e-06]
 [9.234782608695653e-06]
curr_lr =  [9.234782608695653e-06]
[2024-02-20 03:53:29,520] [INFO] [timer.py:260:stop] epoch=0/micro_step=12200/global_step=610, RunningAvgSamplesPerSec=17.091154771041605, CurrSamplesPerSec=17.424575093490596, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.234782608695653e-06]
curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =   curr_lr =   [9.233444816053512e-06]Epoch: [1][111/500]     Time 148.598 (148.011)  Loss 4.8338 (7.9139)    VQALoss 0.0000 (0.0000) VGLoss 4.8338 (7.9139)[9.233444816053512e-06] 
[9.233444816053512e-06]

[9.233444816053512e-06][9.233444816053512e-06]


[9.233444816053512e-06]
curr_lr =  [9.233444816053512e-06]
curr_lr =  [9.233444816053512e-06]
curr_lr =  curr_lr = curr_lr =  [9.232107023411372e-06] 
[9.232107023411372e-06][9.232107023411372e-06]

curr_lr = curr_lr =  curr_lr =   [9.232107023411372e-06]
[9.232107023411372e-06][9.232107023411372e-06]

curr_lr =  [9.232107023411372e-06]
curr_lr =  [9.232107023411372e-06]
curr_lr = curr_lr =   curr_lr =  [9.230769230769232e-06]
curr_lr = [9.230769230769232e-06]
 [9.230769230769232e-06]
curr_lr =  [9.230769230769232e-06]
[9.230769230769232e-06]
curr_lr =  [9.230769230769232e-06]
curr_lr =  [9.230769230769232e-06]
curr_lr =  [9.230769230769232e-06]
curr_lr =  [9.22943143812709e-06]curr_lr = 
 [9.22943143812709e-06]
curr_lr =  [9.22943143812709e-06]
curr_lr = curr_lr =   [9.22943143812709e-06][9.22943143812709e-06]

curr_lr =  [9.22943143812709e-06]
curr_lr =  [9.22943143812709e-06]
curr_lr =  [9.22943143812709e-06]
curr_lr = curr_lr = curr_lr =    curr_lr = [9.22809364548495e-06][9.22809364548495e-06]

 [9.22809364548495e-06]
[9.22809364548495e-06]
curr_lr =  [9.22809364548495e-06]
curr_lr =  [9.22809364548495e-06]
curr_lr =  [9.22809364548495e-06]
curr_lr =  [9.22809364548495e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =    curr_lr =   [9.22675585284281e-06]
 [9.22675585284281e-06][9.22675585284281e-06][9.22675585284281e-06]


Epoch: [1][116/500]     Time 148.921 (148.393)  Loss 4.5211 (7.4716)    VQALoss 0.0000 (0.0000) VGLoss 4.5211 (7.4716)[9.22675585284281e-06][9.22675585284281e-06]
[9.22675585284281e-06]


curr_lr =  [9.22675585284281e-06]
curr_lr = curr_lr = curr_lr =    curr_lr = [9.225418060200669e-06][9.225418060200669e-06]

[9.225418060200669e-06] 
[9.225418060200669e-06]
curr_lr =  curr_lr = [9.225418060200669e-06]
 [9.225418060200669e-06]
curr_lr =  [9.225418060200669e-06]
curr_lr =  [9.225418060200669e-06]
curr_lr = curr_lr =  curr_lr =  curr_lr =   [9.22408026755853e-06][9.22408026755853e-06]

[9.22408026755853e-06]
[9.22408026755853e-06]
curr_lr =  [9.22408026755853e-06]
curr_lr =  [9.22408026755853e-06]
curr_lr =  [9.22408026755853e-06]
curr_lr =  [9.22408026755853e-06]
curr_lr = curr_lr =   [9.222742474916388e-06]
[9.222742474916388e-06]
curr_lr =  [9.222742474916388e-06]
curr_lr =  [9.222742474916388e-06]
curr_lr = curr_lr =   [9.222742474916388e-06][9.222742474916388e-06]

curr_lr =  curr_lr = [9.222742474916388e-06]
 [9.222742474916388e-06]
[2024-02-20 04:18:12,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=23, lr=[9.222073578595318e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.221404682274248e-06]
curr_lr = curr_lr =   [9.221404682274248e-06]
[9.221404682274248e-06]
curr_lr =  curr_lr = [9.221404682274248e-06]
curr_lr =   [9.221404682274248e-06]
[9.221404682274248e-06]
curr_lr =  [9.221404682274248e-06]
[2024-02-20 04:18:12,617] [INFO] [timer.py:260:stop] epoch=0/micro_step=12400/global_step=620, RunningAvgSamplesPerSec=17.094050103246918, CurrSamplesPerSec=17.364726336750774, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.221404682274248e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =   curr_lr = Epoch: [1][121/500]        Time 148.101 (148.127)  Loss 4.3840 (7.6159)    VQALoss 0.0000 (0.0000) VGLoss 4.3840 (7.6159) [9.220066889632108e-06]curr_lr = curr_lr =  [9.220066889632108e-06][9.220066889632108e-06]

[9.220066889632108e-06]  


[9.220066889632108e-06][9.220066889632108e-06]

[9.220066889632108e-06]
curr_lr =  [9.220066889632108e-06]
curr_lr =  curr_lr = curr_lr =   [9.218729096989966e-06]
[9.218729096989966e-06][9.218729096989966e-06]

curr_lr =  curr_lr = [9.218729096989966e-06]
curr_lr =   [9.218729096989966e-06]
[9.218729096989966e-06]
curr_lr =  [9.218729096989966e-06]
curr_lr =  [9.218729096989966e-06]
curr_lr =  curr_lr = curr_lr = [9.217391304347826e-06]  
[9.217391304347826e-06][9.217391304347826e-06]

curr_lr =  [9.217391304347826e-06]
curr_lr =  curr_lr = [9.217391304347826e-06]
 [9.217391304347826e-06]
curr_lr =  [9.217391304347826e-06]
curr_lr =  [9.217391304347826e-06]
curr_lr =  [9.216053511705686e-06]
curr_lr = curr_lr = curr_lr =    [9.216053511705686e-06]
[9.216053511705686e-06][9.216053511705686e-06]

curr_lr = curr_lr =   [9.216053511705686e-06][9.216053511705686e-06]

curr_lr =  [9.216053511705686e-06]
curr_lr =  [9.216053511705686e-06]
curr_lr = curr_lr =   curr_lr = [9.214715719063546e-06] 
[9.214715719063546e-06]
curr_lr = [9.214715719063546e-06] 
[9.214715719063546e-06]
curr_lr =  curr_lr = [9.214715719063546e-06]
 [9.214715719063546e-06]
curr_lr =  curr_lr =  [9.214715719063546e-06]
[9.214715719063546e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =      [9.213377926421404e-06]curr_lr = Epoch: [1][126/500]  Time 148.655 (148.312)  Loss 5.4581 (7.7810)    VQALoss 0.0000 (0.0000) VGLoss 5.4581 (7.7810)[9.213377926421404e-06]
[9.213377926421404e-06]
[9.213377926421404e-06] 
[9.213377926421404e-06][9.213377926421404e-06]



[9.213377926421404e-06]
curr_lr =  [9.213377926421404e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     [9.212040133779266e-06][9.212040133779266e-06]

[9.212040133779266e-06][9.212040133779266e-06]

curr_lr = curr_lr =   [9.212040133779266e-06][9.212040133779266e-06]

curr_lr =  [9.212040133779266e-06]
curr_lr =  [9.212040133779266e-06]
curr_lr = curr_lr =   [9.210702341137124e-06][9.210702341137124e-06]curr_lr = 

curr_lr =   [9.210702341137124e-06]
[9.210702341137124e-06]
curr_lr =  [9.210702341137124e-06]
curr_lr =  [9.210702341137124e-06]
curr_lr =  curr_lr = [9.210702341137124e-06] 
[9.210702341137124e-06]
curr_lr =  curr_lr = curr_lr = [9.209364548494984e-06]
  curr_lr =  [9.209364548494984e-06][9.209364548494984e-06]

[9.209364548494984e-06]
curr_lr =  curr_lr = [9.209364548494984e-06]
 [9.209364548494984e-06]
curr_lr =  [9.209364548494984e-06]
curr_lr =  [9.209364548494984e-06]
[2024-02-20 04:42:53,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=23, lr=[9.208695652173914e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =   [9.208026755852844e-06][9.208026755852844e-06]

curr_lr = curr_lr = curr_lr =    [9.208026755852844e-06][9.208026755852844e-06]

[9.208026755852844e-06]
curr_lr =  [9.208026755852844e-06]
curr_lr =  [9.208026755852844e-06]
[2024-02-20 04:42:54,138] [INFO] [timer.py:260:stop] epoch=0/micro_step=12600/global_step=630, RunningAvgSamplesPerSec=17.097171692422844, CurrSamplesPerSec=17.39372528687173, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.208026755852844e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    curr_lr = curr_lr =    [9.206688963210702e-06] 
[9.206688963210702e-06]
[9.206688963210702e-06]
[9.206688963210702e-06]Epoch: [1][131/500]      Time 148.343 (148.040)  Loss 12.6505 (7.3291)   VQALoss 0.0000 (0.0000) VGLoss 12.6505 (7.3291)[9.206688963210702e-06][9.206688963210702e-06][9.206688963210702e-06]




curr_lr =  [9.206688963210702e-06]
curr_lr = curr_lr =   curr_lr = [9.205351170568562e-06]curr_lr = [9.205351170568562e-06] 

 [9.205351170568562e-06]
[9.205351170568562e-06]
curr_lr =  curr_lr = [9.205351170568562e-06]
 [9.205351170568562e-06]
curr_lr =  [9.205351170568562e-06]
curr_lr =  [9.205351170568562e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =    [9.204013377926422e-06]
[9.204013377926422e-06][9.204013377926422e-06][9.204013377926422e-06]


curr_lr =  [9.204013377926422e-06]
curr_lr =  [9.204013377926422e-06]
curr_lr =  [9.204013377926422e-06]
curr_lr =  [9.204013377926422e-06]
curr_lr = curr_lr =   curr_lr = curr_lr =  [9.202675585284282e-06]
 [9.202675585284282e-06]
[9.202675585284282e-06]
[9.202675585284282e-06]
curr_lr =  [9.202675585284282e-06]curr_lr = 
 [9.202675585284282e-06]
curr_lr =  [9.202675585284282e-06]
curr_lr =  [9.202675585284282e-06]
curr_lr = curr_lr =   curr_lr = [9.201337792642142e-06][9.201337792642142e-06]

 [9.201337792642142e-06]
curr_lr =  [9.201337792642142e-06]curr_lr = 
curr_lr =   [9.201337792642142e-06][9.201337792642142e-06]

curr_lr =  [9.201337792642142e-06]
curr_lr =  [9.201337792642142e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =  curr_lr =   curr_lr = curr_lr = [9.200000000000002e-06] 
[9.200000000000002e-06][9.200000000000002e-06]

[9.200000000000002e-06]  
[9.200000000000002e-06]
Epoch: [1][136/500]     Time 148.814 (148.521)  Loss 4.7496 (7.5055)    VQALoss 0.0000 (0.0000) VGLoss 4.7496 (7.5055)[9.200000000000002e-06][9.200000000000002e-06]


curr_lr =  [9.200000000000002e-06]
curr_lr = curr_lr =   [9.19866220735786e-06]
[9.19866220735786e-06]
curr_lr =  [9.19866220735786e-06]
curr_lr =  [9.19866220735786e-06]
curr_lr = curr_lr =   [9.19866220735786e-06]
[9.19866220735786e-06]
curr_lr =  [9.19866220735786e-06]
curr_lr =  [9.19866220735786e-06]
curr_lr =  [9.19732441471572e-06]
curr_lr = curr_lr =   [9.19732441471572e-06]
[9.19732441471572e-06]
curr_lr =  curr_lr = curr_lr =   [9.19732441471572e-06]
[9.19732441471572e-06]
[9.19732441471572e-06]
curr_lr =  [9.19732441471572e-06]
curr_lr =  [9.19732441471572e-06]
curr_lr = curr_lr =   curr_lr =  [9.19598662207358e-06]
[9.19598662207358e-06]
[9.19598662207358e-06]
curr_lr =  curr_lr =  [9.19598662207358e-06]
[9.19598662207358e-06]
curr_lr = curr_lr =   [9.19598662207358e-06]
[9.19598662207358e-06]
curr_lr =  [9.19598662207358e-06]
[2024-02-20 05:07:37,761] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=23, lr=[9.19531772575251e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr = curr_lr =    [9.194648829431438e-06]curr_lr = 
[9.194648829431438e-06][9.194648829431438e-06]

 [9.194648829431438e-06]
curr_lr =  curr_lr = [9.194648829431438e-06]
 [9.194648829431438e-06]
curr_lr =  [9.194648829431438e-06]
[2024-02-20 05:07:37,960] [INFO] [timer.py:260:stop] epoch=0/micro_step=12800/global_step=640, RunningAvgSamplesPerSec=17.099760908216275, CurrSamplesPerSec=17.408111473565388, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.194648829431438e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =      [9.193311036789298e-06]curr_lr = 
[9.193311036789298e-06][9.193311036789298e-06][9.193311036789298e-06]Epoch: [1][141/500]        Time 148.695 (148.314)  Loss 16.7715 (8.0147)   VQALoss 0.0000 (0.0000) VGLoss 16.7715 (8.0147)

 
[9.193311036789298e-06]
[9.193311036789298e-06]

[9.193311036789298e-06]
curr_lr =  [9.193311036789298e-06]
curr_lr =  [9.191973244147158e-06]
curr_lr = curr_lr =   curr_lr = curr_lr =  [9.191973244147158e-06] 
[9.191973244147158e-06]
[9.191973244147158e-06][9.191973244147158e-06]
curr_lr = 
 [9.191973244147158e-06]
curr_lr =  curr_lr = [9.191973244147158e-06]
 [9.191973244147158e-06]
curr_lr =  curr_lr = [9.190635451505018e-06]curr_lr = 
  [9.190635451505018e-06]
[9.190635451505018e-06]
curr_lr =  curr_lr = [9.190635451505018e-06] 
curr_lr = [9.190635451505018e-06]
 [9.190635451505018e-06]
curr_lr =  [9.190635451505018e-06]
curr_lr =  [9.190635451505018e-06]
curr_lr =  curr_lr = curr_lr = [9.189297658862878e-06]
curr_lr =    [9.189297658862878e-06]
[9.189297658862878e-06]
[9.189297658862878e-06]
curr_lr =  [9.189297658862878e-06]
curr_lr =  [9.189297658862878e-06]
curr_lr =  [9.189297658862878e-06]
curr_lr =  [9.189297658862878e-06]
curr_lr = curr_lr = curr_lr =  curr_lr =    [9.187959866220737e-06][9.187959866220737e-06]

[9.187959866220737e-06]
[9.187959866220737e-06]
curr_lr = curr_lr =   [9.187959866220737e-06]
[9.187959866220737e-06]
curr_lr =  [9.187959866220737e-06]
curr_lr =  [9.187959866220737e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =        [9.186622073578596e-06][9.186622073578596e-06][9.186622073578596e-06][9.186622073578596e-06]Epoch: [1][146/500]    Time 148.519 (148.276)  Loss 4.2361 (7.3269)    VQALoss 0.0000 (0.0000) VGLoss 4.2361 (7.3269)[9.186622073578596e-06][9.186622073578596e-06]


[9.186622073578596e-06]




curr_lr =  [9.186622073578596e-06]
curr_lr = curr_lr =   curr_lr = [9.185284280936456e-06]
curr_lr =  [9.185284280936456e-06]
 [9.185284280936456e-06]
[9.185284280936456e-06]
curr_lr =  [9.185284280936456e-06]
curr_lr =  [9.185284280936456e-06]
curr_lr =  curr_lr = [9.185284280936456e-06]
 [9.185284280936456e-06]
curr_lr =  [9.183946488294316e-06]curr_lr = 
curr_lr =   curr_lr = [9.183946488294316e-06]
[9.183946488294316e-06]
 [9.183946488294316e-06]
curr_lr =  [9.183946488294316e-06]
curr_lr =  [9.183946488294316e-06]
curr_lr =  [9.183946488294316e-06]
curr_lr =  [9.183946488294316e-06]
curr_lr =  curr_lr = curr_lr = [9.182608695652174e-06]  
[9.182608695652174e-06][9.182608695652174e-06]

curr_lr =  [9.182608695652174e-06]
curr_lr =  [9.182608695652174e-06]
curr_lr =  [9.182608695652174e-06]
curr_lr =  [9.182608695652174e-06]
curr_lr =  [9.182608695652174e-06]
[2024-02-20 05:32:20,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=23, lr=[9.181939799331104e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.181270903010034e-06]
[9.181270903010034e-06]
  [9.181270903010034e-06]
curr_lr = [9.181270903010034e-06]
curr_lr =   [9.181270903010034e-06]
[9.181270903010034e-06]
curr_lr =  [9.181270903010034e-06]
[2024-02-20 05:32:20,877] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=650, RunningAvgSamplesPerSec=17.10242684516767, CurrSamplesPerSec=17.386849107053127, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.181270903010034e-06]
curr_lr = curr_lr = curr_lr =    curr_lr = curr_lr = [9.179933110367894e-06][9.179933110367894e-06][9.179933110367894e-06]curr_lr =  


 curr_lr = Epoch: [1][151/500]  Time 148.129 (148.194)  Loss 4.4711 (7.2191)    VQALoss 0.0000 (0.0000) VGLoss 4.4711 (7.2191) [9.179933110367894e-06]

 [9.179933110367894e-06]
[9.179933110367894e-06]
[9.179933110367894e-06]
curr_lr =  [9.179933110367894e-06]
[2024-02-20 05:34:53,106] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step651 is about to be saved!
[2024-02-20 05:35:05,248] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_1/global_step651/mp_rank_00_model_states.pt
[2024-02-20 05:35:05,248] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step651/mp_rank_00_model_states.pt...
[2024-02-20 05:37:23,430] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step651/mp_rank_00_model_states.pt.
[2024-02-20 05:37:24,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step651/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-20 05:37:24,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step651/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-20 05:37:24,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step651/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-20 05:37:24,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step651/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-20 05:37:24,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step651/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-20 05:37:24,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step651/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-20 05:37:24,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step651/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-20 05:37:24,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step651/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-20 05:37:28,938] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step651/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-20 05:37:28,938] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step651/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-20 05:37:28,938] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step651 is ready now!
[2024-02-20 05:37:28,939] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step651/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-20 05:37:28,939] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step651/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-20 05:37:28,939] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step651 is ready now!
[2024-02-20 05:37:28,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step651/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-20 05:37:28,943] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step651/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-20 05:37:28,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step651 is ready now!
[2024-02-20 05:37:28,967] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step651/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-20 05:37:28,967] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step651/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-20 05:37:28,968] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step651 is ready now!
[2024-02-20 05:37:29,126] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step651/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-20 05:37:29,126] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step651/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-20 05:37:29,126] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step651 is ready now!
[2024-02-20 05:37:29,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step651/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-20 05:37:29,187] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step651/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-20 05:37:29,187] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step651 is ready now!
[2024-02-20 05:37:29,190] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step651/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-20 05:37:29,200] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step651/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-20 05:37:29,200] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step651 is ready now!
[2024-02-20 05:37:29,217] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step651/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-20 05:37:29,217] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step651/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-20 05:37:29,217] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step651 is ready now!
curr_lr =  [9.178595317725753e-06]
curr_lr = curr_lr =  curr_lr =  [9.178595317725753e-06]
 [9.178595317725753e-06]
[9.178595317725753e-06]
curr_lr = curr_lr =   [9.178595317725753e-06]
[9.178595317725753e-06]
curr_lr =  [9.178595317725753e-06]
curr_lr =  [9.178595317725753e-06]
curr_lr =  curr_lr =  [9.177257525083613e-06]
curr_lr = [9.177257525083613e-06]curr_lr = 
  [9.177257525083613e-06]
[9.177257525083613e-06]
curr_lr =  curr_lr = [9.177257525083613e-06] 
[9.177257525083613e-06]
curr_lr =  [9.177257525083613e-06]
curr_lr =  [9.177257525083613e-06]
curr_lr = curr_lr = curr_lr =    [9.175919732441473e-06]
[9.175919732441473e-06]
[9.175919732441473e-06]
curr_lr = curr_lr =   [9.175919732441473e-06][9.175919732441473e-06]

curr_lr =  [9.175919732441473e-06]
curr_lr =  [9.175919732441473e-06]
curr_lr =  [9.175919732441473e-06]
curr_lr =  curr_lr = [9.174581939799331e-06]curr_lr = 
  curr_lr = [9.174581939799331e-06]
 [9.174581939799331e-06]
curr_lr = curr_lr = [9.174581939799331e-06]
  [9.174581939799331e-06][9.174581939799331e-06]

curr_lr =  [9.174581939799331e-06]
curr_lr =  [9.174581939799331e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    Epoch: [1][156/500]      Time 148.866 (180.301)  Loss 3.6181 (7.6878)    VQALoss 0.0000 (0.0000) VGLoss 3.6181 (7.6878)   
[9.173244147157191e-06][9.173244147157191e-06]curr_lr = 

[9.173244147157191e-06][9.173244147157191e-06]
[9.173244147157191e-06][9.173244147157191e-06] 


[9.173244147157191e-06]
curr_lr =  [9.173244147157191e-06]
curr_lr = curr_lr =   curr_lr = curr_lr = [9.171906354515051e-06][9.171906354515051e-06] 

 [9.171906354515051e-06]
[9.171906354515051e-06]
curr_lr =  [9.171906354515051e-06]
curr_lr =  [9.171906354515051e-06]
curr_lr =  [9.171906354515051e-06]
curr_lr =  [9.171906354515051e-06]
curr_lr = curr_lr =  curr_lr = [9.17056856187291e-06] 
 curr_lr =  [9.17056856187291e-06][9.17056856187291e-06]

[9.17056856187291e-06]curr_lr = 
 curr_lr =  [9.17056856187291e-06]
[9.17056856187291e-06]
curr_lr =  [9.17056856187291e-06]
curr_lr =  [9.17056856187291e-06]
curr_lr = curr_lr =   curr_lr = [9.169230769230771e-06] curr_lr = 
[9.169230769230771e-06]
 [9.169230769230771e-06]
[9.169230769230771e-06]
curr_lr =  [9.169230769230771e-06]
curr_lr = curr_lr =   [9.169230769230771e-06]
[9.169230769230771e-06]
curr_lr =  [9.169230769230771e-06]
[2024-02-20 05:59:42,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=23, lr=[9.1685618729097e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =  curr_lr =   [9.16789297658863e-06]
curr_lr = [9.16789297658863e-06][9.16789297658863e-06]

 [9.16789297658863e-06]
curr_lr =  [9.16789297658863e-06]
curr_lr =  [9.16789297658863e-06]
curr_lr =  [9.16789297658863e-06]
[2024-02-20 05:59:42,506] [INFO] [timer.py:260:stop] epoch=0/micro_step=13200/global_step=660, RunningAvgSamplesPerSec=17.105274708503316, CurrSamplesPerSec=17.38210547576883, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.16789297658863e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =      Epoch: [1][161/500]      Time 148.289 (148.056)  Loss 14.1467 (7.5282)   VQALoss 0.0000 (0.0000) VGLoss 14.1467 (7.5282)[9.16655518394649e-06]curr_lr = [9.16655518394649e-06][9.16655518394649e-06][9.16655518394649e-06]


[9.16655518394649e-06]

curr_lr = 
  [9.16655518394649e-06][9.16655518394649e-06]

curr_lr =  [9.16655518394649e-06]
curr_lr = curr_lr =  curr_lr =  curr_lr =  [9.165217391304349e-06] [9.165217391304349e-06]

[9.165217391304349e-06][9.165217391304349e-06]

curr_lr =  [9.165217391304349e-06]
curr_lr =  curr_lr = [9.165217391304349e-06]
 [9.165217391304349e-06]
curr_lr =  [9.165217391304349e-06]
curr_lr =  curr_lr =  [9.163879598662207e-06]
curr_lr = [9.163879598662207e-06]
 curr_lr = [9.163879598662207e-06]
 [9.163879598662207e-06]
curr_lr = curr_lr =   [9.163879598662207e-06]
[9.163879598662207e-06]
curr_lr =  [9.163879598662207e-06]
curr_lr =  [9.163879598662207e-06]
curr_lr =  curr_lr = [9.162541806020067e-06]
 curr_lr =  [9.162541806020067e-06]
curr_lr = [9.162541806020067e-06]
 [9.162541806020067e-06]
curr_lr =  [9.162541806020067e-06]
curr_lr =  [9.162541806020067e-06]
curr_lr =  [9.162541806020067e-06]
curr_lr =  [9.162541806020067e-06]
curr_lr =  curr_lr =  curr_lr = [9.161204013377927e-06]
curr_lr =   [9.161204013377927e-06]
[9.161204013377927e-06]
[9.161204013377927e-06]
curr_lr =  [9.161204013377927e-06]
curr_lr =  [9.161204013377927e-06]
curr_lr =  [9.161204013377927e-06]
curr_lr =  [9.161204013377927e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr =    Epoch: [1][166/500]     Time 149.053 (148.317)  Loss 3.7022 (7.5396)    VQALoss 0.0000 (0.0000) VGLoss 3.7022 (7.5396)  
[9.159866220735787e-06][9.159866220735787e-06]curr_lr = [9.159866220735787e-06]

[9.159866220735787e-06]

[9.159866220735787e-06][9.159866220735787e-06]

 [9.159866220735787e-06]
curr_lr =  [9.159866220735787e-06]
curr_lr = curr_lr =   curr_lr = [9.158528428093645e-06]
 [9.158528428093645e-06]
[9.158528428093645e-06]
curr_lr = curr_lr =   [9.158528428093645e-06][9.158528428093645e-06]

curr_lr =  [9.158528428093645e-06]
curr_lr =  [9.158528428093645e-06]
curr_lr =  [9.158528428093645e-06]
curr_lr = curr_lr =   curr_lr = [9.157190635451507e-06]
[9.157190635451507e-06] 
[9.157190635451507e-06]
curr_lr = curr_lr =  curr_lr =   [9.157190635451507e-06][9.157190635451507e-06][9.157190635451507e-06]


curr_lr =  [9.157190635451507e-06]
curr_lr =  [9.157190635451507e-06]
curr_lr =  curr_lr = curr_lr =   [9.155852842809365e-06]
[9.155852842809365e-06][9.155852842809365e-06]

curr_lr =  curr_lr = [9.155852842809365e-06] 
curr_lr = [9.155852842809365e-06]
 [9.155852842809365e-06]
curr_lr =  curr_lr = [9.155852842809365e-06]
 [9.155852842809365e-06]
[2024-02-20 06:24:22,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=23, lr=[9.155183946488295e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr = curr_lr = [9.154515050167225e-06]
  [9.154515050167225e-06]
[9.154515050167225e-06]
curr_lr =  [9.154515050167225e-06]curr_lr = 
 [9.154515050167225e-06]
curr_lr =  [9.154515050167225e-06]
curr_lr =  [9.154515050167225e-06]
[2024-02-20 06:24:22,932] [INFO] [timer.py:260:stop] epoch=0/micro_step=13400/global_step=670, RunningAvgSamplesPerSec=17.108152558773675, CurrSamplesPerSec=17.57023416066593, MemAllocated=27.33GB, MaxMemAllocated=40.63GB
curr_lr =  [9.154515050167225e-06]
curr_lr = curr_lr =  curr_lr = curr_lr = curr_lr =  curr_lr = Epoch: [1][171/500]       Time 150.317 (148.174)  Loss 14.8241 (7.2280)   VQALoss 0.0000 (0.0000) VGLoss 14.8241 (7.2280)  [9.153177257525085e-06] 
 [9.153177257525085e-06]
curr_lr = 
[9.153177257525085e-06][9.153177257525085e-06]

[9.153177257525085e-06] [9.153177257525085e-06]

[9.153177257525085e-06]
curr_lr =  [9.153177257525085e-06]
curr_lr = curr_lr =   curr_lr =  [9.151839464882943e-06]
[9.151839464882943e-06][9.151839464882943e-06]

curr_lr =  [9.151839464882943e-06]
curr_lr =  [9.151839464882943e-06]curr_lr = 
 [9.151839464882943e-06]
curr_lr = curr_lr =   [9.151839464882943e-06]
[9.151839464882943e-06]
curr_lr =  curr_lr = [9.150501672240803e-06] 
curr_lr = [9.150501672240803e-06]
 [9.150501672240803e-06]
curr_lr =  [9.150501672240803e-06]
curr_lr =  curr_lr =  [9.150501672240803e-06]
[9.150501672240803e-06]
curr_lr =  [9.150501672240803e-06]
curr_lr =  [9.150501672240803e-06]
curr_lr = curr_lr =   [9.149163879598663e-06][9.149163879598663e-06]

curr_lr =  curr_lr =  [9.149163879598663e-06]
[9.149163879598663e-06]
curr_lr =  [9.149163879598663e-06]
curr_lr =  [9.149163879598663e-06]
curr_lr =  [9.149163879598663e-06]
curr_lr =  [9.149163879598663e-06]
curr_lr =  [9.147826086956523e-06]
curr_lr =  curr_lr =  curr_lr = [9.147826086956523e-06][9.147826086956523e-06] 

[9.147826086956523e-06]
curr_lr =  [9.147826086956523e-06]
curr_lr =  [9.147826086956523e-06]
curr_lr =  [9.147826086956523e-06]
curr_lr =  [9.147826086956523e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = Epoch: [1][176/500]   Time 148.666 (148.567)  Loss 4.5191 (7.3521)    VQALoss 0.0000 (0.0000) VGLoss 4.5191 (7.3521) curr_lr = curr_lr =     
  [9.146488294314381e-06]
[9.146488294314381e-06][9.146488294314381e-06][9.146488294314381e-06]
[9.146488294314381e-06]
[9.146488294314381e-06]
[9.146488294314381e-06]


curr_lr =  [9.146488294314381e-06]
curr_lr = curr_lr =   curr_lr =  [9.145150501672243e-06]
[9.145150501672243e-06]
curr_lr = [9.145150501672243e-06]
 [9.145150501672243e-06]
curr_lr =  curr_lr = [9.145150501672243e-06]
 [9.145150501672243e-06]
curr_lr =  [9.145150501672243e-06]
curr_lr =  [9.145150501672243e-06]
curr_lr =  [9.143812709030101e-06]
curr_lr = curr_lr =   curr_lr =  [9.143812709030101e-06]
[9.143812709030101e-06]
[9.143812709030101e-06]
curr_lr =  [9.143812709030101e-06]
curr_lr =  [9.143812709030101e-06]curr_lr = 
 [9.143812709030101e-06]
curr_lr =  [9.143812709030101e-06]
curr_lr = curr_lr =   curr_lr = [9.14247491638796e-06] 
[9.14247491638796e-06]
[9.14247491638796e-06]
curr_lr =  [9.14247491638796e-06]
curr_lr =  curr_lr = [9.14247491638796e-06]
 [9.14247491638796e-06]
curr_lr =  [9.14247491638796e-06]
curr_lr =  [9.14247491638796e-06]
[2024-02-20 06:49:09,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=23, lr=[9.14180602006689e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =  curr_lr =  curr_lr =  [9.14113712374582e-06] 
[9.14113712374582e-06]
[9.14113712374582e-06][9.14113712374582e-06]

curr_lr =  [9.14113712374582e-06]
curr_lr =  [9.14113712374582e-06]
curr_lr =  [9.14113712374582e-06]
[2024-02-20 06:49:09,477] [INFO] [timer.py:260:stop] epoch=0/micro_step=13600/global_step=680, RunningAvgSamplesPerSec=17.109967089898536, CurrSamplesPerSec=17.21915179978904, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.14113712374582e-06]
curr_lr = curr_lr = curr_lr = curr_lr =  curr_lr = curr_lr =   curr_lr =  Epoch: [1][181/500]   Time 147.595 (148.198)  Loss 2.7093 (7.4269)    VQALoss 0.0000 (0.0000) VGLoss 2.7093 (7.4269)[9.139799331103679e-06]  [9.139799331103679e-06]

[9.139799331103679e-06]
 [9.139799331103679e-06]

[9.139799331103679e-06][9.139799331103679e-06]

[9.139799331103679e-06]
curr_lr =  [9.139799331103679e-06]
curr_lr =  curr_lr =  [9.138461538461539e-06]
[9.138461538461539e-06]
curr_lr =  curr_lr = [9.138461538461539e-06]
curr_lr =   [9.138461538461539e-06][9.138461538461539e-06]

curr_lr =  [9.138461538461539e-06]curr_lr = 
 [9.138461538461539e-06]
curr_lr =  [9.138461538461539e-06]
curr_lr = curr_lr =   [9.137123745819399e-06]curr_lr = 
 [9.137123745819399e-06]curr_lr = 
 [9.137123745819399e-06]
[9.137123745819399e-06]
curr_lr =  [9.137123745819399e-06]
curr_lr =  [9.137123745819399e-06]
curr_lr =  [9.137123745819399e-06]
curr_lr =  [9.137123745819399e-06]
curr_lr =  curr_lr = [9.135785953177259e-06] curr_lr = 
 [9.135785953177259e-06]
[9.135785953177259e-06]
curr_lr =  [9.135785953177259e-06]
curr_lr =  curr_lr =  [9.135785953177259e-06]
[9.135785953177259e-06]curr_lr = 
 [9.135785953177259e-06]
curr_lr =  [9.135785953177259e-06]
curr_lr =  curr_lr =  [9.134448160535117e-06]
[9.134448160535117e-06]
curr_lr =  curr_lr =  [9.134448160535117e-06]
[9.134448160535117e-06]
curr_lr = curr_lr =   [9.134448160535117e-06]
[9.134448160535117e-06]
curr_lr =  [9.134448160535117e-06]
curr_lr =  [9.134448160535117e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =        [9.133110367892978e-06][9.133110367892978e-06][9.133110367892978e-06][9.133110367892978e-06]Epoch: [1][186/500]    Time 148.659 (148.484)  Loss 2.8311 (7.3373)    VQALoss 0.0000 (0.0000) VGLoss 2.8311 (7.3373)[9.133110367892978e-06][9.133110367892978e-06]


[9.133110367892978e-06]




curr_lr =  [9.133110367892978e-06]
curr_lr = curr_lr =  curr_lr =  curr_lr =  [9.131772575250837e-06]
 [9.131772575250837e-06]
[9.131772575250837e-06]
[9.131772575250837e-06]
curr_lr =  [9.131772575250837e-06]
curr_lr =  [9.131772575250837e-06]
curr_lr =  [9.131772575250837e-06]
curr_lr =  [9.131772575250837e-06]
curr_lr = curr_lr =  curr_lr =  curr_lr =  [9.130434782608697e-06][9.130434782608697e-06]

 [9.130434782608697e-06]
curr_lr = [9.130434782608697e-06]
 curr_lr = [9.130434782608697e-06]
 [9.130434782608697e-06]
curr_lr =  curr_lr =  [9.130434782608697e-06]
[9.130434782608697e-06]
curr_lr =  [9.129096989966556e-06]
curr_lr =  [9.129096989966556e-06]
curr_lr =  [9.129096989966556e-06]
curr_lr = curr_lr =   [9.129096989966556e-06][9.129096989966556e-06]

curr_lr =  [9.129096989966556e-06]
curr_lr =  [9.129096989966556e-06]
curr_lr =  [9.129096989966556e-06]
[2024-02-20 07:13:52,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=23, lr=[9.128428093645486e-06], mom=[(0.9, 0.95)]
curr_lr =  curr_lr =  [9.127759197324415e-06]
[9.127759197324415e-06]curr_lr = 
 [9.127759197324415e-06]
curr_lr =  [9.127759197324415e-06]
curr_lr =  [9.127759197324415e-06]curr_lr = 
 [9.127759197324415e-06]
curr_lr =  [9.127759197324415e-06]
[2024-02-20 07:13:52,527] [INFO] [timer.py:260:stop] epoch=0/micro_step=13800/global_step=690, RunningAvgSamplesPerSec=17.112312556528643, CurrSamplesPerSec=17.253424646484397, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.127759197324415e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    Epoch: [1][191/500]      Time 147.585 (148.124)  Loss 4.2239 (7.9443)    VQALoss 0.0000 (0.0000) VGLoss 4.2239 (7.9443)curr_lr = 
   [9.126421404682275e-06][9.126421404682275e-06]

[9.126421404682275e-06] 
[9.126421404682275e-06][9.126421404682275e-06]

[9.126421404682275e-06]
[9.126421404682275e-06]
curr_lr =  [9.126421404682275e-06]
curr_lr = curr_lr =   curr_lr = curr_lr =  [9.125083612040134e-06][9.125083612040134e-06]
 
[9.125083612040134e-06]
[9.125083612040134e-06]
curr_lr =  curr_lr = [9.125083612040134e-06]
 [9.125083612040134e-06]
curr_lr =  [9.125083612040134e-06]
curr_lr =  [9.125083612040134e-06]
curr_lr =  [9.123745819397994e-06]curr_lr = curr_lr = 
curr_lr =    [9.123745819397994e-06]
[9.123745819397994e-06][9.123745819397994e-06]

curr_lr =  curr_lr = [9.123745819397994e-06] 
[9.123745819397994e-06]
curr_lr =  [9.123745819397994e-06]
curr_lr =  [9.123745819397994e-06]
curr_lr = curr_lr =   [9.122408026755854e-06]curr_lr = 
[9.122408026755854e-06]
 curr_lr = [9.122408026755854e-06] 
curr_lr = curr_lr = [9.122408026755854e-06]  
[9.122408026755854e-06][9.122408026755854e-06]

curr_lr =  [9.122408026755854e-06]
curr_lr =  [9.122408026755854e-06]
curr_lr = curr_lr =   curr_lr = [9.121070234113714e-06][9.121070234113714e-06]

 [9.121070234113714e-06]
curr_lr =  curr_lr = [9.121070234113714e-06] 
curr_lr = [9.121070234113714e-06]
 [9.121070234113714e-06]
curr_lr =  [9.121070234113714e-06]
curr_lr =  [9.121070234113714e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =  Epoch: [1][196/500]  Time 148.952 (148.347)  Loss 4.7489 (7.9228)    VQALoss 0.0000 (0.0000) VGLoss 4.7489 (7.9228) curr_lr =    
 [9.119732441471572e-06][9.119732441471572e-06][9.119732441471572e-06]curr_lr = [9.119732441471572e-06]

[9.119732441471572e-06]

[9.119732441471572e-06]
 
[9.119732441471572e-06]
curr_lr =  [9.119732441471572e-06]
curr_lr = curr_lr = curr_lr =   curr_lr =   [9.118394648829432e-06][9.118394648829432e-06][9.118394648829432e-06]


[9.118394648829432e-06]
curr_lr = curr_lr =   [9.118394648829432e-06]
[9.118394648829432e-06]
curr_lr =  [9.118394648829432e-06]
curr_lr =  [9.118394648829432e-06]
curr_lr =  curr_lr = [9.117056856187292e-06] 
curr_lr =  [9.117056856187292e-06]
curr_lr = [9.117056856187292e-06]
 [9.117056856187292e-06]
curr_lr =  [9.117056856187292e-06]
curr_lr =  curr_lr = [9.117056856187292e-06] 
[9.117056856187292e-06]
curr_lr =  [9.117056856187292e-06]
curr_lr = curr_lr = curr_lr = curr_lr =     [9.11571906354515e-06][9.11571906354515e-06][9.11571906354515e-06][9.11571906354515e-06]



curr_lr =  [9.11571906354515e-06]curr_lr = 
 [9.11571906354515e-06]
curr_lr =  [9.11571906354515e-06]
curr_lr =  [9.11571906354515e-06]
[2024-02-20 07:38:34,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=23, lr=[9.11505016722408e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.11438127090301e-06]
curr_lr =  [9.11438127090301e-06]
curr_lr =  curr_lr = [9.11438127090301e-06]
 curr_lr =  [9.11438127090301e-06]
[9.11438127090301e-06]
curr_lr =  [9.11438127090301e-06]
curr_lr =  [9.11438127090301e-06]
[2024-02-20 07:38:34,939] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=700, RunningAvgSamplesPerSec=17.11469160166114, CurrSamplesPerSec=17.215166665935033, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.11438127090301e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =       [9.11304347826087e-06][9.11304347826087e-06][9.11304347826087e-06][9.11304347826087e-06]

curr_lr = [9.11304347826087e-06]

[9.11304347826087e-06]

 [9.11304347826087e-06]
Epoch: [1][201/500]     Time 147.577 (148.134)  Loss 5.6667 (6.7145)    VQALoss 0.0000 (0.0000) VGLoss 5.6667 (6.7145)
curr_lr =  [9.11304347826087e-06]
[2024-02-20 07:41:07,026] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step701 is about to be saved!
[2024-02-20 07:41:19,281] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_1/global_step701/mp_rank_00_model_states.pt
[2024-02-20 07:41:19,281] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step701/mp_rank_00_model_states.pt...
[2024-02-20 07:43:35,530] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step701/mp_rank_00_model_states.pt.
[2024-02-20 07:43:36,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step701/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-20 07:43:36,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step701/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-20 07:43:36,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step701/zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-02-20 07:43:36,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step701/zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-02-20 07:43:36,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step701/zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-02-20 07:43:36,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step701/zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-02-20 07:43:36,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step701/zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-02-20 07:43:36,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step701/zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-02-20 07:43:41,096] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step701/zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-02-20 07:43:41,096] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step701/zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-02-20 07:43:41,096] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step701 is ready now!
[2024-02-20 07:43:41,106] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step701/zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-02-20 07:43:41,106] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step701/zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-02-20 07:43:41,106] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step701 is ready now!
[2024-02-20 07:43:41,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step701/zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-02-20 07:43:41,107] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step701/zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-02-20 07:43:41,107] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step701 is ready now!
[2024-02-20 07:43:41,123] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step701/zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-02-20 07:43:41,123] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step701/zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-02-20 07:43:41,124] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step701 is ready now!
[2024-02-20 07:43:41,203] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step701/zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-02-20 07:43:41,203] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step701/zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-02-20 07:43:41,203] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step701 is ready now!
[2024-02-20 07:43:41,208] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step701/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-20 07:43:41,219] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step701/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-20 07:43:41,219] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step701 is ready now!
[2024-02-20 07:43:41,235] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step701/zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-02-20 07:43:41,235] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step701/zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-02-20 07:43:41,235] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step701 is ready now!
[2024-02-20 07:43:41,282] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step701/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-20 07:43:41,282] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step701/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-20 07:43:41,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step701 is ready now!
curr_lr =  curr_lr =  [9.11170568561873e-06]
[9.11170568561873e-06]curr_lr = 
curr_lr =   curr_lr = [9.11170568561873e-06][9.11170568561873e-06] 

curr_lr = [9.11170568561873e-06] 
[9.11170568561873e-06]
curr_lr =  [9.11170568561873e-06]
curr_lr =  [9.11170568561873e-06]
curr_lr = curr_lr =   curr_lr = [9.11036789297659e-06]
[9.11036789297659e-06] 
[9.11036789297659e-06]
curr_lr = curr_lr =   [9.11036789297659e-06][9.11036789297659e-06]

curr_lr =  [9.11036789297659e-06]
curr_lr =  [9.11036789297659e-06]
curr_lr =  [9.11036789297659e-06]
curr_lr = curr_lr =   curr_lr = [9.10903010033445e-06]curr_lr = 
[9.10903010033445e-06]
  [9.10903010033445e-06]
[9.10903010033445e-06]
curr_lr =  [9.10903010033445e-06]
curr_lr =  [9.10903010033445e-06]
curr_lr =  [9.10903010033445e-06]
curr_lr =  [9.10903010033445e-06]
curr_lr =  curr_lr = [9.107692307692308e-06] 
[9.107692307692308e-06]
curr_lr =  curr_lr =  [9.107692307692308e-06]
[9.107692307692308e-06]
curr_lr =  [9.107692307692308e-06]
curr_lr =  [9.107692307692308e-06]
curr_lr =  [9.107692307692308e-06]
curr_lr =  [9.107692307692308e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =    curr_lr =  curr_lr =   [9.106354515050168e-06][9.106354515050168e-06][9.106354515050168e-06]


[9.106354515050168e-06]
 [9.106354515050168e-06][9.106354515050168e-06]

[9.106354515050168e-06]
Epoch: [1][206/500]     Time 148.382 (180.127)  Loss 4.6893 (7.3017)    VQALoss 0.0000 (0.0000) VGLoss 4.6893 (7.3017)
curr_lr =  [9.106354515050168e-06]
curr_lr = curr_lr =  curr_lr =  curr_lr =   [9.105016722408028e-06]
[9.105016722408028e-06]
[9.105016722408028e-06]
[9.105016722408028e-06]
curr_lr = curr_lr =   [9.105016722408028e-06]
[9.105016722408028e-06]
curr_lr =  [9.105016722408028e-06]
curr_lr =  [9.105016722408028e-06]
curr_lr =  curr_lr = [9.103678929765886e-06]curr_lr = 
curr_lr = curr_lr =    [9.103678929765886e-06][9.103678929765886e-06]
 
[9.103678929765886e-06]
[9.103678929765886e-06]
curr_lr =  [9.103678929765886e-06]
curr_lr =  [9.103678929765886e-06]
curr_lr =  [9.103678929765886e-06]
curr_lr =  [9.102341137123746e-06]
curr_lr = curr_lr =  curr_lr =   [9.102341137123746e-06][9.102341137123746e-06]

[9.102341137123746e-06]
curr_lr =  [9.102341137123746e-06]
curr_lr =  [9.102341137123746e-06]curr_lr = 
 [9.102341137123746e-06]
curr_lr =  [9.102341137123746e-06]
[2024-02-20 08:05:56,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=23, lr=[9.101672240802676e-06], mom=[(0.9, 0.95)]
curr_lr = curr_lr =  curr_lr =  curr_lr =  [9.101003344481606e-06][9.101003344481606e-06]

[9.101003344481606e-06] 
curr_lr =  [9.101003344481606e-06]
[9.101003344481606e-06]
curr_lr =  [9.101003344481606e-06]
curr_lr =  [9.101003344481606e-06]
[2024-02-20 08:05:56,578] [INFO] [timer.py:260:stop] epoch=0/micro_step=14200/global_step=710, RunningAvgSamplesPerSec=17.11694068665018, CurrSamplesPerSec=17.239752055813337, MemAllocated=27.34GB, MaxMemAllocated=40.63GB
curr_lr =  [9.101003344481606e-06]
curr_lr = curr_lr = curr_lr = curr_lr = curr_lr = curr_lr =   curr_lr =      [9.099665551839466e-06]
[9.099665551839466e-06][9.099665551839466e-06][9.099665551839466e-06]

[9.099665551839466e-06]Epoch: [1][211/500]      Time 147.548 (148.195)  Loss 5.2450 (7.2433)    VQALoss 0.0000 (0.0000) VGLoss 5.2450 (7.2433)
[9.099665551839466e-06][9.099665551839466e-06]



curr_lr =  [9.099665551839466e-06]
curr_lr =  [9.098327759197326e-06]
curr_lr =  [9.098327759197326e-06]
curr_lr =  curr_lr = curr_lr = curr_lr = [9.098327759197326e-06]  
 [9.098327759197326e-06][9.098327759197326e-06]

[9.098327759197326e-06]
curr_lr =  [9.098327759197326e-06]
curr_lr =  [9.098327759197326e-06]
Traceback (most recent call last):
  File "/workspace/ChatterBox-Finetuning/custom_train_stage1.py", line 446, in <module>
    main(sys.argv[1:])
  File "/workspace/ChatterBox-Finetuning/custom_train_stage1.py", line 304, in main
    train(
  File "/workspace/ChatterBox-Finetuning/custom_train_stage1.py", line 386, in train
    output_dict = model(**meta_input_dict)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 1807, in forward
    loss = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/ChatterBox-Finetuning/model/ChatterBox_Referrring_Grounding_grounding_dino.py", line 277, in forward
    .expand(end_i - start_i, -1, -1, -1)  # why here expand?
RuntimeError: numel: integer multiplication overflow

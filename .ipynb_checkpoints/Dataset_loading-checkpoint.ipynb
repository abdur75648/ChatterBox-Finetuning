{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e756d544-2bbb-4e65-b0db-4a2efb870f13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa946a9-83bd-4e9b-a191-01e57422f26f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 20:25:49.477188: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-18 20:25:49.482837: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-18 20:25:49.538934: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 20:25:50.493850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from utils.chatterbox_dataset_grounding import MyGroundingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061b1817-6b01-4f8f-a909-010f1f512773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"llava-llama-2-13b-chat-lightning-preview\",\n",
    "    cache_dir=None,\n",
    "    model_max_length=2048,#args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.add_tokens(\"[VG]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176905a6-2a82-4067-a393-46b959b3a7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='llava-llama-2-13b-chat-lightning-preview', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46cb480-9cf2-4248-a469-a9ef1210c90a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cdc7f10-66e1-411d-b725-1039a1f72d24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8516"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata = MyGroundingDataset(\n",
    "    base_root=\"Augmented-Apollo-MLLM-data/\", # path to images in grounding_gt.json\n",
    "    tokenizer=tokenizer,\n",
    "    vision_tower=\"openai/clip-vit-large-patch14\",\n",
    "    anno_path='Augmented-Apollo-MLLM-data/'\n",
    ")\n",
    "len(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18bde64-1a49-455e-9b4d-599669592759",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:01<00:07, 10.98it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "unique_values = {0: set(), 1: set(), 2: set(), 3: set()}\n",
    "for i in tqdm(range(100)):\n",
    "    sample = mydata[i]\n",
    "    unique_values[0].add(sample[0].shape)\n",
    "    unique_values[1].add(sample[1].shape)\n",
    "    unique_values[2].add(len(sample[2]))\n",
    "    unique_values[3].add(len(sample[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eb47fe9-6182-4c16-b3d1-c8e629294416",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {torch.Size([3, 512, 512])},\n",
       " 1: {torch.Size([3, 224, 224])},\n",
       " 2: {1},\n",
       " 3: {1}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b45a8d-751a-45c3-9197-f118e198c8fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e50c8afe-81b9-470c-8cf0-15e0993d7f7b",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mmydata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/ChatterBox-Finetuning/utils/chatterbox_dataset_grounding.py:539\u001b[0m, in \u001b[0;36mMyGroundingDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    536\u001b[0m ori_size \u001b[38;5;241m=\u001b[39m images_ori\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# preprocess images for clip\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m images_clip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_image_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_ori\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    541\u001b[0m ][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    542\u001b[0m image_token_len \u001b[38;5;241m=\u001b[39m (images_clip\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m14\u001b[39m) \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m    543\u001b[0m         images_clip\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m14\u001b[39m\n\u001b[1;32m    544\u001b[0m )  \u001b[38;5;66;03m# FIXME: 14 is hardcoded patch size\u001b[39;00m\n\u001b[1;32m    546\u001b[0m images, _, ratios \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(Image\u001b[38;5;241m.\u001b[39mfromarray(images_ori))  \u001b[38;5;66;03m# preprocess images for dino, check this\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/image_processing_clip.py:323\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m images \u001b[38;5;241m=\u001b[39m [to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 323\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize, resample\u001b[38;5;241m=\u001b[39mresample) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_center_crop:\n\u001b[1;32m    326\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39mcrop_size) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/image_processing_clip.py:323\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    320\u001b[0m images \u001b[38;5;241m=\u001b[39m [to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 323\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_center_crop:\n\u001b[1;32m    326\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39mcrop_size) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/image_processing_clip.py:150\u001b[0m, in \u001b[0;36mCLIPImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `size` parameter must contain the key `shortest_edge`. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m output_size \u001b[38;5;241m=\u001b[39m get_resize_output_image_size(image, size\u001b[38;5;241m=\u001b[39msize[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshortest_edge\u001b[39m\u001b[38;5;124m\"\u001b[39m], default_to_square\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py:306\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    305\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[0;32m--> 306\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py:192\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(image, do_rescale)\u001b[0m\n\u001b[1;32m    189\u001b[0m     image \u001b[38;5;241m=\u001b[39m rescale(image, \u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    191\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3013\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3011\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtostring()\n\u001b[0;32m-> 3013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py:2940\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2937\u001b[0m         im\u001b[38;5;241m.\u001b[39mreadonly \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m im\n\u001b[0;32m-> 2940\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py:2882\u001b[0m, in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2879\u001b[0m     args \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2881\u001b[0m im \u001b[38;5;241m=\u001b[39m new(mode, size)\n\u001b[0;32m-> 2882\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2883\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py:812\u001b[0m, in \u001b[0;36mImage.frombytes\u001b[0;34m(self, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m    810\u001b[0m d \u001b[38;5;241m=\u001b[39m _getdecoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode, decoder_name, args)\n\u001b[1;32m    811\u001b[0m d\u001b[38;5;241m.\u001b[39msetimage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim)\n\u001b[0;32m--> 812\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot enough image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "error_occurred = False\n",
    "i = 0\n",
    "\n",
    "while not error_occurred and i < len(mydata):\n",
    "    if i%100==0:\n",
    "        print(i+1)\n",
    "    try:\n",
    "        sample = mydata[i]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred at index {i}: {e}\")\n",
    "        error_occurred = True\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16925fcb-a5b1-4a0b-b518-d87aeb4f3442",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Give me the list of all people working in Intuit as sales associate with exp. of 3 years \n",
      " Previous Action: CLICK: Click min year button \n",
      "Give me the next action?\n",
      "TYPE: Type 5 in min year  tab. <tab: [188, 427, 268, 461]>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "indx_req = 2122\n",
    "def print_sample(index):\n",
    "    with open('Augmented-Apollo-MLLM-data/apollo_8k_GND.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        sample = data['data'][index]\n",
    "        return sample\n",
    "\n",
    "# Print the sample at index 1\n",
    "human_input = print_sample(indx_req)[\"conversation\"][0]['value']\n",
    "gpt_output = print_sample(indx_req)[\"conversation\"][1]['value']\n",
    "print(human_input)\n",
    "print(gpt_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f1ab51d-343f-4a59-a705-623a46caf013",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(r\"<(.+?)>\", human_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40681be5-6fbe-4424-a80e-8e61c9e2948a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac49b94-8fad-48fb-b249-ec620a4f5b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a4f3a-92b2-485d-b5f3-0f0815293938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a05ee6-42ec-486e-9ad7-b35575e19fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.chatterbox_dataset_grounding import GroundingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aae0659-809d-4395-b4ce-1067c00401e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8516"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grounding_dataset = GroundingDataset(\n",
    "    \"Augmented-Apollo-MLLM-data\",\n",
    "    tokenizer,\n",
    "    \"openai/clip-vit-large-patch14\",\n",
    "    dataset=\"mygr\",#\"jackground||jacklogicground\",\n",
    "    sample_rate=[1],#[1, 1],\n",
    "    vqa_data='llava_instruct_150k',\n",
    ")\n",
    "len(grounding_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be933d-6d5c-41bc-8670-771da311bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grounding_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33750da7-1fd7-4023-8b8b-0884721096d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0c127-69c4-4be1-9dc1-ab6c4f4f2696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce128a95-79ff-4e6e-bfdb-0808c56100ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d41145-5287-496e-a882-70c1e28bac2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-18 19:16:07,999] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 19:16:11.971760: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-18 19:16:11.975706: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-18 19:16:12.015050: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 19:16:12.727549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import deepspeed\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from model.ChatterBox_Referrring_Grounding_grounding_dino import ChatterBox\n",
    "\n",
    "from utils.chatterbox_dataset_grounding import GroundingDataset\n",
    "from utils.chatterbox_dataset_grounding import collate_fn as grounding_collate_fn\n",
    "\n",
    "\n",
    "from utils.utils import (\n",
    "    AverageMeter,\n",
    "    ProgressMeter,\n",
    "    Summary,\n",
    "    dict_to_cuda,\n",
    "    intersectionAndUnionGPU,\n",
    ")\n",
    "\n",
    "import json\n",
    "from utils.slconfig import DictAction, SLConfig\n",
    "\n",
    "\n",
    "def parse_args(args):\n",
    "    parser = argparse.ArgumentParser(description=\"ChatterBox Model Training\")\n",
    "    parser.add_argument(\"--local_rank\", default=0, type=int, help=\"node rank\")\n",
    "    parser.add_argument(\n",
    "        \"--version\", default=\"llava-llama-2-13b-chat-lightning-preview\"\n",
    "    )\n",
    "    parser.add_argument(\"--vis_save_path\", default=\"./vis_output\", type=str)\n",
    "    parser.add_argument(\n",
    "        \"--precision\",\n",
    "        # default=\"bf16\",\n",
    "        default=\"fp16\",\n",
    "        type=str,\n",
    "        choices=[\"fp32\", \"bf16\", \"fp16\"],\n",
    "        help=\"precision for inference\",\n",
    "    )\n",
    "    parser.add_argument(\"--image_size\", default=768, type=int, help=\"image size\")\n",
    "    parser.add_argument(\"--model_max_length\", default=2048, type=int)\n",
    "    parser.add_argument(\"--lora_r\", default=16, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--vision-tower\", default=\"openai/clip-vit-large-patch14\", type=str\n",
    "    )\n",
    "    parser.add_argument(\"--load_in_8bit\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--load_in_4bit\", action=\"store_true\", default=False)\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--dataset\", default=\"jack||vqa\", type=str\n",
    "    )\n",
    "    parser.add_argument(\"--vqa_data\", default=\"../datasets/llava_instruct_150k\", type=str)\n",
    "    # parser.add_argument(\"--reason_seg_data\", default=\"ReasonSeg|train\", type=str)\n",
    "    parser.add_argument(\"--val_dataset\", default=\"ReasonSeg|val\", type=str)\n",
    "    parser.add_argument(\"--dataset_dir\", default=\"../datasets/VG/\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--base_coco17_dir\", default=\"../datasets/MSCOCO2017/\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--base_coco14_dir\", default=\"../datasets/MSCOCO2014/\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--base_vg_dir\", default=\"../datasets/VG/\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--base_flickr_dir\", default=\"../datasets/flicker30k/\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--log_base_dir\", default=\"./runs\", type=str)\n",
    "    parser.add_argument(\"--exp_name\", default=\"chatterbox\", type=str)\n",
    "\n",
    "    parser.add_argument(\"--epochs\", default=15, type=int)\n",
    "    parser.add_argument(\"--steps_per_epoch\", default=1000, type=int)\n",
    "\n",
    "    parser.add_argument(\"--grounding_batch_size\", default=1, type=int, help=\"batch size per device per step\")\n",
    "    parser.add_argument(\"--referring_batch_size\", default=1, type=int, help=\"batch size per device per step\")\n",
    "    parser.add_argument(\"--vqa_batch_size\", default=1, type=int, help=\"batch size per device per step\")\n",
    "    parser.add_argument(\"--grounding_grad_accumulation_steps\", default=1, type=int)\n",
    "    parser.add_argument(\"--referring_grad_accumulation_steps\", default=1, type=int)\n",
    "    parser.add_argument(\"--vqa_grad_accumulation_steps\", default=1, type=int)\n",
    "    parser.add_argument(\"--val_batch_size\", default=1, type=int)\n",
    "    parser.add_argument(\"--workers\", default=4, type=int)\n",
    "    parser.add_argument(\"--lr\", default=0.000010, type=float)\n",
    "    parser.add_argument(\"--dice_loss_weight\", default=0.5, type=float)\n",
    "    parser.add_argument(\"--bce_loss_weight\", default=2.0, type=float)\n",
    "    parser.add_argument(\"--lora_alpha\", default=16, type=int)\n",
    "    parser.add_argument(\"--lora_dropout\", default=0.05, type=float)\n",
    "    parser.add_argument(\"--lora_target_modules\", default=\"q_proj,v_proj\", type=str)\n",
    "    parser.add_argument(\"--explanatory\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--beta1\", default=0.9, type=float)\n",
    "    parser.add_argument(\"--beta2\", default=0.95, type=float)\n",
    "    parser.add_argument(\"--num_classes_per_sample\", default=3, type=int)\n",
    "    parser.add_argument(\"--exclude_val\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--no_eval\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--eval_only\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--vision_pretrained\", default=\"PATH_TO_DINO\", type=str)\n",
    "    parser.add_argument(\"--weight\", default=\"\",type=str)\n",
    "    parser.add_argument(\"--print_freq\", default=1, type=int)\n",
    "    parser.add_argument(\"--start_epoch\", default=0, type=int)\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "def vision_branch_args():\n",
    "    def get_args_parser():\n",
    "        parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "        parser.add_argument('--config_file', '-c', default=\"./config/cfg_odvg_swinbase.py\",type=str)\n",
    "        parser.add_argument('--options',\n",
    "            nargs='+',\n",
    "            action=DictAction,\n",
    "            help='override some settings in the used config, the key-value pair '\n",
    "            'in xxx=yyy format will be merged into config file.')\n",
    "        parser.add_argument('--version', default='llava-llama-2-13b-chat-lightning-preview', help='version of the model')\n",
    "\n",
    "        # dataset parameters\n",
    "        parser.add_argument(\"--datasets\", type=str, help='path to datasets json')\n",
    "        parser.add_argument('--remove_difficult', action='store_true')\n",
    "        parser.add_argument('--fix_size', action='store_true')\n",
    "\n",
    "        # training parameters\n",
    "        parser.add_argument('--output_dir', default='',\n",
    "                            help='path where to save, empty for no saving')\n",
    "        parser.add_argument('--note', default='',\n",
    "                            help='add some notes to the experiment')\n",
    "        parser.add_argument('--device', default='cuda',\n",
    "                            help='device to use for training / testing')\n",
    "        parser.add_argument('--seed', default=42, type=int)\n",
    "        parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "        parser.add_argument('--pretrained', default=\"groundingdino_swinb_cogcoor.pth\",help='load from other checkpoint')\n",
    "        parser.add_argument('--finetune_ignore', type=str, nargs='+')\n",
    "        parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                            help='start epoch')\n",
    "        parser.add_argument('--eval', action='store_true')\n",
    "        parser.add_argument('--num_workers', default=8, type=int)\n",
    "        parser.add_argument('--test', action='store_true')\n",
    "        parser.add_argument('--debug', action='store_true')\n",
    "        parser.add_argument('--find_unused_params', action='store_true')\n",
    "        parser.add_argument('--save_results', action='store_true')\n",
    "        parser.add_argument('--save_log', action='store_true')\n",
    "\n",
    "        # distributed training parameters\n",
    "        parser.add_argument('--world_size', default=1, type=int,\n",
    "                            help='number of distributed processes')\n",
    "        parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "        parser.add_argument('--rank', default=0, type=int,\n",
    "                            help='number of distributed processes')\n",
    "        parser.add_argument(\"--local_rank\", type=int, help='local rank for DistributedDataParallel')\n",
    "        parser.add_argument(\"--local-rank\", type=int, help='local rank for DistributedDataParallel')\n",
    "        parser.add_argument('--amp', action='store_true',\n",
    "                            help=\"Train with mixed precision\")\n",
    "        return parser\n",
    "\n",
    "    parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    cfg = SLConfig.fromfile(args.config_file)\n",
    "    if args.options is not None:\n",
    "        cfg.merge_from_dict(args.options)\n",
    "\n",
    "    save_json_path = os.path.join(args.output_dir, \"config_args_raw.json\")\n",
    "    with open(save_json_path, 'w') as f:\n",
    "        json.dump(vars(args), f, indent=2)\n",
    "\n",
    "    cfg_dict = cfg._cfg_dict.to_dict()\n",
    "    args_vars = vars(args)\n",
    "    for k, v in cfg_dict.items():\n",
    "        if k not in args_vars:\n",
    "            setattr(args, k, v)\n",
    "        else:\n",
    "            raise ValueError(\"Key {} can used by args only\".format(k))\n",
    "\n",
    "    return args\n",
    "\n",
    "def train(\n",
    "        grounding_dataloader,\n",
    "        model,\n",
    "        epoch,\n",
    "        scheduler,\n",
    "        writer,\n",
    "        grounding_iter,\n",
    "        optimizer,\n",
    "        args,\n",
    "):\n",
    "    \"\"\"Main training loop.\"\"\"\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
    "    data_time = AverageMeter(\"Data\", \":6.3f\")\n",
    "    losses = AverageMeter(\"Loss\", \":.4f\")\n",
    "    vqa_losses = AverageMeter(\"VQALoss\", \":.4f\")\n",
    "    vg_losses = AverageMeter(\"VGLoss\", \":.4f\")\n",
    "\n",
    "    progress = ProgressMeter(\n",
    "        args.steps_per_epoch,\n",
    "        [\n",
    "            batch_time,\n",
    "            losses,\n",
    "            vqa_losses,\n",
    "            vg_losses,\n",
    "        ],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch),\n",
    "    )\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for global_step in range(args.steps_per_epoch):\n",
    "\n",
    "        for i in range(args.grounding_grad_accumulation_steps):\n",
    "            try:\n",
    "                input_dict = next(grounding_iter)\n",
    "            except:\n",
    "                grounding_iter = iter(grounding_dataloader)\n",
    "                input_dict = next(grounding_iter)\n",
    "\n",
    "            data_time.update(time.time() - end)\n",
    "            input_dict = dict_to_cuda(input_dict)\n",
    "\n",
    "            try:\n",
    "                if args.precision == \"fp16\":\n",
    "                    input_dict[\"images\"] = input_dict[\"images\"].half()\n",
    "                    input_dict[\"images_clip\"] = input_dict[\"images_clip\"].half()\n",
    "                elif args.precision == \"bf16\":\n",
    "                    input_dict[\"images\"] = input_dict[\"images\"].bfloat16()\n",
    "                    input_dict[\"images_clip\"] = input_dict[\"images_clip\"].bfloat16()\n",
    "                else:\n",
    "                    input_dict[\"images\"] = input_dict[\"images\"].float()\n",
    "                    input_dict[\"images_clip\"] = input_dict[\"images_clip\"].float()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            meta_input_dict = {\n",
    "                \"images\": input_dict['images'],\n",
    "                \"images_clip\": input_dict['images_clip'],\n",
    "                \"regions_lists\": [],\n",
    "                'input_ids': input_dict['input_ids'],\n",
    "                'labels': input_dict['labels'],\n",
    "                'attention_masks': input_dict['attention_masks'],\n",
    "                'offset': input_dict['offset'],\n",
    "                'bboxes_gt_list': input_dict['bboxes_gt_list'],\n",
    "                'label_gt_list': input_dict['label_list'],\n",
    "            }\n",
    "\n",
    "\n",
    "            # with torch.cuda.amp.autocast(enabled=True):\n",
    "            output_dict = model(**meta_input_dict)\n",
    "\n",
    "            vqa_loss = output_dict[\"vqa_loss\"]\n",
    "            vg_loss = output_dict[\"vg_loss\"]\n",
    "            loss = vqa_loss + vg_loss\n",
    "\n",
    "            losses.update(loss.item(), input_dict[\"images\"].size(0))\n",
    "            vqa_losses.update(vqa_loss.item(), input_dict[\"images\"].size(0))\n",
    "            vg_losses.update(vg_loss.item(), input_dict[\"images\"].size(0))\n",
    "            model.backward(loss)\n",
    "            model.step()\n",
    "\n",
    "            print(\"Step:\", global_step)\n",
    "            print(\"Loss:\", loss.item())\n",
    "            print(\"VQA Loss:\", vqa_loss.item())\n",
    "            print(\"VG Loss:\", vg_loss.item())\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # if global_step % args.print_freq == 0:\n",
    "        if args.distributed:\n",
    "            batch_time.all_reduce()\n",
    "            data_time.all_reduce()\n",
    "\n",
    "            losses.all_reduce()\n",
    "            vqa_losses.all_reduce()\n",
    "            vg_losses.all_reduce()\n",
    "\n",
    "        if args.local_rank == 0:\n",
    "            progress.display(global_step + 1)\n",
    "            writer.add_scalar(\"train/loss\", losses.avg, global_step)\n",
    "            writer.add_scalar(\"train/vg_losses\", vg_losses.avg, global_step)\n",
    "            writer.add_scalar(\n",
    "                \"metrics/total_secs_per_batch\", batch_time.avg, global_step\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"metrics/data_secs_per_batch\", data_time.avg, global_step\n",
    "            )\n",
    "\n",
    "        batch_time.reset()\n",
    "        data_time.reset()\n",
    "        losses.reset()\n",
    "        vqa_losses.reset()\n",
    "        vg_losses.reset()\n",
    "\n",
    "        if global_step != 0:\n",
    "            scheduler.step()  # new add line\n",
    "            curr_lr = scheduler.get_last_lr()\n",
    "            print('curr_lr = ', curr_lr)\n",
    "            if args.local_rank == 0:\n",
    "                writer.add_scalar(\"train/lr\", curr_lr[0], global_step)\n",
    "\n",
    "        if global_step % 50 == 0:\n",
    "            save_dir = os.path.join('./outputs/', f'epoch_{epoch}')\n",
    "            if args.local_rank == 0:\n",
    "                if os.path.exists(save_dir):\n",
    "                    shutil.rmtree(save_dir)\n",
    "            torch.distributed.barrier()\n",
    "            model.save_checkpoint(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033d157f-3dd4-4247-8e1b-856a8661fabe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(local_rank=0, version='llava-llama-2-13b-chat-lightning-preview', vis_save_path='./vis_output', precision='fp16', image_size=768, model_max_length=2048, lora_r=16, vision_tower='openai/clip-vit-large-patch14', load_in_8bit=False, load_in_4bit=False, dataset='jack||vqa', vqa_data='../datasets/llava_instruct_150k', val_dataset='ReasonSeg|val', dataset_dir='../datasets/VG/', base_coco17_dir='../datasets/MSCOCO2017/', base_coco14_dir='../datasets/MSCOCO2014/', base_vg_dir='../datasets/VG/', base_flickr_dir='../datasets/flicker30k/', log_base_dir='./runs', exp_name='chatterbox', epochs=15, steps_per_epoch=1000, grounding_batch_size=1, referring_batch_size=1, vqa_batch_size=1, grounding_grad_accumulation_steps=1, referring_grad_accumulation_steps=1, vqa_grad_accumulation_steps=1, val_batch_size=1, workers=4, lr=1e-05, dice_loss_weight=0.5, bce_loss_weight=2.0, lora_alpha=16, lora_dropout=0.05, lora_target_modules='q_proj,v_proj', explanatory=0.1, beta1=0.9, beta2=0.95, num_classes_per_sample=3, exclude_val=False, no_eval=False, eval_only=False, vision_pretrained='PATH_TO_DINO', weight='', print_freq=1, start_epoch=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "args = argparse.Namespace()\n",
    "args = parse_args([])\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f0505b-8d21-4d4d-b8c8-b3316e53b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.log_dir = os.path.join(args.log_base_dir, args.exp_name)\n",
    "if args.local_rank == 0:\n",
    "    os.makedirs(args.log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(args.log_dir)\n",
    "else:\n",
    "    writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4813db4b-e3f2-4c60-8b8c-1a506287073a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='llava-llama-2-13b-chat-lightning-preview', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    args.version,\n",
    "    cache_dir=None,\n",
    "    model_max_length=args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "num_added_tokens = tokenizer.add_tokens(\"[VG]\")\n",
    "ret_token_idx = tokenizer(\"[VG]\", add_special_tokens=False).input_ids\n",
    "args.vg_token_idx = ret_token_idx[0]  # 30523\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d34bec9-5cf7-477c-9a62-ca9e0adc9d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(config_file='./config/cfg_odvg_swinbase.py', options=None, version='llava-llama-2-13b-chat-lightning-preview', datasets=None, remove_difficult=False, fix_size=False, output_dir='', note='', device='cuda', seed=42, resume='', pretrained='groundingdino_swinb_cogcoor.pth', finetune_ignore=None, start_epoch=0, eval=False, num_workers=8, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=None, amp=False, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, batch_size=6, modelname='groundingdino', backbone='swin_B_384_22k', position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], enc_layers=6, dec_layers=6, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, num_feature_levels=4, enc_n_points=4, dec_n_points=4, two_stage_type='standard', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, transformer_activation='relu', dec_pred_bbox_embed_share=True, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, dn_label_coef=1.0, dn_bbox_coef=1.0, embed_init_tgt=True, dn_labelbook_size=91, max_text_len=256, text_encoder_type='bert-base-uncased', use_text_enhancer=True, use_fusion_layer=True, use_checkpoint=True, use_transformer_ckpt=True, use_text_cross_attention=True, text_dropout=0.0, fusion_dropout=0.0, fusion_droppath=0.1, sub_sentence_present=True, max_labels=50, lr=0.0001, backbone_freeze_keywords=None, freeze_keywords=['bert'], lr_backbone=1e-05, lr_backbone_names=['backbone.0', 'bert'], lr_linear_proj_mult=1e-05, lr_linear_proj_names=['ref_point_head', 'sampling_offsets'], weight_decay=0.0001, param_dict_type='ddetr_in_mmdet', ddetr_lr_param=False, epochs=15, lr_drop=4, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[4, 8], frozen_weights=None, dilation=False, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=1.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=2.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, mask_loss_coef=1.0, dice_loss_coef=1.0, focal_alpha=0.25, focal_gamma=2.0, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_class_embed_share=True, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, use_coco_eval=True, dn_scalar=100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_args = vision_branch_args()\n",
    "vision_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966a52be-3f3a-4d84-b621-e5467b6ea0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config  >>>  LlavaConfig {\n",
      "  \"_name_or_path\": \"llava-llama-2-13b-chat-lightning-preview\",\n",
      "  \"architectures\": [\n",
      "    \"LlavaLlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"freeze_mm_mlp_adapter\": false,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 5120,\n",
      "  \"image_aspect_ratio\": \"square\",\n",
      "  \"image_grid_pinpoints\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 13824,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mm_hidden_size\": 1024,\n",
      "  \"mm_resampler_type\": null,\n",
      "  \"mm_use_im_patch_token\": false,\n",
      "  \"mm_use_im_start_end\": false,\n",
      "  \"mm_vision_select_feature\": \"patch\",\n",
      "  \"mm_vision_select_layer\": -2,\n",
      "  \"mm_vision_tower\": \"openai/clip-vit-large-patch14\",\n",
      "  \"model_type\": \"llava\",\n",
      "  \"num_attention_heads\": 40,\n",
      "  \"num_hidden_layers\": 40,\n",
      "  \"num_key_value_heads\": 40,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"tune_mm_mlp_adapter\": false,\n",
      "  \"tune_mm_vision_resampler\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"use_mm_proj\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd4dd8c232c4c299d84038ab2526143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model = ChatterBox(\n",
    "    args.local_rank,\n",
    "    args.vg_token_idx,\n",
    "    tokenizer,\n",
    "    args.version,\n",
    "    args.lora_r,\n",
    "    args.precision,\n",
    "    vision_tower=args.vision_tower,\n",
    "    load_in_8bit=args.load_in_8bit,\n",
    "    load_in_4bit=args.load_in_4bit,\n",
    "    vision_branch_args=vision_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ebdfe2-1b6f-4633-b4a7-450abae54bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d89c126d-2dc3-4327-80fb-1d6958bb76b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])\n",
      "loading vision branch  >>  None\n"
     ]
    }
   ],
   "source": [
    "if vision_args.pretrained:\n",
    "    state_dict = torch.load(vision_args.pretrained, map_location='cpu')['model']\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if 'transformer.tgt_embed.weight' not in k and 'class' not in k:\n",
    "            new_state_dict[k] = v\n",
    "    msg = model.load_vision_dict(new_state_dict, strict=False)\n",
    "    print('loading vision branch  >> ', msg)\n",
    "\n",
    "if args.weight:\n",
    "    print('loading from ', args.weight)\n",
    "    state_dict = torch.load(args.weight, map_location=\"cpu\")['module']\n",
    "    model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "526cd481-2acf-4741-aa48-17bb4426f92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_size = 1 # torch.cuda.device_count()\n",
    "args.distributed = world_size > 1\n",
    "args.distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e380dd2-5ea9-4de2-9a67-a436ef966a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[2.1804, 2.1804, 2.1804,  ..., 2.1804, 2.1804, 2.1804],\n",
       "          [2.0263, 2.0263, 2.0263,  ..., 2.0263, 2.0263, 2.0263],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          ...,\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.0777, 2.0948, 2.1119],\n",
       "          [2.1290, 2.1290, 2.1290,  ..., 1.9749, 1.9578, 1.9920],\n",
       "          [1.6153, 1.6153, 1.6153,  ..., 1.5982, 1.4440, 1.5982]],\n",
       " \n",
       "         [[2.2885, 2.2885, 2.2885,  ..., 2.2885, 2.2885, 2.2885],\n",
       "          [2.1660, 2.1660, 2.1660,  ..., 2.1660, 2.1660, 2.1660],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          ...,\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.2535, 2.2710, 2.2885],\n",
       "          [2.3060, 2.3060, 2.3060,  ..., 2.1485, 2.1310, 2.1660],\n",
       "          [1.7808, 1.7808, 1.7808,  ..., 1.7633, 1.6057, 1.7633]],\n",
       " \n",
       "         [[2.3088, 2.3088, 2.3088,  ..., 2.3088, 2.3088, 2.3088],\n",
       "          [2.2391, 2.2391, 2.2391,  ..., 2.2391, 2.2391, 2.2391],\n",
       "          [2.5703, 2.5703, 2.5703,  ..., 2.5703, 2.5703, 2.5703],\n",
       "          ...,\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.4657, 2.4831, 2.5006],\n",
       "          [2.5180, 2.5180, 2.5180,  ..., 2.3611, 2.3437, 2.3786],\n",
       "          [1.9951, 1.9951, 1.9951,  ..., 1.9777, 1.8208, 1.9777]]]),\n",
       " tensor([[[1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.9157, 1.9157, 1.9157,  ..., 1.9303, 1.9303, 1.9303],\n",
       "          [1.9303, 1.9303, 1.9303,  ..., 1.8865, 1.8865, 1.8865],\n",
       "          ...,\n",
       "          [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n",
       "          [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n",
       "          [1.6676, 1.6676, 1.6676,  ..., 1.6822, 1.6676, 1.6676]],\n",
       " \n",
       "         [[1.9248, 1.9248, 1.9248,  ..., 1.9248, 1.9248, 1.9248],\n",
       "          [2.0599, 2.0599, 2.0599,  ..., 2.0749, 2.0749, 2.0749],\n",
       "          [2.0749, 2.0749, 2.0749,  ..., 2.0299, 2.0299, 2.0299],\n",
       "          ...,\n",
       "          [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
       "          [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
       "          [1.8047, 1.8047, 1.8047,  ..., 1.8198, 1.8047, 1.8047]],\n",
       " \n",
       "         [[1.8757, 1.8757, 1.8757,  ..., 1.8757, 1.8757, 1.8757],\n",
       "          [2.1175, 2.1175, 2.1175,  ..., 2.1175, 2.1175, 2.1175],\n",
       "          [2.1317, 2.1317, 2.1317,  ..., 2.0890, 2.0890, 2.0890],\n",
       "          ...,\n",
       "          [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n",
       "          [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n",
       "          [1.8899, 1.8899, 1.8899,  ..., 1.9042, 1.8899, 1.8899]]]),\n",
       " [\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <im_start><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_patch><im_end>\\n Task: Give me all people working in Google in New York with minimum 5 years of experience. \\n Previous Action: TYPE: Type Google in the Enter Companies tab \\nGive me the next action ASSISTANT: CLICK: Click the Apply Filters  button. </s>\"],\n",
       " [tensor([0.9315, 0.3548, 0.0734, 0.0400])],\n",
       " [-1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print('before build dataset')\n",
    "grounding_dataset = GroundingDataset(\n",
    "    args.dataset_dir,\n",
    "    # args.base_coco17_dir,\n",
    "    tokenizer,\n",
    "    args.vision_tower,\n",
    "    dataset=\"mygr\",#\"jackground||jacklogicground\",\n",
    "    sample_rate=[1],#[1, 1],\n",
    "    vqa_data='llava_instruct_150k',\n",
    ")\n",
    "grounding_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef342918-66ae-4bce-ae0e-1d4f19d18f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds_grounding_config = {\n",
    "    \"train_micro_batch_size_per_gpu\": args.grounding_batch_size,\n",
    "    # \"train_micro_batch_size_per_gpu\": args.grounding_batch_size,\n",
    "    \"gradient_accumulation_steps\": args.grounding_grad_accumulation_steps,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": args.lr,\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"betas\": (args.beta1, args.beta2),\n",
    "        },\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupDecayLR\",\n",
    "        \"params\": {\n",
    "            \"total_num_steps\": args.epochs * args.steps_per_epoch,\n",
    "            \"warmup_min_lr\": 0,\n",
    "            \"warmup_max_lr\": args.lr,\n",
    "            \"warmup_num_steps\": 50,\n",
    "            \"warmup_type\": \"linear\",\n",
    "        },\n",
    "    },\n",
    "    \"fp16\": {\n",
    "        \"enabled\": args.precision == \"fp16\",\n",
    "        # \"loss_scale\": 0,\n",
    "        # \"loss_scale_window\": 1000,\n",
    "        # \"hysteresis\": 2,\n",
    "        # \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"bf16\": {\n",
    "        \"enabled\": args.precision == \"bf16\",\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"overlap_comm\": True,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 5e8,\n",
    "        \"allgather_bucket_size\": 5e8,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "595cfa2a-1850-4989-95e5-0c2a4bf7bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming grounding_dataset and grounding_collate_fn are already defined\n",
    "grounding_dataloader = DataLoader(grounding_dataset, batch_size=32, shuffle=True, collate_fn=partial(grounding_collate_fn, tokenizer=tokenizer))\n",
    "\n",
    "# Assuming model is already defined\n",
    "model = model.to('cuda')  # Move model to GPU\n",
    "\n",
    "# Define your optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "556e1f6f-faa2-417a-84f8-85bd7434100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(grounding_dataloader):\n",
    "    batch = {k: v.to('cuda') if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15859332-1759-41fe-adc7-420709322abb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RUnning...\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expand(): argument 'size' (position 1) must be tuple of ints, but found element of type Tensor at pos 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     14\u001b[0m vqa_loss \u001b[38;5;241m=\u001b[39m output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvqa_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/ChatterBox-Finetuning/model/ChatterBox_Referrring_Grounding_grounding_dino.py:278\u001b[0m, in \u001b[0;36mChatterBox.forward\u001b[0;34m(self, images, images_clip, regions_lists, input_ids, labels, attention_masks, offset, bboxes_gt_list, label_gt_list, inference, image_path, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(offset) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    274\u001b[0m     start_i, end_i \u001b[38;5;241m=\u001b[39m offset[i], offset[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    275\u001b[0m     images_clip_i \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    276\u001b[0m         \u001b[43mimages_clip\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# why here expand?\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m     images_clip_list\u001b[38;5;241m.\u001b[39mappend(images_clip_i)\n\u001b[1;32m    282\u001b[0m images_clip \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(images_clip_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: expand(): argument 'size' (position 1) must be tuple of ints, but found element of type Tensor at pos 0"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(grounding_dataloader):\n",
    "    # Move batch to GPU\n",
    "    # batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "    batch = {k: v.half().to('cuda') if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "    batch['regions_lists'] = []\n",
    "    batch['label_gt_list'] = batch['label_list']\n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output_dict = model(**batch)\n",
    "\n",
    "    # Compute loss\n",
    "    vqa_loss = output_dict[\"vqa_loss\"]\n",
    "    vg_loss = output_dict[\"vg_loss\"]\n",
    "    loss = vqa_loss + vg_loss\n",
    "    break\n",
    "\n",
    "#     # Backward pass and optimize\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Update the learning rate\n",
    "#     scheduler.step()\n",
    "\n",
    "#     # Print some statistics\n",
    "#     if i % 100 == 0:\n",
    "#         print(f'Epoch [{epoch}/{args.epochs}], Step [{i}/{len(grounding_dataloader)}], Loss: {loss.item()}')\n",
    "\n",
    "# # Save the model checkpoint\n",
    "# torch.save(model.state_dict(), f'./outputs/epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07780e4-9c2e-412d-8e0d-00543b7a668a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358c588-54ba-4a27-a782-8569be50590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_engine, optimizer, grounding_dataloader, scheduler = deepspeed.initialize(\n",
    "    model=model,\n",
    "    model_parameters=model.parameters(),\n",
    "    training_data=grounding_dataset,\n",
    "    collate_fn=partial(grounding_collate_fn, tokenizer=tokenizer),\n",
    "    config=ds_grounding_config,\n",
    ")\n",
    "\n",
    "grounding_iter = iter(grounding_dataloader)\n",
    "\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    train(\n",
    "        grounding_dataloader,\n",
    "        model_engine,\n",
    "        epoch,\n",
    "        scheduler,\n",
    "        writer,\n",
    "        grounding_iter,\n",
    "        optimizer,\n",
    "        args,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

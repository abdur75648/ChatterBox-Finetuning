 ______                 ______            _
(_____ \               (_____ \          | |
 _____) ) _   _  ____   _____) )___    __| |
|  __  / | | | ||  _ \ |  ____// _ \  / _  |
| |  \ \ | |_| || | | || |    | |_| |( (_| |
|_|   |_||____/ |_| |_||_|     \___/  \____|

For detailed documentation and guides, please visit:
[1;34mhttps://docs.runpod.io/[0m and [1;34mhttps://blog.runpod.io/[0m


[?2004hroot@459b28a48274:/workspace/ChatterBox-Finetuning# 
[?2004l[?2004hroot@459b28a48274:/workspace/ChatterBox-Finetuning# clear
[?2004l[H[2J[?2004hroot@459b28a48274:/workspace/ChatterBox-Finetuning# clearscreen -dmS test -Lclear[Kapt-get install screen[3Pscreen -dmS test -Lclear[Kmv screenlog.0 screenlog_old.txtclear[Kgit pull[3Pclearrm -rf outputs/[10Pcleardeepspeed --include localhost:0,1 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview[A[C[C[C[C[C[Cclear[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cdeepspeed --include localhost:0,1 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview[A[C[C[C[C[C[Cclear[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cdeepspeed --include localhost:0,1 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview
[?2004l[2024-02-22 22:01:35,284] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-02-22 22:01:37.922053: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-22 22:01:37.925587: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-22 22:01:37.964724: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-22 22:01:38.815889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-02-22 22:01:40,076] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-02-22 22:01:40,077] [INFO] [runner.py:570:main] cmd = /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=54906 --enable_each_rank_log=None custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview
[2024-02-22 22:01:41,755] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-02-22 22:01:44.130182: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-22 22:01:44.133651: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-22 22:01:44.174053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-22 22:01:44.928795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-02-22 22:01:46,013] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8
[2024-02-22 22:01:46,013] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1
[2024-02-22 22:01:46,013] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1
[2024-02-22 22:01:46,013] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-02-22 22:01:46,013] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8
[2024-02-22 22:01:46,013] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-02-22 22:01:46,014] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1
[2024-02-22 22:01:46,014] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2024-02-22 22:01:46,014] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-02-22 22:01:46,014] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-02-22 22:01:46,014] [INFO] [launch.py:163:main] dist_world_size=2
[2024-02-22 22:01:46,014] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2024-02-22 22:01:47,989] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-22 22:01:48,004] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[Kroot@459b28a48274:/workspace/ChatterBox-Finetuning# ^C[?2004l[?2004h[?2004l
[?2004hroot@459b28a48274:/workspace/ChatterBox-Finetuning# [?2004l
exit
2024-02-22 22:01:50.487303: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-22 22:01:50.490759: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-22 22:01:50.530504: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-22 22:01:50.627028: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-22 22:01:50.630486: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-22 22:01:50.669708: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-22 22:01:51.286959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-22 22:01:51.426352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|                                                                               | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                               | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███████████████████████▋                                               | 1/3 [00:57<01:55, 57.85s/it]Loading checkpoint shards:  33%|███████████████████████▋                                               | 1/3 [00:57<01:55, 57.93s/it]Loading checkpoint shards:  67%|███████████████████████████████████████████████▎                       | 2/3 [01:42<00:50, 50.30s/it]Loading checkpoint shards:  67%|███████████████████████████████████████████████▎                       | 2/3 [01:42<00:50, 50.35s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [02:19<00:00, 43.94s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [02:19<00:00, 46.43s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [02:19<00:00, 43.93s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [02:19<00:00, 46.41s/it]
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Number of trainable parameters: 791.85M
Number of trainable parameters in model.lm: 340.84M
Number of trainable parameters in model.visual_grounding_model: 123.22M
trainable params: 340,838,400 || all params: 13,034,270,720 || trainable%: 2.6149403163539633
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-22 22:05:42,612] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-22 22:05:42,612] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-22 22:05:42,612] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Number of trainable parameters: 791.85M
Number of trainable parameters in model.lm: 340.84M
Number of trainable parameters in model.visual_grounding_model: 123.22M
trainable params: 340,838,400 || all params: 13,034,270,720 || trainable%: 2.6149403163539633
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-22 22:05:45,121] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-22 22:05:45,122] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-22 22:06:06,270] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.08755898475646973 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10170459747314453 seconds
[2024-02-22 22:06:06,637] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-02-22 22:06:06,638] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-02-22 22:06:06,926] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-02-22 22:06:06,926] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-02-22 22:06:06,926] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-02-22 22:06:06,926] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 500000000
[2024-02-22 22:06:06,926] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 500000000
[2024-02-22 22:06:06,926] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: False
[2024-02-22 22:06:06,926] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
==> Epoch 0/30
[2024-02-22 22:06:09,642] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-02-22 22:06:09,643] [INFO] [utils.py:803:see_memory_usage] MA 27.36 GB         Max_MA 28.09 GB         CA 28.46 GB         Max_CA 28 GB 
[2024-02-22 22:06:09,643] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 80.57 GB, percent = 16.0%
[2024-02-22 22:06:09,852] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-02-22 22:06:09,853] [INFO] [utils.py:803:see_memory_usage] MA 30.31 GB         Max_MA 31.78 GB         CA 32.88 GB         Max_CA 33 GB 
[2024-02-22 22:06:09,853] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 80.66 GB, percent = 16.0%
[2024-02-22 22:06:09,854] [INFO] [stage_1_and_2.py:513:__init__] optimizer state initialized
[2024-02-22 22:06:10,061] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-02-22 22:06:10,062] [INFO] [utils.py:803:see_memory_usage] MA 30.31 GB         Max_MA 30.31 GB         CA 32.88 GB         Max_CA 33 GB 
[2024-02-22 22:06:10,062] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 80.87 GB, percent = 16.1%
[2024-02-22 22:06:10,084] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-02-22 22:06:10,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-02-22 22:06:10,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f3990b0f340>
[2024-02-22 22:06:10,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2024-02-22 22:06:10,090] [INFO] [config.py:968:print] DeepSpeedEngine configuration:
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   amp_enabled .................. False
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   amp_params ................... False
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   bfloat16_enabled ............. False
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   checkpoint_parallel_write_pipeline  False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   checkpoint_tag_validation_enabled  True
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   checkpoint_tag_validation_fail  False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3990b0fdf0>
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   communication_data_type ...... None
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   curriculum_enabled_legacy .... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   curriculum_params_legacy ..... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   data_efficiency_enabled ...... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   dataloader_drop_last ......... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   disable_allgather ............ False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   dump_state ................... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   dynamic_loss_scale_args ...... None
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_enabled ........... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_gas_boundary_resolution  1
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_layer_num ......... 0
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_max_iter .......... 100
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_stability ......... 1e-06
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_tol ............... 0.01
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_verbose ........... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   elasticity_enabled ........... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   fp16_auto_cast ............... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   fp16_enabled ................. True
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   fp16_master_weights_and_gradients  False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   global_rank .................. 0
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   grad_accum_dtype ............. None
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   gradient_accumulation_steps .. 20
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   gradient_clipping ............ 1.0
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   gradient_predivide_factor .... 1.0
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   initial_dynamic_scale ........ 65536
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   load_universal_checkpoint .... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   loss_scale ................... 0
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   memory_breakdown ............. False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   mics_hierarchial_params_gather  False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   mics_shard_size .............. -1
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   optimizer_legacy_fusion ...... False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   optimizer_name ............... adamw
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   optimizer_params ............. {'lr': 1e-05, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   pld_enabled .................. False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   pld_params ................... False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   prescale_gradients ........... False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   scheduler_name ............... WarmupDecayLR
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   scheduler_params ............. {'total_num_steps': 15000, 'warmup_min_lr': 0, 'warmup_max_lr': 1e-05, 'warmup_num_steps': 50, 'warmup_type': 'linear'}
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   sparse_attention ............. None
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   sparse_gradients_enabled ..... False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   steps_per_print .............. 10
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   train_batch_size ............. 160
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   train_micro_batch_size_per_gpu  4
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   use_node_local_storage ....... False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   wall_clock_breakdown ......... False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   weight_quantization_config ... None
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   world_size ................... 2
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   zero_allow_untested_optimizer  False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   zero_enabled ................. True
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   zero_force_ds_cpu_optimizer .. True
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   zero_optimization_stage ...... 2
[2024-02-22 22:06:10,092] [INFO] [config.py:958:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 20, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 1e-05, 
            "weight_decay": 0.0, 
            "betas": [0.9, 0.95]
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "total_num_steps": 1.500000e+04, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 1e-05, 
            "warmup_num_steps": 50, 
            "warmup_type": "linear"
        }
    }, 
    "fp16": {
        "enabled": true
    }, 
    "bf16": {
        "enabled": false
    }, 
    "gradient_clipping": 1.0, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "allgather_bucket_size": 5.000000e+08
    }
}
==> Epoch 0/30
/workspace/ChatterBox-Finetuning/model/llava/model/llava_jack_gpt4roi.py:302: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  assert bbox_token_id == None or (torch.sum(torch.tensor(cur_input_ids == bbox_token_id)) == 0)
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/workspace/ChatterBox-Finetuning/model/llava/model/llava_jack_gpt4roi.py:302: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  assert bbox_token_id == None or (torch.sum(torch.tensor(cur_input_ids == bbox_token_id)) == 0)
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2024-02-22 22:06:56,221] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
Epoch: [0][  1/500]	Time 45.756 (46.208)	Loss 20.6868 (20.3404)	VQALoss 2.7363 (2.7582)	VGLoss 17.9505 (17.5822)
[2024-02-22 22:06:56,227] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1 is about to be saved!
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-22 22:07:08,835] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step1/mp_rank_00_model_states.pt
[2024-02-22 22:07:08,835] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/mp_rank_00_model_states.pt...
[2024-02-22 22:09:39,680] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/mp_rank_00_model_states.pt.
[2024-02-22 22:09:41,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-22 22:09:41,394] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-22 22:09:54,038] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-22 22:09:54,039] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step1/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-22 22:09:54,039] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2024-02-22 22:09:54,635] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-22 22:09:54,654] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-22 22:09:54,655] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2024-02-22 22:10:38,571] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
curr_lr =  [0.0]
curr_lr =  [0.0]
[2024-02-22 22:11:22,263] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
curr_lr =  curr_lr =  [2.0000000000000002e-07]
[2.0000000000000002e-07]
[2024-02-22 22:12:05,864] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
curr_lr =  [4.0000000000000003e-07]
curr_lr =  [4.0000000000000003e-07]
[2024-02-22 22:12:49,994] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
curr_lr =  [6.000000000000001e-07]
curr_lr =  [6.000000000000001e-07]
[2024-02-22 22:13:34,080] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
curr_lr = Epoch: [0][  6/500]	Time 44.086 (79.572)	Loss 19.6604 (20.5404)	VQALoss 2.8027 (2.7996)	VGLoss 16.8577 (17.7408) 
[8.000000000000001e-07]
curr_lr =  [8.000000000000001e-07]
[2024-02-22 22:14:18,488] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
curr_lr =  [1.0000000000000002e-06]
curr_lr =  [1.0000000000000002e-06]
[2024-02-22 22:15:02,486] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
curr_lr =  [1.2000000000000002e-06]
curr_lr =  [1.2000000000000002e-06]
[2024-02-22 22:15:46,304] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608
curr_lr =  [1.4000000000000001e-06]
curr_lr =  [1.4000000000000001e-06]
[2024-02-22 22:16:30,415] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304
curr_lr =  [1.6000000000000001e-06]
[2024-02-22 22:16:30,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[1.4000000000000001e-06], mom=[(0.9, 0.95)]
[2024-02-22 22:16:30,417] [INFO] [timer.py:260:stop] epoch=0/micro_step=200/global_step=10, RunningAvgSamplesPerSec=3.638054578505289, CurrSamplesPerSec=3.627239079725708, MemAllocated=31.1GB, MaxMemAllocated=36.52GB
curr_lr =  [1.6000000000000001e-06]
[2024-02-22 22:17:14,393] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152
Epoch: [0][ 11/500]	Time 43.977 (44.063)	Loss 18.7299 (20.6891)	VQALoss 2.6602 (2.8103)	VGLoss 16.0697 (17.8787)
curr_lr =  [1.8000000000000001e-06]
curr_lr =  [1.8000000000000001e-06]
[2024-02-22 22:17:58,202] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
curr_lr =  [2.0000000000000003e-06]
curr_lr =  [2.0000000000000003e-06]
[2024-02-22 22:18:41,971] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288
curr_lr =  [2.2e-06]
curr_lr =  [2.2e-06]
[2024-02-22 22:19:25,736] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144
curr_lr =  [2.4000000000000003e-06]
curr_lr =  [2.4000000000000003e-06]
[2024-02-22 22:20:09,376] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
curr_lr =  [2.6e-06]
curr_lr =  [2.6e-06]
[2024-02-22 22:20:54,214] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536
Epoch: [0][ 16/500]	Time 44.838 (43.964)	Loss 18.4172 (20.5097)	VQALoss 2.5488 (2.7912)	VGLoss 15.8684 (17.7185)
curr_lr =  [2.8000000000000003e-06]
curr_lr =  [2.8000000000000003e-06]
[2024-02-22 22:21:38,818] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
curr_lr =  [3e-06]
curr_lr =  [3e-06]
[2024-02-22 22:22:23,639] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
curr_lr =  [3.2000000000000003e-06]
curr_lr =  [3.2000000000000003e-06]
[2024-02-22 22:23:08,338] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
curr_lr =  [3.4000000000000005e-06]
curr_lr =  [3.4000000000000005e-06]
[2024-02-22 22:23:52,703] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
curr_lr =  [3.6000000000000003e-06]
[2024-02-22 22:23:52,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=20, lr=[3.4000000000000005e-06], mom=[(0.9, 0.95)]
[2024-02-22 22:23:52,704] [INFO] [timer.py:260:stop] epoch=0/micro_step=400/global_step=20, RunningAvgSamplesPerSec=3.6266919055726574, CurrSamplesPerSec=3.6065692478794658, MemAllocated=31.1GB, MaxMemAllocated=36.52GB
curr_lr =  [3.6000000000000003e-06]
[2024-02-22 22:24:36,343] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
curr_lr = Epoch: [0][ 21/500]	Time 43.640 (44.426)	Loss 15.6460 (20.7960)	VQALoss 2.6797 (2.7858)	VGLoss 12.9663 (18.0102) 
[3.8000000000000005e-06]
curr_lr =  [3.8000000000000005e-06]
curr_lr =  [4.2000000000000004e-06]
curr_lr =  [4.2000000000000004e-06]
curr_lr =  [4.600000000000001e-06]
curr_lr =  [4.600000000000001e-06]
[2024-02-22 22:26:49,523] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
curr_lr =  [4.800000000000001e-06]
curr_lr =  [4.800000000000001e-06]
curr_lr =  [5.2e-06]
curr_lr =  [5.2e-06]
curr_lr =  Epoch: [0][ 26/500]	Time 44.121 (44.358)	Loss 14.3240 (18.9280)	VQALoss 2.7422 (2.7897)	VGLoss 11.5818 (16.1382)[5.600000000000001e-06]

curr_lr =  [5.600000000000001e-06]
curr_lr =  [6e-06]
curr_lr =  [6e-06]
curr_lr =  [6.4000000000000006e-06]
curr_lr =  [6.4000000000000006e-06]
curr_lr =  [6.800000000000001e-06]
curr_lr =  [6.800000000000001e-06]
[2024-02-22 22:31:17,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=22, lr=[7e-06], mom=[(0.9, 0.95)]
curr_lr =  [7.2000000000000005e-06]
[2024-02-22 22:31:18,045] [INFO] [timer.py:260:stop] epoch=0/micro_step=600/global_step=30, RunningAvgSamplesPerSec=3.614536639492197, CurrSamplesPerSec=3.5238325883494683, MemAllocated=31.1GB, MaxMemAllocated=36.52GB
curr_lr =  [7.2000000000000005e-06]
curr_lr =  [7.600000000000001e-06]
Epoch: [0][ 31/500]	Time 44.587 (44.899)	Loss 12.6414 (17.0049)	VQALoss 2.2695 (2.7288)	VGLoss 10.3719 (14.2761)
curr_lr =  [7.600000000000001e-06]
curr_lr =  [8.000000000000001e-06]
curr_lr =  [8.000000000000001e-06]
curr_lr =  [8.400000000000001e-06]
curr_lr =  [8.400000000000001e-06]
curr_lr =  [8.8e-06]
curr_lr =  [8.8e-06]
curr_lr =  [9.200000000000002e-06]
curr_lr =  [9.200000000000002e-06]
curr_lr =  [9.600000000000001e-06]
Epoch: [0][ 36/500]	Time 45.001 (44.844)	Loss 11.2371 (16.2821)	VQALoss 2.5488 (2.6356)	VGLoss 8.6883 (13.6465)
curr_lr =  [9.600000000000001e-06]
curr_lr =  [1e-05]
curr_lr =  [1e-05]
curr_lr =  [9.998662207357859e-06]
curr_lr =  [9.998662207357859e-06]
curr_lr =  [9.99732441471572e-06]
curr_lr =  [9.99732441471572e-06]
[2024-02-22 22:38:46,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=22, lr=[9.99665551839465e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.995986622073579e-06]
[2024-02-22 22:38:46,777] [INFO] [timer.py:260:stop] epoch=0/micro_step=800/global_step=40, RunningAvgSamplesPerSec=3.6015570796782135, CurrSamplesPerSec=3.5385135785256647, MemAllocated=31.1GB, MaxMemAllocated=36.52GB
curr_lr =  [9.995986622073579e-06]
curr_lr =  [9.994648829431439e-06]
Epoch: [0][ 41/500]	Time 45.439 (45.072)	Loss 11.9831 (14.5810)	VQALoss 2.4355 (2.5239)	VGLoss 9.5476 (12.0571)
curr_lr =  [9.994648829431439e-06]
curr_lr =  [9.993311036789299e-06]
curr_lr =  [9.993311036789299e-06]
curr_lr =  [9.991973244147159e-06]
curr_lr =  [9.991973244147159e-06]
curr_lr =  [9.990635451505017e-06]
curr_lr =  [9.990635451505017e-06]
curr_lr =  [9.989297658862878e-06]
curr_lr =  [9.989297658862878e-06]
curr_lr = Epoch: [0][ 46/500]	Time 44.584 (44.842)	Loss 38.1763 (15.5640)	VQALoss 2.3965 (2.3738)	VGLoss 35.7798 (13.1902) 
[9.987959866220737e-06]
curr_lr =  [9.987959866220737e-06]
curr_lr =  [9.986622073578597e-06]
curr_lr =  [9.986622073578597e-06]
curr_lr =  [9.985284280936456e-06]
curr_lr =  [9.985284280936456e-06]
curr_lr =  [9.983946488294315e-06]
curr_lr =  [9.983946488294315e-06]
[2024-02-22 22:46:13,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=22, lr=[9.983277591973245e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.982608695652175e-06]
[2024-02-22 22:46:13,828] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=50, RunningAvgSamplesPerSec=3.596856093301609, CurrSamplesPerSec=3.5971282970301224, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.982608695652175e-06]
curr_lr =  [9.981270903010034e-06]
Epoch: [0][ 51/500]	Time 43.912 (44.262)	Loss 44.9737 (14.3341)	VQALoss 1.7373 (2.2667)	VGLoss 43.2364 (12.0675)
curr_lr =  [9.981270903010034e-06]
[2024-02-22 22:47:02,563] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step51 is about to be saved!
[2024-02-22 22:47:14,742] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step51/mp_rank_00_model_states.pt
[2024-02-22 22:47:14,743] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step51/mp_rank_00_model_states.pt...
[2024-02-22 22:49:45,654] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step51/mp_rank_00_model_states.pt.
[2024-02-22 22:49:47,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step51/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-22 22:49:47,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step51/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-22 22:50:03,498] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step51/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-22 22:50:03,498] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step51/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-22 22:50:03,498] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step51 is ready now!
[2024-02-22 22:50:03,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step51/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-22 22:50:03,917] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step51/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-22 22:50:03,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step51 is ready now!
curr_lr =  [9.979933110367894e-06]
curr_lr =  [9.979933110367894e-06]
curr_lr =  [9.978595317725753e-06]
curr_lr =  [9.978595317725753e-06]
curr_lr =  [9.977257525083612e-06]
curr_lr =  [9.977257525083612e-06]
curr_lr =  [9.975919732441472e-06]
curr_lr =  [9.975919732441472e-06]
curr_lr =  [9.974581939799332e-06]
Epoch: [0][ 56/500]	Time 42.949 (81.081)	Loss 10.4508 (13.4869)	VQALoss 1.7891 (2.1293)	VGLoss 8.6618 (11.3575)
curr_lr =  [9.974581939799332e-06]
curr_lr =  [9.973244147157192e-06]
curr_lr =  [9.973244147157192e-06]
curr_lr =  [9.97190635451505e-06]
curr_lr =  [9.97190635451505e-06]
curr_lr =  [9.97056856187291e-06]
curr_lr =  [9.97056856187291e-06]
[2024-02-22 22:56:39,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=22, lr=[9.96989966555184e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.96923076923077e-06]
[2024-02-22 22:56:40,030] [INFO] [timer.py:260:stop] epoch=0/micro_step=1200/global_step=60, RunningAvgSamplesPerSec=3.6041002046816395, CurrSamplesPerSec=3.5895433841664643, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.96923076923077e-06]
curr_lr =  [9.96789297658863e-06]
Epoch: [0][ 61/500]	Time 43.922 (44.162)	Loss 9.5004 (12.9077)	VQALoss 1.7451 (1.9518)	VGLoss 7.7553 (10.9559)
curr_lr =  [9.96789297658863e-06]
curr_lr =  [9.966555183946488e-06]
curr_lr =  [9.966555183946488e-06]
curr_lr =  [9.965217391304348e-06]
curr_lr =  [9.965217391304348e-06]
curr_lr =  [9.963879598662208e-06]
curr_lr =  [9.963879598662208e-06]
curr_lr =  [9.962541806020068e-06]
curr_lr =  [9.962541806020068e-06]
curr_lr =  [9.961204013377928e-06]
Epoch: [0][ 66/500]	Time 46.506 (45.323)	Loss 7.1038 (12.2383)	VQALoss 1.4570 (1.8633)	VGLoss 5.6467 (10.3749)
curr_lr =  [9.961204013377928e-06]
curr_lr =  [9.959866220735786e-06]
curr_lr =  [9.959866220735786e-06]
curr_lr =  [9.958528428093646e-06]
curr_lr =  [9.958528428093646e-06]
curr_lr =  [9.957190635451506e-06]
curr_lr =  [9.957190635451506e-06]
[2024-02-22 23:04:12,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=22, lr=[9.956521739130436e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.955852842809364e-06]
[2024-02-22 23:04:12,745] [INFO] [timer.py:260:stop] epoch=0/micro_step=1400/global_step=70, RunningAvgSamplesPerSec=3.5936689806167106, CurrSamplesPerSec=3.5482087034903507, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.955852842809364e-06]
curr_lr =  [9.954515050167226e-06]
Epoch: [0][ 71/500]	Time 45.839 (45.604)	Loss 37.8696 (13.1495)	VQALoss 1.2852 (1.7077)	VGLoss 36.5845 (11.4418)
curr_lr =  [9.954515050167226e-06]
curr_lr =  [9.953177257525084e-06]
curr_lr =  [9.953177257525084e-06]
curr_lr =  [9.951839464882944e-06]
curr_lr =  [9.951839464882944e-06]
curr_lr =  [9.950501672240804e-06]
curr_lr =  [9.950501672240804e-06]
curr_lr =  [9.949163879598664e-06]
curr_lr =  [9.949163879598664e-06]
curr_lr =  [9.947826086956522e-06]
Epoch: [0][ 76/500]	Time 45.360 (45.707)	Loss 22.4001 (12.4735)	VQALoss 1.7051 (1.6135)	VGLoss 20.6950 (10.8600)
curr_lr =  [9.947826086956522e-06]
curr_lr =  [9.946488294314382e-06]
curr_lr =  [9.946488294314382e-06]
curr_lr =  [9.945150501672242e-06]
curr_lr =  [9.945150501672242e-06]
curr_lr =  [9.9438127090301e-06]
curr_lr =  [9.9438127090301e-06]
[2024-02-22 23:11:49,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=22, lr=[9.94314381270903e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.942474916387962e-06]
[2024-02-22 23:11:49,670] [INFO] [timer.py:260:stop] epoch=0/micro_step=1600/global_step=80, RunningAvgSamplesPerSec=3.5816199644638576, CurrSamplesPerSec=3.501470540778174, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.942474916387962e-06]
curr_lr =  [9.94113712374582e-06]
Epoch: [0][ 81/500]	Time 45.602 (45.630)	Loss 15.0976 (12.6350)	VQALoss 1.3711 (1.5035)	VGLoss 13.7265 (11.1316)
curr_lr =  [9.94113712374582e-06]
curr_lr =  [9.93979933110368e-06]
curr_lr =  [9.93979933110368e-06]
curr_lr =  [9.93846153846154e-06]
curr_lr =  [9.93846153846154e-06]
curr_lr =  [9.9371237458194e-06]
curr_lr =  [9.9371237458194e-06]
curr_lr =  [9.935785953177258e-06]
curr_lr =  [9.935785953177258e-06]
curr_lr =  [9.934448160535118e-06]
Epoch: [0][ 86/500]	Time 45.397 (45.334)	Loss 5.5073 (11.3062)	VQALoss 1.0684 (1.4158)	VGLoss 4.4390 (9.8905)
curr_lr =  [9.934448160535118e-06]
curr_lr =  [9.933110367892978e-06]
curr_lr =  [9.933110367892978e-06]
curr_lr =  [9.931772575250836e-06]
curr_lr =  [9.931772575250836e-06]
curr_lr =  [9.930434782608697e-06]
curr_lr =  [9.930434782608697e-06]
[2024-02-22 23:19:21,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=22, lr=[9.929765886287627e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.929096989966556e-06]
[2024-02-22 23:19:21,853] [INFO] [timer.py:260:stop] epoch=0/micro_step=1800/global_step=90, RunningAvgSamplesPerSec=3.576666678105627, CurrSamplesPerSec=3.507200410598681, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.929096989966556e-06]
curr_lr =  [9.927759197324415e-06]
Epoch: [0][ 91/500]	Time 45.316 (45.045)	Loss 14.0521 (11.1689)	VQALoss 1.2383 (1.3024)	VGLoss 12.8138 (9.8665)
curr_lr =  [9.927759197324415e-06]
curr_lr =  [9.926421404682275e-06]
curr_lr =  [9.926421404682275e-06]
curr_lr =  [9.925083612040135e-06]
curr_lr =  [9.925083612040135e-06]
curr_lr =  [9.923745819397994e-06]
curr_lr =  [9.923745819397994e-06]
curr_lr =  [9.922408026755853e-06]
curr_lr =  [9.922408026755853e-06]
curr_lr =  [9.921070234113713e-06]
Epoch: [0][ 96/500]	Time 45.441 (45.217)	Loss 5.5097 (11.7224)	VQALoss 1.3252 (1.2664)	VGLoss 4.1845 (10.4560)
curr_lr =  [9.921070234113713e-06]
curr_lr =  [9.919732441471572e-06]
curr_lr =  [9.919732441471572e-06]
curr_lr =  [9.918394648829433e-06]
curr_lr =  [9.918394648829433e-06]
curr_lr =  [9.917056856187291e-06]
curr_lr =  [9.917056856187291e-06]
[2024-02-22 23:26:54,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=22, lr=[9.916387959866221e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.915719063545151e-06]
[2024-02-22 23:26:54,721] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=100, RunningAvgSamplesPerSec=3.5721779851469835, CurrSamplesPerSec=3.511238594501119, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.915719063545151e-06]
curr_lr =  [9.914381270903011e-06]
Epoch: [0][101/500]	Time 45.212 (45.335)	Loss 6.5075 (10.7208)	VQALoss 1.2256 (1.1764)	VGLoss 5.2819 (9.5444)
curr_lr =  [9.914381270903011e-06]
[2024-02-22 23:27:45,560] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step101 is about to be saved!
[2024-02-22 23:27:57,786] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step101/mp_rank_00_model_states.pt
[2024-02-22 23:27:57,786] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step101/mp_rank_00_model_states.pt...
[2024-02-22 23:30:22,264] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step101/mp_rank_00_model_states.pt.
[2024-02-22 23:30:23,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step101/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-22 23:30:23,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step101/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-22 23:30:37,861] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step101/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-22 23:30:37,861] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step101/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-22 23:30:37,861] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step101 is ready now!
[2024-02-22 23:30:38,666] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step101/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-22 23:30:38,673] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step101/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-22 23:30:38,673] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step101 is ready now!
curr_lr =  [9.913043478260871e-06]
curr_lr =  [9.913043478260871e-06]
curr_lr =  [9.91170568561873e-06]
curr_lr =  [9.91170568561873e-06]
curr_lr =  [9.91036789297659e-06]
curr_lr =  [9.91036789297659e-06]
curr_lr =  [9.909030100334449e-06]
curr_lr =  [9.909030100334449e-06]
curr_lr =  [9.907692307692309e-06]
Epoch: [0][106/500]	Time 45.209 (80.568)	Loss 10.4806 (11.0584)	VQALoss 1.1855 (1.1171)	VGLoss 9.2950 (9.9412)
curr_lr =  [9.907692307692309e-06]
curr_lr =  [9.906354515050169e-06]
curr_lr =  [9.906354515050169e-06]
curr_lr =  [9.905016722408027e-06]
curr_lr =  [9.905016722408027e-06]
curr_lr =  [9.903678929765887e-06]
curr_lr =  [9.903678929765887e-06]
[2024-02-22 23:37:20,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=22, lr=[9.903010033444817e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.902341137123747e-06]
[2024-02-22 23:37:20,757] [INFO] [timer.py:260:stop] epoch=0/micro_step=2200/global_step=110, RunningAvgSamplesPerSec=3.572914614444761, CurrSamplesPerSec=3.5830484714416966, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.902341137123747e-06]
curr_lr =  [9.901003344481607e-06]
Epoch: [0][111/500]	Time 44.799 (44.556)	Loss 8.2071 (10.3062)	VQALoss 0.8960 (1.0624)	VGLoss 7.3111 (9.2438)
curr_lr =  [9.901003344481607e-06]
curr_lr =  [9.899665551839465e-06]
curr_lr =  [9.899665551839465e-06]
curr_lr =  [9.898327759197325e-06]
curr_lr =  [9.898327759197325e-06]
curr_lr =  [9.896989966555185e-06]
curr_lr =  [9.896989966555185e-06]
curr_lr =  [9.895652173913045e-06]
curr_lr =  [9.895652173913045e-06]
curr_lr =  [9.894314381270905e-06]Epoch: [0][116/500]	Time 44.612 (44.341)	Loss 5.8516 (10.4870)	VQALoss 0.8423 (0.9876)	VGLoss 5.0093 (9.4994)

curr_lr =  [9.894314381270905e-06]
curr_lr =  [9.892976588628763e-06]
curr_lr =  [9.892976588628763e-06]
curr_lr =  [9.891638795986623e-06]
curr_lr =  [9.891638795986623e-06]
curr_lr =  [9.890301003344483e-06]
curr_lr =  [9.890301003344483e-06]
[2024-02-22 23:44:46,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=22, lr=[9.889632107023413e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.888963210702341e-06]
[2024-02-22 23:44:46,327] [INFO] [timer.py:260:stop] epoch=0/micro_step=2400/global_step=120, RunningAvgSamplesPerSec=3.5744406399428694, CurrSamplesPerSec=3.5846522260941622, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.888963210702341e-06]
curr_lr =  Epoch: [0][121/500]	Time 44.709 (44.755)	Loss 13.1840 (10.1140)	VQALoss 1.0254 (0.9668)	VGLoss 12.1586 (9.1472)[9.8876254180602e-06]

curr_lr =  [9.8876254180602e-06]
curr_lr =  [9.88628762541806e-06]
curr_lr =  [9.88628762541806e-06]
curr_lr =  [9.88494983277592e-06]
curr_lr =  [9.88494983277592e-06]
curr_lr =  [9.88361204013378e-06]
curr_lr =  [9.88361204013378e-06]
curr_lr =  [9.88227424749164e-06]
curr_lr =  [9.88227424749164e-06]
curr_lr =  [9.880936454849499e-06]
Epoch: [0][126/500]	Time 44.563 (44.685)	Loss 11.1038 (10.3264)	VQALoss 0.6802 (0.9150)	VGLoss 10.4236 (9.4114)
curr_lr =  [9.880936454849499e-06]
curr_lr =  [9.879598662207359e-06]
curr_lr =  [9.879598662207359e-06]
curr_lr =  [9.878260869565218e-06]
curr_lr =  [9.878260869565218e-06]
curr_lr =  [9.876923076923077e-06]
curr_lr =  [9.876923076923077e-06]
[2024-02-22 23:52:16,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=22, lr=[9.876254180602007e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.875585284280937e-06]
[2024-02-22 23:52:16,246] [INFO] [timer.py:260:stop] epoch=0/micro_step=2600/global_step=130, RunningAvgSamplesPerSec=3.573017210617472, CurrSamplesPerSec=3.4658433062153575, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.875585284280937e-06]
curr_lr =  [9.874247491638797e-06]
Epoch: [0][131/500]	Time 44.757 (45.309)	Loss 9.5957 (10.8573)	VQALoss 1.2246 (0.8661)	VGLoss 8.3711 (9.9912)
curr_lr =  [9.874247491638797e-06]
curr_lr =  [9.872909698996656e-06]
curr_lr =  [9.872909698996656e-06]
curr_lr =  [9.871571906354516e-06]
curr_lr =  [9.871571906354516e-06]
curr_lr =  [9.870234113712376e-06]
curr_lr =  [9.870234113712376e-06]
curr_lr =  [9.868896321070234e-06]
curr_lr =  [9.868896321070234e-06]
curr_lr =  [9.867558528428094e-06]
Epoch: [0][136/500]	Time 45.150 (45.247)	Loss 4.7359 (10.4837)	VQALoss 0.6680 (0.8378)	VGLoss 4.0679 (9.6459)
curr_lr =  [9.867558528428094e-06]
curr_lr =  [9.866220735785954e-06]
curr_lr =  [9.866220735785954e-06]
curr_lr =  [9.864882943143812e-06]
curr_lr =  [9.864882943143812e-06]
curr_lr =  [9.863545150501674e-06]
curr_lr =  [9.863545150501674e-06]
[2024-02-22 23:59:48,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=22, lr=[9.862876254180604e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.862207357859532e-06]
[2024-02-22 23:59:48,346] [INFO] [timer.py:260:stop] epoch=0/micro_step=2800/global_step=140, RunningAvgSamplesPerSec=3.5705414092819807, CurrSamplesPerSec=3.5716269579576894, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.862207357859532e-06]
curr_lr =  Epoch: [0][141/500]	Time 44.708 (45.164)	Loss 14.2263 (10.0318)	VQALoss 0.8608 (0.7871)	VGLoss 13.3654 (9.2447)[9.860869565217392e-06]

curr_lr =  [9.860869565217392e-06]
curr_lr =  [9.859531772575252e-06]
curr_lr =  [9.859531772575252e-06]
curr_lr =  [9.858193979933112e-06]
curr_lr =  [9.858193979933112e-06]
curr_lr =  [9.85685618729097e-06]
curr_lr =  [9.85685618729097e-06]
curr_lr =  [9.85551839464883e-06]
curr_lr =  [9.85551839464883e-06]
curr_lr =  [9.85418060200669e-06]
Epoch: [0][146/500]	Time 44.964 (45.075)	Loss 10.8026 (9.8984)	VQALoss 0.8413 (0.7313)	VGLoss 9.9613 (9.1670)
curr_lr =  [9.85418060200669e-06]
curr_lr =  [9.852842809364548e-06]
curr_lr =  [9.852842809364548e-06]
curr_lr =  [9.85150501672241e-06]
curr_lr =  [9.85150501672241e-06]
curr_lr =  [9.850167224080268e-06]
curr_lr =  [9.850167224080268e-06]
[2024-02-23 00:07:19,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=22, lr=[9.849498327759198e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.848829431438128e-06]
[2024-02-23 00:07:19,975] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=150, RunningAvgSamplesPerSec=3.5686565668437806, CurrSamplesPerSec=3.5145386318802276, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.848829431438128e-06]
curr_lr =  [9.847491638795988e-06]
Epoch: [0][151/500]	Time 45.371 (45.383)	Loss 8.2310 (10.1589)	VQALoss 0.5811 (0.7215)	VGLoss 7.6499 (9.4374)
curr_lr =  [9.847491638795988e-06]
[2024-02-23 00:08:10,785] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step151 is about to be saved!
[2024-02-23 00:08:23,600] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step151/mp_rank_00_model_states.pt
[2024-02-23 00:08:23,601] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step151/mp_rank_00_model_states.pt...
[2024-02-23 00:10:40,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step151/mp_rank_00_model_states.pt.
[2024-02-23 00:10:42,371] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step151/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-23 00:10:42,371] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step151/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-23 00:10:56,231] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step151/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-23 00:10:56,232] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step151/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-23 00:10:56,232] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step151 is ready now!
[2024-02-23 00:10:56,817] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step151/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-23 00:10:56,826] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step151/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-23 00:10:56,826] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step151 is ready now!
curr_lr =  [9.846153846153848e-06]
curr_lr =  [9.846153846153848e-06]
curr_lr =  [9.844816053511706e-06]
curr_lr =  [9.844816053511706e-06]
curr_lr =  [9.843478260869566e-06]
curr_lr =  [9.843478260869566e-06]
curr_lr =  [9.842140468227426e-06]
curr_lr =  [9.842140468227426e-06]
curr_lr =  [9.840802675585284e-06]
Epoch: [0][156/500]	Time 45.563 (79.229)	Loss 12.6293 (9.3758)	VQALoss 0.5918 (0.6834)	VGLoss 12.0375 (8.6924)
curr_lr =  [9.840802675585284e-06]
curr_lr =  [9.839464882943146e-06]
curr_lr =  [9.839464882943146e-06]
curr_lr =  [9.838127090301004e-06]
curr_lr =  [9.838127090301004e-06]
curr_lr =  [9.836789297658864e-06]
curr_lr =  [9.836789297658864e-06]
[2024-02-23 00:17:42,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=22, lr=[9.836120401337794e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.835451505016724e-06]
[2024-02-23 00:17:42,367] [INFO] [timer.py:260:stop] epoch=0/micro_step=3200/global_step=160, RunningAvgSamplesPerSec=3.5675704634403473, CurrSamplesPerSec=3.5964192005948155, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.835451505016724e-06]
curr_lr =  [9.834113712374582e-06]
Epoch: [0][161/500]	Time 45.100 (45.195)	Loss 6.6471 (9.1707)	VQALoss 0.7046 (0.6686)	VGLoss 5.9425 (8.5021)
curr_lr =  [9.834113712374582e-06]
curr_lr =  [9.832775919732442e-06]
curr_lr =  [9.832775919732442e-06]
curr_lr =  [9.831438127090302e-06]
curr_lr =  [9.831438127090302e-06]
curr_lr =  [9.830100334448162e-06]
curr_lr =  [9.830100334448162e-06]
curr_lr =  [9.82876254180602e-06]
curr_lr =  [9.82876254180602e-06]
Epoch: [0][166/500]	Time 44.471 (45.171)	Loss 29.3749 (9.8627)	VQALoss 0.7588 (0.6549)	VGLoss 28.6161 (9.2078)curr_lr = 
 [9.827424749163881e-06]
curr_lr =  [9.827424749163881e-06]
curr_lr =  [9.82608695652174e-06]
curr_lr =  [9.82608695652174e-06]
curr_lr =  [9.8247491638796e-06]
curr_lr =  [9.8247491638796e-06]
curr_lr =  [9.82341137123746e-06]
curr_lr =  [9.82341137123746e-06]
[2024-02-23 00:25:13,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=22, lr=[9.82274247491639e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.822073578595318e-06]
[2024-02-23 00:25:13,662] [INFO] [timer.py:260:stop] epoch=0/micro_step=3400/global_step=170, RunningAvgSamplesPerSec=3.566246617461625, CurrSamplesPerSec=3.5672370937106774, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.822073578595318e-06]
curr_lr = Epoch: [0][171/500]	Time 44.865 (45.041)	Loss 7.4380 (9.2758)	VQALoss 0.6880 (0.6322)	VGLoss 6.7500 (8.6436) 
[9.820735785953178e-06]
curr_lr =  [9.820735785953178e-06]
curr_lr =  [9.819397993311037e-06]
curr_lr =  [9.819397993311037e-06]
curr_lr =  [9.818060200668897e-06]
curr_lr =  [9.818060200668897e-06]
curr_lr =  [9.816722408026757e-06]
curr_lr =  [9.816722408026757e-06]
curr_lr =  [9.815384615384617e-06]
curr_lr =  [9.815384615384617e-06]
curr_lr =  [9.814046822742475e-06]
Epoch: [0][176/500]	Time 46.158 (45.222)	Loss 6.0452 (8.8679)	VQALoss 0.4053 (0.6074)	VGLoss 5.6400 (8.2605)
curr_lr =  [9.814046822742475e-06]
curr_lr =  [9.812709030100335e-06]
curr_lr =  [9.812709030100335e-06]
curr_lr =  [9.811371237458195e-06]
curr_lr =  [9.811371237458195e-06]
curr_lr =  [9.810033444816053e-06]
curr_lr =  [9.810033444816053e-06]
[2024-02-23 00:32:46,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=22, lr=[9.809364548494983e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.808695652173913e-06]
[2024-02-23 00:32:46,443] [INFO] [timer.py:260:stop] epoch=0/micro_step=3600/global_step=180, RunningAvgSamplesPerSec=3.564409678569515, CurrSamplesPerSec=3.5519339107023513, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.808695652173913e-06]
curr_lr =  [9.807357859531773e-06]
Epoch: [0][181/500]	Time 44.396 (45.240)	Loss 8.5690 (9.0744)	VQALoss 0.4448 (0.5965)	VGLoss 8.1241 (8.4779)
curr_lr =  [9.807357859531773e-06]
curr_lr =  [9.806020066889633e-06]
curr_lr =  [9.806020066889633e-06]
curr_lr =  [9.804682274247493e-06]
curr_lr =  [9.804682274247493e-06]
curr_lr =  [9.803344481605353e-06]
curr_lr =  [9.803344481605353e-06]
curr_lr =  [9.802006688963211e-06]
curr_lr =  [9.802006688963211e-06]
curr_lr =  Epoch: [0][186/500]	Time 45.826 (45.442)	Loss 4.2430 (9.5374)	VQALoss 0.5713 (0.5819)	VGLoss 3.6717 (8.9555)[9.800668896321071e-06]

curr_lr =  [9.800668896321071e-06]
curr_lr =  [9.799331103678931e-06]
curr_lr =  [9.799331103678931e-06]
curr_lr =  [9.79799331103679e-06]
curr_lr =  [9.79799331103679e-06]
curr_lr =  [9.796655518394649e-06]
curr_lr =  [9.796655518394649e-06]
[2024-02-23 00:40:19,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=22, lr=[9.795986622073579e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.795317725752509e-06]
[2024-02-23 00:40:20,061] [INFO] [timer.py:260:stop] epoch=0/micro_step=3800/global_step=190, RunningAvgSamplesPerSec=3.562415965580826, CurrSamplesPerSec=3.516275367302819, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.795317725752509e-06]
curr_lr =  [9.793979933110369e-06]
Epoch: [0][191/500]	Time 45.979 (45.598)	Loss 3.5849 (8.7254)	VQALoss 0.5259 (0.5903)	VGLoss 3.0590 (8.1352)
curr_lr =  [9.793979933110369e-06]
curr_lr =  [9.792642140468229e-06]
curr_lr =  [9.792642140468229e-06]
curr_lr =  [9.791304347826089e-06]
curr_lr =  [9.791304347826089e-06]
curr_lr =  [9.789966555183947e-06]
curr_lr =  [9.789966555183947e-06]
curr_lr =  [9.788628762541807e-06]
curr_lr =  [9.788628762541807e-06]
curr_lr =  [9.787290969899667e-06]
Epoch: [0][196/500]	Time 45.272 (45.113)	Loss 4.3082 (8.3867)	VQALoss 0.4260 (0.5128)	VGLoss 3.8822 (7.8739)
curr_lr =  [9.787290969899667e-06]
curr_lr =  [9.785953177257525e-06]
curr_lr =  [9.785953177257525e-06]
curr_lr =  [9.784615384615387e-06]
curr_lr =  [9.784615384615387e-06]
curr_lr =  [9.783277591973245e-06]
curr_lr =  [9.783277591973245e-06]
[2024-02-23 00:47:51,409] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=22, lr=[9.782608695652175e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.781939799331105e-06]
[2024-02-23 00:47:51,556] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=200, RunningAvgSamplesPerSec=3.561475626415815, CurrSamplesPerSec=3.559502331884779, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.781939799331105e-06]
curr_lr =  [9.780602006688965e-06]
Epoch: [0][201/500]	Time 44.999 (44.989)	Loss 9.7581 (9.3560)	VQALoss 0.3894 (0.5308)	VGLoss 9.3687 (8.8251)
curr_lr =  [9.780602006688965e-06]
[2024-02-23 00:48:40,904] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step201 is about to be saved!
[2024-02-23 00:48:53,275] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step201/mp_rank_00_model_states.pt
[2024-02-23 00:48:53,275] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step201/mp_rank_00_model_states.pt...
[2024-02-23 00:51:20,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step201/mp_rank_00_model_states.pt.
[2024-02-23 00:51:22,473] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step201/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-23 00:51:22,473] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step201/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-23 00:51:35,510] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step201/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-23 00:51:35,514] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step201/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-23 00:51:35,515] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step201 is ready now!
[2024-02-23 00:51:35,521] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step201/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-23 00:51:35,521] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step201/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-23 00:51:35,521] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step201 is ready now!
curr_lr =  [9.779264214046823e-06]
curr_lr =  [9.779264214046823e-06]
curr_lr =  [9.777926421404683e-06]
curr_lr =  [9.777926421404683e-06]
curr_lr =  [9.776588628762543e-06]
curr_lr =  [9.776588628762543e-06]
curr_lr =  [9.775250836120403e-06]
curr_lr =  [9.775250836120403e-06]
curr_lr =  [9.77391304347826e-06]
Epoch: [0][206/500]	Time 45.704 (81.322)	Loss 9.4371 (8.1239)	VQALoss 0.4460 (0.5219)	VGLoss 8.9911 (7.6020)
curr_lr =  [9.77391304347826e-06]
curr_lr =  [9.772575250836122e-06]
curr_lr =  [9.772575250836122e-06]
curr_lr =  [9.77123745819398e-06]
curr_lr =  [9.77123745819398e-06]
curr_lr =  [9.76989966555184e-06]
curr_lr =  [9.76989966555184e-06]
[2024-02-23 00:58:24,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=22, lr=[9.76923076923077e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.7685618729097e-06]
[2024-02-23 00:58:24,476] [INFO] [timer.py:260:stop] epoch=0/micro_step=4200/global_step=210, RunningAvgSamplesPerSec=3.5596895451239825, CurrSamplesPerSec=3.540270957860709, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.7685618729097e-06]
curr_lr =  Epoch: [0][211/500]	Time 45.186 (45.300)	Loss 11.0943 (8.7349)	VQALoss 0.3093 (0.4968)	VGLoss 10.7850 (8.2382)[9.767224080267559e-06]

curr_lr =  [9.767224080267559e-06]
curr_lr =  [9.765886287625419e-06]
curr_lr =  [9.765886287625419e-06]
curr_lr =  [9.764548494983278e-06]
curr_lr =  [9.764548494983278e-06]
curr_lr =  [9.763210702341138e-06]
curr_lr =  [9.763210702341138e-06]
curr_lr =  [9.761872909698997e-06]
curr_lr =  [9.761872909698997e-06]
curr_lr =  [9.760535117056858e-06]
Epoch: [0][216/500]	Time 45.317 (45.284)	Loss 3.0395 (8.7130)	VQALoss 0.3281 (0.5054)	VGLoss 2.7114 (8.2076)
curr_lr =  [9.760535117056858e-06]
curr_lr =  [9.759197324414716e-06]
curr_lr =  [9.759197324414716e-06]
curr_lr =  [9.757859531772576e-06]
curr_lr =  [9.757859531772576e-06]
curr_lr =  [9.756521739130436e-06]
curr_lr =  [9.756521739130436e-06]
[2024-02-23 01:05:56,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=22, lr=[9.755852842809366e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.755183946488294e-06]
[2024-02-23 01:05:56,766] [INFO] [timer.py:260:stop] epoch=0/micro_step=4400/global_step=220, RunningAvgSamplesPerSec=3.558819447497388, CurrSamplesPerSec=3.57169814693853, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.755183946488294e-06]
curr_lr =  [9.753846153846154e-06]
Epoch: [0][221/500]	Time 45.866 (45.310)	Loss 7.8697 (8.4193)	VQALoss 0.5479 (0.5045)	VGLoss 7.3218 (7.9148)
curr_lr =  [9.753846153846154e-06]
curr_lr =  [9.752508361204014e-06]
curr_lr =  [9.752508361204014e-06]
curr_lr =  [9.751170568561874e-06]
curr_lr =  [9.751170568561874e-06]
curr_lr =  [9.749832775919732e-06]
curr_lr =  [9.749832775919732e-06]
curr_lr =  [9.748494983277594e-06]
curr_lr =  [9.748494983277594e-06]
curr_lr =  Epoch: [0][226/500]	Time 44.925 (45.245)	Loss 2.8215 (8.3414)	VQALoss 0.4060 (0.4832)	VGLoss 2.4155 (7.8582)
[9.747157190635452e-06]
curr_lr =  [9.747157190635452e-06]
curr_lr =  [9.745819397993312e-06]
curr_lr =  [9.745819397993312e-06]
curr_lr =  [9.744481605351172e-06]
curr_lr =  [9.744481605351172e-06]
curr_lr =  [9.74314381270903e-06]
curr_lr =  [9.74314381270903e-06]
[2024-02-23 01:13:28,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=22, lr=[9.74247491638796e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.74180602006689e-06]
[2024-02-23 01:13:28,615] [INFO] [timer.py:260:stop] epoch=0/micro_step=4600/global_step=230, RunningAvgSamplesPerSec=3.5580394676740283, CurrSamplesPerSec=3.5357846962662642, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.74180602006689e-06]
curr_lr =  [9.74046822742475e-06]
Epoch: [0][231/500]	Time 45.054 (44.962)	Loss 8.2578 (8.8522)	VQALoss 0.8203 (0.4663)	VGLoss 7.4375 (8.3860)
curr_lr =  [9.74046822742475e-06]
curr_lr =  [9.73913043478261e-06]
curr_lr =  [9.73913043478261e-06]
curr_lr =  [9.737792642140468e-06]
curr_lr =  [9.737792642140468e-06]
curr_lr =  [9.73645484949833e-06]
curr_lr =  [9.73645484949833e-06]
curr_lr =  [9.735117056856188e-06]
curr_lr =  [9.735117056856188e-06]
curr_lr =  [9.733779264214048e-06]
Epoch: [0][236/500]	Time 45.382 (45.257)	Loss 4.1907 (8.4791)	VQALoss 0.2507 (0.4531)	VGLoss 3.9399 (8.0260)
curr_lr =  [9.733779264214048e-06]
curr_lr =  [9.732441471571908e-06]
curr_lr =  [9.732441471571908e-06]
curr_lr =  [9.731103678929766e-06]
curr_lr =  [9.731103678929766e-06]
curr_lr =  [9.729765886287626e-06]
curr_lr =  [9.729765886287626e-06]
[2024-02-23 01:21:02,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=22, lr=[9.729096989966556e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.728428093645486e-06]
[2024-02-23 01:21:02,459] [INFO] [timer.py:260:stop] epoch=0/micro_step=4800/global_step=240, RunningAvgSamplesPerSec=3.556662053008781, CurrSamplesPerSec=3.463603734192408, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.728428093645486e-06]
curr_lr =  [9.727090301003346e-06]
Epoch: [0][241/500]	Time 44.879 (45.476)	Loss 3.6759 (8.6082)	VQALoss 0.3308 (0.4515)	VGLoss 3.3451 (8.1567)
curr_lr =  [9.727090301003346e-06]
curr_lr =  [9.725752508361206e-06]
curr_lr =  [9.725752508361206e-06]
curr_lr =  [9.724414715719064e-06]
curr_lr =  [9.724414715719064e-06]
curr_lr =  [9.723076923076924e-06]
curr_lr =  [9.723076923076924e-06]
curr_lr =  [9.721739130434784e-06]
curr_lr =  [9.721739130434784e-06]
curr_lr =  [9.720401337792643e-06]
Epoch: [0][246/500]	Time 45.153 (45.082)	Loss 5.1266 (8.5731)	VQALoss 0.5005 (0.4464)	VGLoss 4.6261 (8.1266)
curr_lr =  [9.720401337792643e-06]
curr_lr =  [9.719063545150502e-06]
curr_lr =  [9.719063545150502e-06]
curr_lr =  [9.717725752508362e-06]
curr_lr =  [9.717725752508362e-06]
curr_lr =  [9.716387959866222e-06]
curr_lr =  [9.716387959866222e-06]
[2024-02-23 01:28:31,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=22, lr=[9.715719063545151e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.715050167224081e-06]
[2024-02-23 01:28:31,960] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=250, RunningAvgSamplesPerSec=3.5567808434530317, CurrSamplesPerSec=3.6280097887534324, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.715050167224081e-06]
curr_lr =  Epoch: [0][251/500]	Time 45.018 (44.846)	Loss 8.1352 (8.6864)	VQALoss 0.4387 (0.4276)	VGLoss 7.6964 (8.2588)[9.713712374581941e-06]

curr_lr =  [9.713712374581941e-06]
[2024-02-23 01:29:22,117] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step251 is about to be saved!
[2024-02-23 01:29:34,374] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step251/mp_rank_00_model_states.pt
[2024-02-23 01:29:34,374] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step251/mp_rank_00_model_states.pt...
[2024-02-23 01:31:53,300] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step251/mp_rank_00_model_states.pt.
[2024-02-23 01:31:54,968] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step251/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-23 01:31:54,968] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step251/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-23 01:32:09,120] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step251/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-23 01:32:09,146] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step251/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-23 01:32:09,146] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step251 is ready now!
[2024-02-23 01:32:09,231] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step251/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-23 01:32:09,232] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step251/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-23 01:32:09,233] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step251 is ready now!
curr_lr =  [9.7123745819398e-06]
curr_lr =  [9.7123745819398e-06]
curr_lr =  [9.71103678929766e-06]
curr_lr =  [9.71103678929766e-06]
curr_lr =  [9.70969899665552e-06]
curr_lr =  [9.70969899665552e-06]
curr_lr =  [9.70836120401338e-06]
curr_lr =  [9.70836120401338e-06]
Epoch: [0][256/500]	Time 44.506 (79.656)	Loss 29.2546 (9.2483)	VQALoss 0.1788 (0.4144)	VGLoss 29.0758 (8.8339)
curr_lr =  [9.707023411371237e-06]
curr_lr =  [9.707023411371237e-06]
curr_lr =  [9.705685618729097e-06]
curr_lr =  [9.705685618729097e-06]
curr_lr =  [9.704347826086957e-06]
curr_lr =  [9.704347826086957e-06]
curr_lr =  [9.703010033444817e-06]
curr_lr =  [9.703010033444817e-06]
[2024-02-23 01:38:55,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=22, lr=[9.702341137123747e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.701672240802677e-06]
[2024-02-23 01:38:55,235] [INFO] [timer.py:260:stop] epoch=0/micro_step=5200/global_step=260, RunningAvgSamplesPerSec=3.556424917690194, CurrSamplesPerSec=3.5649370463745336, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.701672240802677e-06]
curr_lr = Epoch: [0][261/500]	Time 45.026 (45.000)	Loss 6.4381 (8.3895)	VQALoss 0.5400 (0.4215)	VGLoss 5.8981 (7.9679) 
[9.700334448160535e-06]
curr_lr =  [9.700334448160535e-06]
curr_lr =  [9.698996655518395e-06]
curr_lr =  [9.698996655518395e-06]
curr_lr =  [9.697658862876255e-06]
curr_lr =  [9.697658862876255e-06]
curr_lr =  [9.696321070234115e-06]
curr_lr =  [9.696321070234115e-06]
curr_lr =  [9.694983277591973e-06]
curr_lr =  [9.694983277591973e-06]
curr_lr =  [9.693645484949835e-06]
Epoch: [0][266/500]	Time 45.922 (45.300)	Loss 32.6668 (8.6457)	VQALoss 0.4441 (0.4122)	VGLoss 32.2227 (8.2336)
curr_lr =  [9.693645484949835e-06]
curr_lr =  [9.692307692307693e-06]
curr_lr =  [9.692307692307693e-06]
curr_lr =  [9.690969899665551e-06]
curr_lr =  [9.690969899665551e-06]
curr_lr =  [9.689632107023413e-06]
curr_lr =  [9.689632107023413e-06]
[2024-02-23 01:46:27,903] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=22, lr=[9.688963210702343e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.688294314381271e-06]
[2024-02-23 01:46:28,052] [INFO] [timer.py:260:stop] epoch=0/micro_step=5400/global_step=270, RunningAvgSamplesPerSec=3.5556788701497895, CurrSamplesPerSec=3.494842816718513, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.688294314381271e-06]
curr_lr = Epoch: [0][271/500]	Time 45.401 (45.338)	Loss 3.3184 (9.4072)	VQALoss 0.4458 (0.4103)	VGLoss 2.8726 (8.9969) 
[9.686956521739131e-06]
curr_lr =  [9.686956521739131e-06]
curr_lr =  [9.685618729096991e-06]
curr_lr =  [9.685618729096991e-06]
curr_lr =  [9.68428093645485e-06]
curr_lr =  [9.68428093645485e-06]
curr_lr =  [9.682943143812709e-06]
curr_lr =  [9.682943143812709e-06]
curr_lr =  [9.68160535117057e-06]
curr_lr =  [9.68160535117057e-06]
curr_lr =  [9.680267558528429e-06]
Epoch: [0][276/500]	Time 45.055 (44.956)	Loss 3.6756 (7.6295)	VQALoss 0.5044 (0.4038)	VGLoss 3.1712 (7.2258)
curr_lr =  [9.680267558528429e-06]
curr_lr =  [9.678929765886289e-06]
curr_lr =  [9.678929765886289e-06]
curr_lr =  [9.677591973244149e-06]
curr_lr =  [9.677591973244149e-06]
curr_lr =  [9.676254180602007e-06]
curr_lr =  [9.676254180602007e-06]
[2024-02-23 01:53:56,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=22, lr=[9.675585284280937e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.674916387959867e-06]
[2024-02-23 01:53:56,424] [INFO] [timer.py:260:stop] epoch=0/micro_step=5600/global_step=280, RunningAvgSamplesPerSec=3.5561409547570837, CurrSamplesPerSec=3.564984485574044, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.674916387959867e-06]
curr_lr =  [9.673578595317727e-06]
Epoch: [0][281/500]	Time 45.226 (44.684)	Loss 8.5328 (8.5986)	VQALoss 0.6587 (0.3879)	VGLoss 7.8741 (8.2107)
curr_lr =  [9.673578595317727e-06]
curr_lr =  [9.672240802675587e-06]
curr_lr =  [9.672240802675587e-06]
curr_lr =  [9.670903010033445e-06]
curr_lr =  [9.670903010033445e-06]
curr_lr =  [9.669565217391305e-06]
curr_lr =  [9.669565217391305e-06]
curr_lr =  [9.668227424749165e-06]
curr_lr =  [9.668227424749165e-06]
curr_lr =  [9.666889632107025e-06]
Epoch: [0][286/500]	Time 44.720 (44.803)	Loss 5.4185 (8.3867)	VQALoss 0.2439 (0.3969)	VGLoss 5.1746 (7.9898)
curr_lr =  [9.666889632107025e-06]
curr_lr =  [9.665551839464884e-06]
curr_lr =  [9.665551839464884e-06]
curr_lr =  [9.664214046822743e-06]
curr_lr =  [9.664214046822743e-06]
curr_lr =  [9.662876254180603e-06]
curr_lr =  [9.662876254180603e-06]
[2024-02-23 02:01:25,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=22, lr=[9.662207357859533e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.661538461538462e-06]
[2024-02-23 02:01:25,770] [INFO] [timer.py:260:stop] epoch=0/micro_step=5800/global_step=290, RunningAvgSamplesPerSec=3.5563037968997273, CurrSamplesPerSec=3.569594944699916, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.661538461538462e-06]
Epoch: [0][291/500]	Time 44.472 (44.916)	Loss 6.3635 (8.1653)	VQALoss 0.3323 (0.3788)	VGLoss 6.0312 (7.7865)curr_lr = 
 [9.660200668896322e-06]
curr_lr =  [9.660200668896322e-06]
curr_lr =  [9.65886287625418e-06]
curr_lr =  [9.65886287625418e-06]
curr_lr =  [9.65752508361204e-06]
curr_lr =  [9.65752508361204e-06]
curr_lr =  [9.6561872909699e-06]
curr_lr =  [9.6561872909699e-06]
curr_lr =  [9.65484949832776e-06]
curr_lr =  [9.65484949832776e-06]
curr_lr =  [9.65351170568562e-06]
Epoch: [0][296/500]	Time 44.865 (44.698)	Loss 8.5804 (8.2401)	VQALoss 0.5181 (0.3751)	VGLoss 8.0623 (7.8650)
curr_lr =  [9.65351170568562e-06]
curr_lr =  [9.652173913043478e-06]
curr_lr =  [9.652173913043478e-06]
curr_lr =  [9.650836120401338e-06]
curr_lr =  [9.650836120401338e-06]
curr_lr =  [9.649498327759198e-06]
curr_lr =  [9.649498327759198e-06]
[2024-02-23 02:08:52,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=22, lr=[9.648829431438128e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.648160535117058e-06]
[2024-02-23 02:08:52,752] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=300, RunningAvgSamplesPerSec=3.557082789651814, CurrSamplesPerSec=3.538113188829867, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.648160535117058e-06]
curr_lr =  Epoch: [0][301/500]	Time 44.468 (44.697)	Loss 15.3049 (8.3757)	VQALoss 0.2681 (0.3717)	VGLoss 15.0368 (8.0041)[9.646822742474918e-06]

curr_lr =  [9.646822742474918e-06]
[2024-02-23 02:09:41,995] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step301 is about to be saved!
[2024-02-23 02:09:54,182] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step301/mp_rank_00_model_states.pt
[2024-02-23 02:09:54,182] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step301/mp_rank_00_model_states.pt...
[2024-02-23 02:12:19,480] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step301/mp_rank_00_model_states.pt.
[2024-02-23 02:12:21,281] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step301/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-23 02:12:21,281] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step301/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-23 02:12:34,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step301/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-23 02:12:34,594] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step301/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-23 02:12:34,594] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step301 is ready now!
[2024-02-23 02:12:34,876] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step301/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-23 02:12:34,881] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step301/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-23 02:12:34,881] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step301 is ready now!
curr_lr =  [9.645484949832776e-06]
curr_lr =  [9.645484949832776e-06]
curr_lr =  [9.644147157190636e-06]
curr_lr =  [9.644147157190636e-06]
curr_lr =  [9.642809364548496e-06]
curr_lr =  [9.642809364548496e-06]
curr_lr =  [9.641471571906356e-06]
curr_lr =  [9.641471571906356e-06]
Epoch: [0][306/500]	Time 44.744 (79.923)	Loss 8.9589 (8.8699)	VQALoss 0.4004 (0.3501)	VGLoss 8.5585 (8.5198)curr_lr = 
 [9.640133779264214e-06]
curr_lr =  [9.640133779264214e-06]
curr_lr =  [9.638795986622074e-06]
curr_lr =  [9.638795986622074e-06]
curr_lr =  [9.637458193979934e-06]
curr_lr =  [9.637458193979934e-06]
curr_lr =  [9.636120401337792e-06]
curr_lr =  [9.636120401337792e-06]
[2024-02-23 02:19:15,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=22, lr=[9.635451505016722e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.634782608695654e-06]
[2024-02-23 02:19:15,599] [INFO] [timer.py:260:stop] epoch=0/micro_step=6200/global_step=310, RunningAvgSamplesPerSec=3.5582737081389118, CurrSamplesPerSec=3.5760414361885555, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.634782608695654e-06]
curr_lr =  [9.633444816053512e-06]
Epoch: [0][311/500]	Time 44.553 (44.664)	Loss 10.1443 (8.5968)	VQALoss 0.2450 (0.3607)	VGLoss 9.8993 (8.2361)
curr_lr =  [9.633444816053512e-06]
curr_lr =  [9.632107023411372e-06]
curr_lr =  [9.632107023411372e-06]
curr_lr =  [9.630769230769232e-06]
curr_lr =  [9.630769230769232e-06]
curr_lr =  [9.629431438127092e-06]
curr_lr =  [9.629431438127092e-06]
curr_lr =  [9.62809364548495e-06]
curr_lr =  [9.62809364548495e-06]
Epoch: [0][316/500]	Time 44.697 (44.963)	Loss 6.5515 (8.6610)	VQALoss 0.4065 (0.3520)	VGLoss 6.1450 (8.3090)curr_lr = 
 [9.62675585284281e-06]
curr_lr =  [9.62675585284281e-06]
curr_lr =  [9.62541806020067e-06]
curr_lr =  [9.62541806020067e-06]
curr_lr =  [9.624080267558528e-06]
curr_lr =  [9.624080267558528e-06]
curr_lr =  [9.62274247491639e-06]
curr_lr =  [9.62274247491639e-06]
[2024-02-23 02:26:42,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=22, lr=[9.62207357859532e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.621404682274248e-06]
[2024-02-23 02:26:42,846] [INFO] [timer.py:260:stop] epoch=0/micro_step=6400/global_step=320, RunningAvgSamplesPerSec=3.5589680882314867, CurrSamplesPerSec=3.58236176206776, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.621404682274248e-06]
curr_lr =  [9.620066889632108e-06]
Epoch: [0][321/500]	Time 44.183 (44.412)	Loss 4.1030 (7.5870)	VQALoss 0.3491 (0.3541)	VGLoss 3.7539 (7.2329)
curr_lr =  [9.620066889632108e-06]
curr_lr =  [9.618729096989968e-06]
curr_lr =  [9.618729096989968e-06]
curr_lr =  [9.617391304347828e-06]
curr_lr =  [9.617391304347828e-06]
curr_lr =  [9.616053511705686e-06]
curr_lr =  [9.616053511705686e-06]
curr_lr =  [9.614715719063546e-06]
curr_lr =  [9.614715719063546e-06]
curr_lr =  [9.613377926421406e-06]
Epoch: [0][326/500]	Time 44.706 (44.765)	Loss 4.0707 (8.6289)	VQALoss 0.2759 (0.3446)	VGLoss 3.7948 (8.2843)
curr_lr =  [9.613377926421406e-06]
curr_lr =  [9.612040133779264e-06]
curr_lr =  [9.612040133779264e-06]
curr_lr =  [9.610702341137125e-06]
curr_lr =  [9.610702341137125e-06]
curr_lr =  [9.609364548494984e-06]
curr_lr =  [9.609364548494984e-06]
[2024-02-23 02:34:09,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=22, lr=[9.608695652173914e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.608026755852844e-06]
[2024-02-23 02:34:09,508] [INFO] [timer.py:260:stop] epoch=0/micro_step=6600/global_step=330, RunningAvgSamplesPerSec=3.5596727881349137, CurrSamplesPerSec=3.589866239993687, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.608026755852844e-06]
curr_lr =  Epoch: [0][331/500]	Time 45.003 (44.731)	Loss 9.2638 (8.2255)	VQALoss 0.2703 (0.3420)	VGLoss 8.9935 (7.8835)[9.606688963210703e-06]

curr_lr =  [9.606688963210703e-06]
curr_lr =  [9.605351170568563e-06]
curr_lr =  [9.605351170568563e-06]
curr_lr =  [9.604013377926422e-06]
curr_lr =  [9.604013377926422e-06]
curr_lr =  [9.602675585284281e-06]
curr_lr =  [9.602675585284281e-06]
curr_lr =  [9.601337792642141e-06]
curr_lr =  [9.601337792642141e-06]
curr_lr =  [9.600000000000001e-06]Epoch: [0][336/500]	Time 45.021 (44.697)	Loss 14.1104 (7.8320)	VQALoss 0.2101 (0.3287)	VGLoss 13.9003 (7.5033)

curr_lr =  [9.600000000000001e-06]
curr_lr =  [9.598662207357861e-06]
curr_lr =  [9.598662207357861e-06]
curr_lr =  [9.59732441471572e-06]
curr_lr =  [9.59732441471572e-06]
curr_lr =  [9.59598662207358e-06]
curr_lr =  [9.59598662207358e-06]
[2024-02-23 02:41:36,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=22, lr=[9.59531772575251e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.59464882943144e-06]
[2024-02-23 02:41:36,441] [INFO] [timer.py:260:stop] epoch=0/micro_step=6800/global_step=340, RunningAvgSamplesPerSec=3.5602726984447357, CurrSamplesPerSec=3.577532519286884, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.59464882943144e-06]
curr_lr =  [9.593311036789299e-06]
Epoch: [0][341/500]	Time 44.713 (44.632)	Loss 8.5042 (8.1439)	VQALoss 0.4700 (0.3293)	VGLoss 8.0342 (7.8146)
curr_lr =  [9.593311036789299e-06]
curr_lr =  [9.591973244147157e-06]
curr_lr =  [9.591973244147157e-06]
curr_lr =  [9.590635451505017e-06]
curr_lr =  [9.590635451505017e-06]
curr_lr =  [9.589297658862877e-06]
curr_lr =  [9.589297658862877e-06]
curr_lr =  [9.587959866220737e-06]
curr_lr =  [9.587959866220737e-06]
curr_lr =  [9.586622073578597e-06]
Epoch: [0][346/500]	Time 44.555 (44.740)	Loss 2.9159 (7.0682)	VQALoss 0.2441 (0.3195)	VGLoss 2.6717 (6.7487)
curr_lr =  [9.586622073578597e-06]
curr_lr =  [9.585284280936455e-06]
curr_lr =  [9.585284280936455e-06]
curr_lr =  [9.583946488294315e-06]
curr_lr =  [9.583946488294315e-06]
curr_lr =  [9.582608695652175e-06]
curr_lr =  [9.582608695652175e-06]
[2024-02-23 02:49:03,231] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=22, lr=[9.581939799331105e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.581270903010033e-06]
[2024-02-23 02:49:03,382] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=350, RunningAvgSamplesPerSec=3.5608364515347053, CurrSamplesPerSec=3.5908726039301655, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.581270903010033e-06]
curr_lr =  [9.579933110367893e-06]
Epoch: [0][351/500]	Time 45.272 (44.760)	Loss 7.5532 (7.1896)	VQALoss 0.4810 (0.3271)	VGLoss 7.0722 (6.8625)
curr_lr =  [9.579933110367893e-06]
[2024-02-23 02:49:54,168] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step351 is about to be saved!
[2024-02-23 02:50:06,567] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step351/mp_rank_00_model_states.pt
[2024-02-23 02:50:06,567] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step351/mp_rank_00_model_states.pt...
[2024-02-23 02:52:38,723] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step351/mp_rank_00_model_states.pt.
[2024-02-23 02:52:40,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step351/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-23 02:52:40,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step351/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-23 02:52:54,119] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step351/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-23 02:52:54,119] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step351/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-23 02:52:54,120] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step351 is ready now!
[2024-02-23 02:52:55,258] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step351/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-23 02:52:55,267] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step351/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-23 02:52:55,267] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step351 is ready now!
curr_lr =  [9.578595317725753e-06]
curr_lr =  [9.578595317725753e-06]
curr_lr =  [9.577257525083613e-06]
curr_lr =  [9.577257525083613e-06]
curr_lr =  [9.575919732441473e-06]
curr_lr =  [9.575919732441473e-06]
curr_lr =  [9.574581939799333e-06]
curr_lr =  [9.574581939799333e-06]
curr_lr =  [9.573244147157191e-06]
Epoch: [0][356/500]	Time 45.904 (82.879)	Loss 6.8882 (7.2738)	VQALoss 0.2522 (0.3204)	VGLoss 6.6360 (6.9534)
curr_lr =  [9.573244147157191e-06]
curr_lr =  [9.57190635451505e-06]
curr_lr =  [9.57190635451505e-06]
curr_lr =  [9.57056856187291e-06]
curr_lr =  [9.57056856187291e-06]
curr_lr =  [9.569230769230769e-06]
curr_lr =  [9.569230769230769e-06]
[2024-02-23 02:59:44,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=22, lr=[9.568561872909699e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.567892976588629e-06]
[2024-02-23 02:59:45,059] [INFO] [timer.py:260:stop] epoch=0/micro_step=7200/global_step=360, RunningAvgSamplesPerSec=3.5595722665128977, CurrSamplesPerSec=3.512965835151044, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.567892976588629e-06]
curr_lr =  [9.566555183946489e-06]
Epoch: [0][361/500]	Time 44.982 (45.399)	Loss 8.2723 (7.3105)	VQALoss 0.2030 (0.3231)	VGLoss 8.0693 (6.9874)
curr_lr =  [9.566555183946489e-06]
curr_lr =  [9.565217391304349e-06]
curr_lr =  [9.565217391304349e-06]
curr_lr =  [9.563879598662209e-06]
curr_lr =  [9.563879598662209e-06]
curr_lr =  [9.562541806020068e-06]
curr_lr =  [9.562541806020068e-06]
curr_lr =  [9.561204013377927e-06]
curr_lr =  [9.561204013377927e-06]
curr_lr =  [9.559866220735787e-06]
Epoch: [0][366/500]	Time 44.950 (44.526)	Loss 5.1593 (7.5197)	VQALoss 0.4153 (0.3140)	VGLoss 4.7440 (7.2057)
curr_lr =  [9.559866220735787e-06]
curr_lr =  [9.558528428093647e-06]
curr_lr =  [9.558528428093647e-06]
curr_lr =  [9.557190635451505e-06]
curr_lr =  [9.557190635451505e-06]
curr_lr =  [9.555852842809366e-06]
curr_lr =  [9.555852842809366e-06]
[2024-02-23 03:07:10,109] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=22, lr=[9.555183946488296e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.554515050167225e-06]
[2024-02-23 03:07:10,257] [INFO] [timer.py:260:stop] epoch=0/micro_step=7400/global_step=370, RunningAvgSamplesPerSec=3.560499600226627, CurrSamplesPerSec=3.590008139040892, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.554515050167225e-06]
curr_lr =  [9.553177257525084e-06]
Epoch: [0][371/500]	Time 43.789 (44.275)	Loss 2.5414 (8.0707)	VQALoss 0.2097 (0.3285)	VGLoss 2.3317 (7.7421)
curr_lr =  [9.553177257525084e-06]
curr_lr =  [9.551839464882944e-06]
curr_lr =  [9.551839464882944e-06]
curr_lr =  [9.550501672240804e-06]
curr_lr =  [9.550501672240804e-06]
curr_lr =  [9.549163879598662e-06]
curr_lr =  [9.549163879598662e-06]
[2024-02-23 03:10:53,098] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512
curr_lr =  [9.548494983277592e-06]
curr_lr =  [9.548494983277592e-06]
curr_lr =  [9.547157190635452e-06]
Epoch: [0][376/500]	Time 44.862 (44.783)	Loss 5.3863 (7.0147)	VQALoss 0.2316 (0.3263)	VGLoss 5.1547 (6.6885)
curr_lr =  [9.547157190635452e-06]
curr_lr =  [9.545819397993312e-06]
curr_lr =  [9.545819397993312e-06]
curr_lr =  [9.54448160535117e-06]
curr_lr =  [9.54448160535117e-06]
curr_lr =  [9.54314381270903e-06]
curr_lr =  [9.54314381270903e-06]
[2024-02-23 03:14:37,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=23, lr=[9.54247491638796e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.54180602006689e-06]
[2024-02-23 03:14:37,359] [INFO] [timer.py:260:stop] epoch=0/micro_step=7600/global_step=380, RunningAvgSamplesPerSec=3.5610655286429767, CurrSamplesPerSec=3.578059202030986, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.54180602006689e-06]
curr_lr =  [9.54046822742475e-06]
Epoch: [0][381/500]	Time 44.918 (44.863)	Loss 5.1016 (6.6717)	VQALoss 0.3523 (0.2939)	VGLoss 4.7493 (6.3778)
curr_lr =  [9.54046822742475e-06]
curr_lr =  [9.53913043478261e-06]
curr_lr =  [9.53913043478261e-06]
curr_lr =  [9.537792642140468e-06]
curr_lr =  [9.537792642140468e-06]
curr_lr =  [9.536454849498328e-06]
curr_lr =  [9.536454849498328e-06]
curr_lr =  [9.535117056856188e-06]
curr_lr =  [9.535117056856188e-06]
Epoch: [0][386/500]	Time 44.402 (44.574)	Loss 8.8948 (7.8819)	VQALoss 0.3093 (0.3076)	VGLoss 8.5855 (7.5743)curr_lr = 
 [9.533779264214048e-06]
curr_lr =  [9.533779264214048e-06]
curr_lr =  [9.532441471571908e-06]
curr_lr =  [9.532441471571908e-06]
curr_lr =  [9.531103678929766e-06]
curr_lr =  [9.531103678929766e-06]
curr_lr =  [9.529765886287626e-06]
curr_lr =  [9.529765886287626e-06]
[2024-02-23 03:22:02,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=23, lr=[9.529096989966556e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.528428093645486e-06]
[2024-02-23 03:22:02,709] [INFO] [timer.py:260:stop] epoch=0/micro_step=7800/global_step=390, RunningAvgSamplesPerSec=3.561876046470838, CurrSamplesPerSec=3.636354434612009, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.528428093645486e-06]
curr_lr =  [9.527090301003346e-06]
Epoch: [0][391/500]	Time 44.162 (44.345)	Loss 13.9846 (7.3816)	VQALoss 0.2893 (0.3007)	VGLoss 13.6953 (7.0809)
curr_lr =  [9.527090301003346e-06]
curr_lr =  [9.525752508361204e-06]
curr_lr =  [9.525752508361204e-06]
curr_lr =  [9.524414715719064e-06]
curr_lr =  [9.524414715719064e-06]
curr_lr =  [9.523076923076924e-06]
curr_lr =  [9.523076923076924e-06]
curr_lr =  [9.521739130434784e-06]
curr_lr =  [9.521739130434784e-06]
curr_lr =  Epoch: [0][396/500]	Time 43.907 (44.656)	Loss 6.4516 (7.2290)	VQALoss 0.4456 (0.3126)	VGLoss 6.0060 (6.9164)[9.520401337792644e-06]

curr_lr =  [9.520401337792644e-06]
curr_lr =  [9.519063545150502e-06]
curr_lr =  [9.519063545150502e-06]
curr_lr =  [9.517725752508362e-06]
curr_lr =  [9.517725752508362e-06]
curr_lr =  [9.516387959866222e-06]
curr_lr =  [9.516387959866222e-06]
[2024-02-23 03:29:28,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=23, lr=[9.515719063545152e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.515050167224082e-06]
[2024-02-23 03:29:28,888] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=400, RunningAvgSamplesPerSec=3.562480822597126, CurrSamplesPerSec=3.581375070007702, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.515050167224082e-06]
curr_lr =  [9.51371237458194e-06]
Epoch: [0][401/500]	Time 44.547 (44.656)	Loss 6.0760 (7.7637)	VQALoss 0.3005 (0.3057)	VGLoss 5.7755 (7.4580)
curr_lr =  [9.51371237458194e-06]
[2024-02-23 03:30:18,167] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step401 is about to be saved!
[2024-02-23 03:30:30,234] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step401/mp_rank_00_model_states.pt
[2024-02-23 03:30:30,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step401/mp_rank_00_model_states.pt...
[2024-02-23 03:32:50,994] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step401/mp_rank_00_model_states.pt.
[2024-02-23 03:32:52,679] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step401/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-23 03:32:52,680] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step401/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-23 03:33:06,480] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step401/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-23 03:33:06,480] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step401/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-23 03:33:06,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step401 is ready now!
[2024-02-23 03:33:06,963] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step401/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-23 03:33:06,973] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step401/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-23 03:33:06,973] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step401 is ready now!
curr_lr =  [9.5123745819398e-06]
curr_lr =  [9.5123745819398e-06]
curr_lr =  [9.51103678929766e-06]
curr_lr =  [9.51103678929766e-06]
curr_lr =  [9.509698996655518e-06]
curr_lr =  [9.509698996655518e-06]
curr_lr =  [9.50836120401338e-06]
curr_lr =  [9.50836120401338e-06]
curr_lr =  Epoch: [0][406/500]	Time 44.699 (79.342)	Loss 5.8443 (7.7263)	VQALoss 0.3511 (0.2906)	VGLoss 5.4932 (7.4357)
[9.507023411371238e-06]
curr_lr =  [9.507023411371238e-06]
curr_lr =  [9.505685618729098e-06]
curr_lr =  [9.505685618729098e-06]
curr_lr =  [9.504347826086958e-06]
curr_lr =  [9.504347826086958e-06]
curr_lr =  [9.503010033444817e-06]
curr_lr =  [9.503010033444817e-06]
[2024-02-23 03:39:47,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=23, lr=[9.502341137123746e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.501672240802676e-06]
[2024-02-23 03:39:48,128] [INFO] [timer.py:260:stop] epoch=0/micro_step=8200/global_step=410, RunningAvgSamplesPerSec=3.5631494347359034, CurrSamplesPerSec=3.6382157911593005, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.501672240802676e-06]
curr_lr =  [9.500334448160537e-06]
Epoch: [0][411/500]	Time 43.596 (44.316)	Loss 2.9397 (7.7081)	VQALoss 0.2445 (0.2764)	VGLoss 2.6951 (7.4316)
curr_lr =  [9.500334448160537e-06]
curr_lr =  [9.498996655518395e-06]
curr_lr =  [9.498996655518395e-06]
curr_lr =  [9.497658862876254e-06]
curr_lr =  [9.497658862876254e-06]
curr_lr =  [9.496321070234115e-06]
curr_lr =  [9.496321070234115e-06]
curr_lr =  [9.494983277591973e-06]
curr_lr =  [9.494983277591973e-06]
curr_lr =  Epoch: [0][416/500]	Time 44.611 (44.208)	Loss 5.2121 (7.0941)	VQALoss 0.6929 (0.2797)	VGLoss 4.5192 (6.8144)[9.493645484949833e-06]

curr_lr =  [9.493645484949833e-06]
curr_lr =  [9.492307692307693e-06]
curr_lr =  [9.492307692307693e-06]
curr_lr =  [9.490969899665553e-06]
curr_lr =  [9.490969899665553e-06]
curr_lr =  [9.489632107023411e-06]
curr_lr =  [9.489632107023411e-06]
[2024-02-23 03:47:10,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=23, lr=[9.488963210702341e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.488294314381271e-06]
[2024-02-23 03:47:10,921] [INFO] [timer.py:260:stop] epoch=0/micro_step=8400/global_step=420, RunningAvgSamplesPerSec=3.564338246611508, CurrSamplesPerSec=3.596616957641977, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.488294314381271e-06]
curr_lr =  Epoch: [0][421/500]	Time 45.177 (44.666)	Loss 6.6402 (7.3038)	VQALoss 0.2957 (0.2711)	VGLoss 6.3445 (7.0327)[9.486956521739131e-06]

curr_lr =  [9.486956521739131e-06]
curr_lr =  [9.485618729096991e-06]
curr_lr =  [9.485618729096991e-06]
curr_lr =  [9.484280936454851e-06]
curr_lr =  [9.484280936454851e-06]
curr_lr =  [9.48294314381271e-06]
curr_lr =  [9.48294314381271e-06]
curr_lr =  [9.48160535117057e-06]
curr_lr =  [9.48160535117057e-06]
curr_lr =  [9.480267558528429e-06]
Epoch: [0][426/500]	Time 44.496 (45.070)	Loss 8.0854 (8.0182)	VQALoss 0.2070 (0.2863)	VGLoss 7.8783 (7.7319)
curr_lr =  [9.480267558528429e-06]
curr_lr =  [9.478929765886289e-06]
curr_lr =  [9.478929765886289e-06]
curr_lr =  [9.477591973244147e-06]
curr_lr =  [9.477591973244147e-06]
curr_lr =  [9.476254180602007e-06]
curr_lr =  [9.476254180602007e-06]
[2024-02-23 03:54:44,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=23, lr=[9.475585284280937e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.474916387959867e-06]
[2024-02-23 03:54:44,305] [INFO] [timer.py:260:stop] epoch=0/micro_step=8600/global_step=430, RunningAvgSamplesPerSec=3.5635840663454394, CurrSamplesPerSec=3.5981240175798384, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.474916387959867e-06]
curr_lr = Epoch: [0][431/500]	Time 45.129 (45.597)	Loss 5.3889 (7.1438)	VQALoss 0.2258 (0.2779)	VGLoss 5.1630 (6.8658) 
[9.473578595317727e-06]
curr_lr =  [9.473578595317727e-06]
curr_lr =  [9.472240802675587e-06]
curr_lr =  [9.472240802675587e-06]
curr_lr =  [9.470903010033445e-06]
curr_lr =  [9.470903010033445e-06]
curr_lr =  [9.469565217391305e-06]
curr_lr =  [9.469565217391305e-06]
curr_lr =  [9.468227424749165e-06]
curr_lr =  [9.468227424749165e-06]
curr_lr =  [9.466889632107025e-06]
Epoch: [0][436/500]	Time 45.323 (45.105)	Loss 24.9221 (7.1770)	VQALoss 0.2612 (0.2888)	VGLoss 24.6609 (6.8882)
curr_lr =  [9.466889632107025e-06]
curr_lr =  [9.465551839464883e-06]
curr_lr =  [9.465551839464883e-06]
curr_lr =  [9.464214046822743e-06]
curr_lr =  [9.464214046822743e-06]
curr_lr =  [9.462876254180603e-06]
curr_lr =  [9.462876254180603e-06]
[2024-02-23 04:02:14,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=23, lr=[9.462207357859533e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.461538461538463e-06]
[2024-02-23 04:02:14,569] [INFO] [timer.py:260:stop] epoch=0/micro_step=8800/global_step=440, RunningAvgSamplesPerSec=3.563355041248678, CurrSamplesPerSec=3.5599271038081124, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.461538461538463e-06]
curr_lr =  [9.460200668896323e-06]
Epoch: [0][441/500]	Time 45.276 (44.977)	Loss 3.0233 (7.2579)	VQALoss 0.2544 (0.2895)	VGLoss 2.7689 (6.9685)
curr_lr =  [9.460200668896323e-06]
curr_lr =  [9.45886287625418e-06]
curr_lr =  [9.45886287625418e-06]
curr_lr =  [9.45752508361204e-06]
curr_lr =  [9.45752508361204e-06]
curr_lr =  [9.4561872909699e-06]
curr_lr =  [9.4561872909699e-06]
curr_lr =  [9.454849498327759e-06]
curr_lr =  [9.454849498327759e-06]
curr_lr =  [9.45351170568562e-06]
Epoch: [0][446/500]	Time 44.871 (45.002)	Loss 3.1594 (7.6741)	VQALoss 0.1378 (0.2843)	VGLoss 3.0216 (7.3898)
curr_lr =  [9.45351170568562e-06]
curr_lr =  [9.452173913043479e-06]
curr_lr =  [9.452173913043479e-06]
curr_lr =  [9.450836120401339e-06]
curr_lr =  [9.450836120401339e-06]
curr_lr =  [9.449498327759198e-06]
curr_lr =  [9.449498327759198e-06]
[2024-02-23 04:09:44,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=23, lr=[9.448829431438128e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.448160535117058e-06]
[2024-02-23 04:09:44,756] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=450, RunningAvgSamplesPerSec=3.5631498303896434, CurrSamplesPerSec=3.5400807302938277, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.448160535117058e-06]
curr_lr =  [9.446822742474917e-06]
Epoch: [0][451/500]	Time 45.209 (45.022)	Loss 8.7361 (7.1711)	VQALoss 0.2443 (0.2714)	VGLoss 8.4918 (6.8998)
curr_lr =  [9.446822742474917e-06]
[2024-02-23 04:10:34,176] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step451 is about to be saved!
[2024-02-23 04:10:46,276] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step451/mp_rank_00_model_states.pt
[2024-02-23 04:10:46,276] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step451/mp_rank_00_model_states.pt...
[2024-02-23 04:13:07,466] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step451/mp_rank_00_model_states.pt.
[2024-02-23 04:13:09,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step451/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-23 04:13:09,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step451/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-23 04:13:22,523] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step451/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-23 04:13:22,523] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step451/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-23 04:13:22,523] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step451 is ready now!
[2024-02-23 04:13:23,091] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step451/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-23 04:13:23,103] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step451/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-23 04:13:23,103] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step451 is ready now!
curr_lr =  [9.445484949832776e-06]
curr_lr =  [9.445484949832776e-06]
curr_lr =  [9.444147157190636e-06]
curr_lr =  [9.444147157190636e-06]
curr_lr =  [9.442809364548495e-06]
curr_lr =  [9.442809364548495e-06]
curr_lr =  [9.441471571906356e-06]
curr_lr =  [9.441471571906356e-06]
Epoch: [0][456/500]	Time 46.175 (80.756)	Loss 31.4605 (7.4995)	VQALoss 0.2185 (0.2766)	VGLoss 31.2420 (7.2230)curr_lr = 
 [9.440133779264214e-06]
curr_lr =  [9.440133779264214e-06]
curr_lr =  [9.438795986622074e-06]
curr_lr =  [9.438795986622074e-06]
curr_lr =  [9.437458193979934e-06]
curr_lr =  [9.437458193979934e-06]
curr_lr =  [9.436120401337794e-06]
curr_lr =  [9.436120401337794e-06]
[2024-02-23 04:20:14,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=23, lr=[9.435451505016722e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.434782608695652e-06]
[2024-02-23 04:20:14,948] [INFO] [timer.py:260:stop] epoch=0/micro_step=9200/global_step=460, RunningAvgSamplesPerSec=3.561765311771519, CurrSamplesPerSec=3.540002206222002, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.434782608695652e-06]
curr_lr =  [9.433444816053512e-06]
Epoch: [0][461/500]	Time 45.346 (45.310)	Loss 4.4696 (7.8677)	VQALoss 0.2546 (0.2519)	VGLoss 4.2150 (7.6158)
curr_lr =  [9.433444816053512e-06]
curr_lr =  [9.432107023411372e-06]
curr_lr =  [9.432107023411372e-06]
curr_lr =  [9.43076923076923e-06]
curr_lr =  [9.43076923076923e-06]
curr_lr =  [9.429431438127092e-06]
curr_lr =  [9.429431438127092e-06]
curr_lr =  [9.42809364548495e-06]
curr_lr =  [9.42809364548495e-06]
curr_lr = Epoch: [0][466/500]	Time 45.371 (45.190)	Loss 4.6660 (7.0999)	VQALoss 0.2185 (0.2682)	VGLoss 4.4475 (6.8317) 
[9.42675585284281e-06]
curr_lr =  [9.42675585284281e-06]
curr_lr =  [9.42541806020067e-06]
curr_lr =  [9.42541806020067e-06]
curr_lr =  [9.42408026755853e-06]
curr_lr =  [9.42408026755853e-06]
curr_lr =  [9.422742474916388e-06]
curr_lr =  [9.422742474916388e-06]
[2024-02-23 04:27:46,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=23, lr=[9.422073578595318e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.421404682274248e-06]
[2024-02-23 04:27:46,155] [INFO] [timer.py:260:stop] epoch=0/micro_step=9400/global_step=470, RunningAvgSamplesPerSec=3.56143019721322, CurrSamplesPerSec=3.5889750808926215, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.421404682274248e-06]
curr_lr = Epoch: [0][471/500]	Time 45.280 (45.039)	Loss 4.7033 (7.7433)	VQALoss 0.2988 (0.2476)	VGLoss 4.4044 (7.4956)
 [9.420066889632108e-06]
curr_lr =  [9.420066889632108e-06]
curr_lr =  [9.418729096989966e-06]
curr_lr =  [9.418729096989966e-06]
curr_lr =  [9.417391304347828e-06]
curr_lr =  [9.417391304347828e-06]
curr_lr =  [9.416053511705686e-06]
curr_lr =  [9.416053511705686e-06]
curr_lr =  [9.414715719063546e-06]
curr_lr =  [9.414715719063546e-06]
curr_lr =  [9.413377926421406e-06]
Epoch: [0][476/500]	Time 45.568 (45.465)	Loss 21.8770 (7.3701)	VQALoss 0.3755 (0.2481)	VGLoss 21.5015 (7.1220)
curr_lr =  [9.413377926421406e-06]
curr_lr =  [9.412040133779266e-06]
curr_lr =  [9.412040133779266e-06]
curr_lr =  [9.410702341137124e-06]
curr_lr =  [9.410702341137124e-06]
curr_lr =  [9.409364548494984e-06]
curr_lr =  [9.409364548494984e-06]
[2024-02-23 04:35:19,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=23, lr=[9.408695652173914e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.408026755852844e-06]
[2024-02-23 04:35:19,552] [INFO] [timer.py:260:stop] epoch=0/micro_step=9600/global_step=480, RunningAvgSamplesPerSec=3.560811882156099, CurrSamplesPerSec=3.6037067954972932, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.408026755852844e-06]
curr_lr = Epoch: [0][481/500]	Time 44.063 (44.971)	Loss 7.1374 (7.2416)	VQALoss 0.3853 (0.2586)	VGLoss 6.7521 (6.9830) 
[9.406688963210704e-06]
curr_lr =  [9.406688963210704e-06]
curr_lr =  [9.405351170568564e-06]
curr_lr =  [9.405351170568564e-06]
curr_lr =  [9.404013377926422e-06]
curr_lr =  [9.404013377926422e-06]
curr_lr =  [9.402675585284282e-06]
curr_lr =  [9.402675585284282e-06]
curr_lr =  [9.401337792642142e-06]
curr_lr =  [9.401337792642142e-06]
curr_lr =  [9.4e-06]
Epoch: [0][486/500]	Time 45.116 (44.975)	Loss 2.4815 (6.9850)	VQALoss 0.1622 (0.2565)	VGLoss 2.3192 (6.7285)
curr_lr =  [9.4e-06]
curr_lr =  [9.39866220735786e-06]
curr_lr =  [9.39866220735786e-06]
curr_lr =  [9.39732441471572e-06]
curr_lr =  [9.39732441471572e-06]
curr_lr =  [9.39598662207358e-06]
curr_lr =  [9.39598662207358e-06]
[2024-02-23 04:42:49,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=23, lr=[9.39531772575251e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.39464882943144e-06]
[2024-02-23 04:42:50,045] [INFO] [timer.py:260:stop] epoch=0/micro_step=9800/global_step=490, RunningAvgSamplesPerSec=3.560626143941826, CurrSamplesPerSec=3.5106892488456327, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.39464882943144e-06]
curr_lr =  [9.3933110367893e-06]
Epoch: [0][491/500]	Time 44.692 (45.249)	Loss 5.4528 (6.5175)	VQALoss 0.3154 (0.2533)	VGLoss 5.1374 (6.2642)
curr_lr =  [9.3933110367893e-06]
curr_lr =  [9.391973244147158e-06]
curr_lr =  [9.391973244147158e-06]
curr_lr =  [9.390635451505017e-06]
curr_lr =  [9.390635451505017e-06]
curr_lr =  [9.389297658862877e-06]
curr_lr =  [9.389297658862877e-06]
curr_lr =  [9.387959866220736e-06]
curr_lr =  [9.387959866220736e-06]
curr_lr =  [9.386622073578595e-06]
Epoch: [0][496/500]	Time 45.372 (45.539)	Loss 11.8762 (6.2865)	VQALoss 0.1665 (0.2523)	VGLoss 11.7097 (6.0342)
curr_lr =  [9.386622073578595e-06]
curr_lr =  [9.385284280936455e-06]
curr_lr =  [9.385284280936455e-06]
curr_lr =  [9.383946488294315e-06]
curr_lr =  [9.383946488294315e-06]
curr_lr =  [9.382608695652175e-06]
curr_lr =  [9.382608695652175e-06]
[2024-02-23 04:50:22,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=23, lr=[9.381939799331105e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.381270903010035e-06]
[2024-02-23 04:50:23,093] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=500, RunningAvgSamplesPerSec=3.5600416075884973, CurrSamplesPerSec=3.4904053784130538, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.381270903010035e-06]
==> Epoch 1/30
==> Epoch 1/30
Epoch: [1][  1/500]	Time 44.639 (44.638)	Loss 10.9938 (6.7928)	VQALoss 0.4785 (0.2587)	VGLoss 10.5153 (6.5341)
[2024-02-23 04:51:07,749] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step501 is about to be saved!
[2024-02-23 04:51:25,676] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_1/global_step501/mp_rank_00_model_states.pt
[2024-02-23 04:51:25,677] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step501/mp_rank_00_model_states.pt...
[2024-02-23 04:53:53,921] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step501/mp_rank_00_model_states.pt.
[2024-02-23 04:53:55,343] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step501/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-23 04:53:55,343] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step501/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-23 04:54:08,699] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step501/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-23 04:54:08,699] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step501/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-23 04:54:08,699] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step501 is ready now!
[2024-02-23 04:54:09,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step501/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-23 04:54:09,316] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step501/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-23 04:54:09,316] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step501 is ready now!
curr_lr =  [9.379264214046823e-06]
curr_lr =  [9.379264214046823e-06]
curr_lr =  [9.377926421404683e-06]
curr_lr =  [9.377926421404683e-06]
curr_lr =  [9.376588628762543e-06]
curr_lr =  [9.376588628762543e-06]
curr_lr =  [9.375250836120401e-06]
curr_lr =  [9.375250836120401e-06]
curr_lr =  [9.373913043478263e-06]
Epoch: [1][  6/500]	Time 45.386 (81.822)	Loss 4.4085 (6.9501)	VQALoss 0.1231 (0.2532)	VGLoss 4.2853 (6.6969)
curr_lr =  [9.373913043478263e-06]
curr_lr =  [9.372575250836121e-06]
curr_lr =  [9.372575250836121e-06]
curr_lr =  [9.371237458193981e-06]
curr_lr =  [9.371237458193981e-06]
curr_lr =  [9.369899665551841e-06]
curr_lr =  [9.369899665551841e-06]
[2024-02-23 05:00:58,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=23, lr=[9.369230769230771e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.368561872909699e-06]
[2024-02-23 05:00:58,547] [INFO] [timer.py:260:stop] epoch=0/micro_step=10200/global_step=510, RunningAvgSamplesPerSec=3.5593525573772333, CurrSamplesPerSec=3.5415784896238254, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.368561872909699e-06]
curr_lr =  [9.367224080267559e-06]
Epoch: [1][ 11/500]	Time 45.285 (45.396)	Loss 5.1427 (6.5957)	VQALoss 0.4768 (0.2597)	VGLoss 4.6659 (6.3361)
curr_lr =  [9.367224080267559e-06]
curr_lr =  [9.365886287625419e-06]
curr_lr =  [9.365886287625419e-06]
curr_lr =  [9.364548494983279e-06]
curr_lr =  [9.364548494983279e-06]
curr_lr =  [9.363210702341137e-06]
curr_lr =  [9.363210702341137e-06]
curr_lr =  [9.361872909698997e-06]
curr_lr =  [9.361872909698997e-06]
curr_lr =  [9.360535117056857e-06]
Epoch: [1][ 16/500]	Time 44.863 (44.982)	Loss 6.5595 (7.5484)	VQALoss 0.4521 (0.2426)	VGLoss 6.1074 (7.3058)
curr_lr =  [9.360535117056857e-06]
curr_lr =  [9.359197324414717e-06]
curr_lr =  [9.359197324414717e-06]
curr_lr =  [9.357859531772577e-06]
curr_lr =  [9.357859531772577e-06]
curr_lr =  [9.356521739130435e-06]
curr_lr =  [9.356521739130435e-06]
[2024-02-23 05:08:29,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=23, lr=[9.355852842809365e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.355183946488295e-06]
[2024-02-23 05:08:29,457] [INFO] [timer.py:260:stop] epoch=0/micro_step=10400/global_step=520, RunningAvgSamplesPerSec=3.5591424451797975, CurrSamplesPerSec=3.5375343119225895, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.355183946488295e-06]
curr_lr =  [9.353846153846155e-06]
Epoch: [1][ 21/500]	Time 45.473 (45.238)	Loss 12.0344 (7.0024)	VQALoss 0.3491 (0.2524)	VGLoss 11.6853 (6.7500)
curr_lr =  [9.353846153846155e-06]
curr_lr =  [9.352508361204015e-06]
curr_lr =  [9.352508361204015e-06]
curr_lr =  [9.351170568561873e-06]
curr_lr =  [9.351170568561873e-06]
curr_lr =  [9.349832775919733e-06]
curr_lr =  [9.349832775919733e-06]
curr_lr =  [9.348494983277593e-06]
curr_lr =  [9.348494983277593e-06]
curr_lr =  Epoch: [1][ 26/500]	Time 45.078 (45.383)	Loss 9.4799 (8.1715)	VQALoss 0.1429 (0.2528)	VGLoss 9.3369 (7.9187)[9.347157190635453e-06]

curr_lr =  [9.347157190635453e-06]
curr_lr =  [9.345819397993312e-06]
curr_lr =  [9.345819397993312e-06]
curr_lr =  [9.34448160535117e-06]
curr_lr =  [9.34448160535117e-06]
curr_lr =  [9.34314381270903e-06]
curr_lr =  [9.34314381270903e-06]
[2024-02-23 05:16:00,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=23, lr=[9.34247491638796e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.34180602006689e-06]
[2024-02-23 05:16:00,523] [INFO] [timer.py:260:stop] epoch=0/micro_step=10600/global_step=530, RunningAvgSamplesPerSec=3.5589165179786746, CurrSamplesPerSec=3.596805367478469, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.34180602006689e-06]
curr_lr = Epoch: [1][ 31/500]	Time 44.660 (44.668)	Loss 9.7509 (6.9213)	VQALoss 0.3433 (0.2349)	VGLoss 9.4076 (6.6864) 
[9.34046822742475e-06]
curr_lr =  [9.34046822742475e-06]
curr_lr =  [9.33913043478261e-06]
curr_lr =  [9.33913043478261e-06]
curr_lr =  [9.337792642140469e-06]
curr_lr =  [9.337792642140469e-06]
curr_lr =  [9.336454849498328e-06]
curr_lr =  [9.336454849498328e-06]
curr_lr =  [9.335117056856188e-06]
curr_lr =  [9.335117056856188e-06]
curr_lr =  [9.333779264214048e-06]
Epoch: [1][ 36/500]	Time 46.395 (45.821)	Loss 4.9098 (7.3937)	VQALoss 0.1903 (0.2513)	VGLoss 4.7195 (7.1424)
curr_lr =  [9.333779264214048e-06]
curr_lr =  [9.332441471571906e-06]
curr_lr =  [9.332441471571906e-06]
curr_lr =  [9.331103678929766e-06]
curr_lr =  [9.331103678929766e-06]
curr_lr =  [9.329765886287626e-06]
curr_lr =  [9.329765886287626e-06]
[2024-02-23 05:23:37,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=23, lr=[9.329096989966556e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.328428093645484e-06]
[2024-02-23 05:23:37,770] [INFO] [timer.py:260:stop] epoch=0/micro_step=10800/global_step=540, RunningAvgSamplesPerSec=3.557847553775703, CurrSamplesPerSec=3.492829455225967, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.328428093645484e-06]
curr_lr =  [9.327090301003346e-06]
Epoch: [1][ 41/500]	Time 46.488 (45.994)	Loss 5.9319 (6.9510)	VQALoss 0.3677 (0.2444)	VGLoss 5.5642 (6.7066)
curr_lr =  [9.327090301003346e-06]
curr_lr =  [9.325752508361204e-06]
curr_lr =  [9.325752508361204e-06]
curr_lr =  [9.324414715719064e-06]
curr_lr =  [9.324414715719064e-06]
curr_lr =  [9.323076923076924e-06]
curr_lr =  [9.323076923076924e-06]
curr_lr =  [9.321739130434784e-06]
curr_lr =  [9.321739130434784e-06]
curr_lr = Epoch: [1][ 46/500]	Time 45.355 (45.475)	Loss 10.1044 (6.7471)	VQALoss 0.1536 (0.2370)	VGLoss 9.9508 (6.5100) 
[9.320401337792642e-06]
curr_lr =  [9.320401337792642e-06]
curr_lr =  [9.319063545150502e-06]
curr_lr =  [9.319063545150502e-06]
curr_lr =  [9.317725752508362e-06]
curr_lr =  [9.317725752508362e-06]
curr_lr =  [9.31638795986622e-06]
curr_lr =  [9.31638795986622e-06]
[2024-02-23 05:31:12,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=23, lr=[9.315719063545152e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.315050167224082e-06]
[2024-02-23 05:31:12,756] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=550, RunningAvgSamplesPerSec=3.5570881764997764, CurrSamplesPerSec=3.5467011521852707, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.315050167224082e-06]
curr_lr =  [9.31371237458194e-06]
Epoch: [1][ 51/500]	Time 45.102 (45.245)	Loss 3.9019 (6.6437)	VQALoss 0.3884 (0.2389)	VGLoss 3.5135 (6.4048)
curr_lr =  [9.31371237458194e-06]
[2024-02-23 05:32:03,342] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step551 is about to be saved!
[2024-02-23 05:32:15,894] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_1/global_step551/mp_rank_00_model_states.pt
[2024-02-23 05:32:15,894] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step551/mp_rank_00_model_states.pt...
[2024-02-23 05:34:36,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step551/mp_rank_00_model_states.pt.
[2024-02-23 05:34:37,761] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step551/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-23 05:34:37,761] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step551/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-23 05:34:50,969] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step551/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-23 05:34:50,969] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step551/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-23 05:34:50,969] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step551 is ready now!
[2024-02-23 05:34:52,647] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step551/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-23 05:34:52,657] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step551/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-23 05:34:52,657] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step551 is ready now!
curr_lr =  [9.3123745819398e-06]
curr_lr =  [9.3123745819398e-06]
curr_lr =  [9.31103678929766e-06]
curr_lr =  [9.31103678929766e-06]
curr_lr =  [9.30969899665552e-06]
curr_lr =  [9.30969899665552e-06]
curr_lr =  [9.308361204013378e-06]
curr_lr =  [9.308361204013378e-06]
curr_lr =  [9.307023411371238e-06]
Epoch: [1][ 56/500]	Time 45.699 (80.658)	Loss 5.6466 (6.5857)	VQALoss 0.1536 (0.2364)	VGLoss 5.4930 (6.3493)
curr_lr =  [9.307023411371238e-06]
curr_lr =  [9.305685618729098e-06]
curr_lr =  [9.305685618729098e-06]
curr_lr =  [9.304347826086956e-06]
curr_lr =  [9.304347826086956e-06]
curr_lr =  [9.303010033444818e-06]
curr_lr =  [9.303010033444818e-06]
[2024-02-23 05:41:42,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=23, lr=[9.302341137123748e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.301672240802676e-06]
[2024-02-23 05:41:42,672] [INFO] [timer.py:260:stop] epoch=0/micro_step=11200/global_step=560, RunningAvgSamplesPerSec=3.556338383668225, CurrSamplesPerSec=3.4924214706424115, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.301672240802676e-06]
curr_lr =  [9.300334448160536e-06]
Epoch: [1][ 61/500]	Time 45.392 (45.383)	Loss 6.0603 (7.4519)	VQALoss 0.1285 (0.2328)	VGLoss 5.9317 (7.2192)
curr_lr =  [9.300334448160536e-06]
curr_lr =  [9.298996655518396e-06]
curr_lr =  [9.298996655518396e-06]
curr_lr =  [9.297658862876256e-06]
curr_lr =  [9.297658862876256e-06]
curr_lr =  [9.296321070234114e-06]
curr_lr =  [9.296321070234114e-06]
curr_lr =  [9.294983277591974e-06]
curr_lr =  [9.294983277591974e-06]
curr_lr =  [9.293645484949834e-06]
Epoch: [1][ 66/500]	Time 45.565 (45.635)	Loss 6.5737 (6.6530)	VQALoss 0.2455 (0.2310)	VGLoss 6.3282 (6.4219)
curr_lr =  [9.293645484949834e-06]
curr_lr =  [9.292307692307694e-06]
curr_lr =  [9.292307692307694e-06]
curr_lr =  [9.290969899665553e-06]
curr_lr =  [9.290969899665553e-06]
curr_lr =  [9.289632107023412e-06]
curr_lr =  [9.289632107023412e-06]
[2024-02-23 05:49:19,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=23, lr=[9.288963210702342e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.288294314381272e-06]
[2024-02-23 05:49:19,338] [INFO] [timer.py:260:stop] epoch=0/micro_step=11400/global_step=570, RunningAvgSamplesPerSec=3.555399323504649, CurrSamplesPerSec=3.496337033923051, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.288294314381272e-06]
curr_lr =  [9.286956521739131e-06]
Epoch: [1][ 71/500]	Time 45.507 (45.721)	Loss 5.7196 (6.6205)	VQALoss 0.1562 (0.2404)	VGLoss 5.5633 (6.3800)
curr_lr =  [9.286956521739131e-06]
curr_lr =  [9.285618729096991e-06]
curr_lr =  [9.285618729096991e-06]
curr_lr =  [9.28428093645485e-06]
curr_lr =  [9.28428093645485e-06]
curr_lr =  [9.28294314381271e-06]
curr_lr =  [9.28294314381271e-06]
curr_lr =  [9.28160535117057e-06]
curr_lr =  [9.28160535117057e-06]
curr_lr =  [9.28026755852843e-06]
Epoch: [1][ 76/500]	Time 45.259 (45.364)	Loss 24.0880 (7.1948)	VQALoss 0.1997 (0.2275)	VGLoss 23.8883 (6.9673)
curr_lr =  [9.28026755852843e-06]
curr_lr =  [9.27892976588629e-06]
curr_lr =  [9.27892976588629e-06]
curr_lr =  [9.277591973244147e-06]
curr_lr =  [9.277591973244147e-06]
curr_lr =  [9.276254180602007e-06]
curr_lr =  [9.276254180602007e-06]
[2024-02-23 05:56:53,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=23, lr=[9.275585284280937e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.274916387959867e-06]
[2024-02-23 05:56:53,468] [INFO] [timer.py:260:stop] epoch=0/micro_step=11600/global_step=580, RunningAvgSamplesPerSec=3.5548399005726457, CurrSamplesPerSec=3.554857668897505, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.274916387959867e-06]
curr_lr =  [9.273578595317725e-06]
Epoch: [1][ 81/500]	Time 45.866 (45.534)	Loss 6.4210 (6.5062)	VQALoss 0.1698 (0.2246)	VGLoss 6.2512 (6.2817)
curr_lr =  [9.273578595317725e-06]
curr_lr =  [9.272240802675585e-06]
curr_lr =  [9.272240802675585e-06]
curr_lr =  [9.270903010033445e-06]
curr_lr =  [9.270903010033445e-06]
curr_lr =  [9.269565217391305e-06]
curr_lr =  [9.269565217391305e-06]
curr_lr =  [9.268227424749165e-06]
curr_lr =  [9.268227424749165e-06]
curr_lr =  [9.266889632107025e-06]
Epoch: [1][ 86/500]	Time 45.493 (46.003)	Loss 9.9311 (7.8838)	VQALoss 0.2004 (0.2378)	VGLoss 9.7307 (7.6460)
curr_lr =  [9.266889632107025e-06]
curr_lr =  [9.265551839464883e-06]
curr_lr =  [9.265551839464883e-06]
curr_lr =  [9.264214046822743e-06]
curr_lr =  [9.264214046822743e-06]
curr_lr =  [9.262876254180603e-06]
curr_lr =  [9.262876254180603e-06]
[2024-02-23 06:04:31,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=23, lr=[9.262207357859533e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.261538461538461e-06]
[2024-02-23 06:04:32,136] [INFO] [timer.py:260:stop] epoch=0/micro_step=11800/global_step=590, RunningAvgSamplesPerSec=3.5537459292009275, CurrSamplesPerSec=3.4797718825990858, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.261538461538461e-06]
curr_lr =  [9.260200668896321e-06]
Epoch: [1][ 91/500]	Time 45.886 (45.735)	Loss 2.7318 (6.7288)	VQALoss 0.0993 (0.2338)	VGLoss 2.6325 (6.4951)
curr_lr =  [9.260200668896321e-06]
curr_lr =  [9.258862876254181e-06]
curr_lr =  [9.258862876254181e-06]
curr_lr =  [9.257525083612041e-06]
curr_lr =  [9.257525083612041e-06]
curr_lr =  [9.2561872909699e-06]
curr_lr =  [9.2561872909699e-06]
curr_lr =  [9.25484949832776e-06]
curr_lr =  [9.25484949832776e-06]
curr_lr = Epoch: [1][ 96/500]	Time 45.123 (45.110)	Loss 2.5365 (6.3625)	VQALoss 0.2355 (0.2241)	VGLoss 2.3010 (6.1383) 
[9.253511705685619e-06]
curr_lr =  [9.253511705685619e-06]
curr_lr =  [9.252173913043479e-06]
curr_lr =  [9.252173913043479e-06]
curr_lr =  [9.250836120401339e-06]
curr_lr =  [9.250836120401339e-06]
curr_lr =  [9.249498327759197e-06]
curr_lr =  [9.249498327759197e-06]
[2024-02-23 06:12:05,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=23, lr=[9.248829431438127e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.248160535117059e-06]
[2024-02-23 06:12:05,665] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=600, RunningAvgSamplesPerSec=3.5533125025739114, CurrSamplesPerSec=3.5317579142371933, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.248160535117059e-06]
curr_lr =  [9.246822742474917e-06]
Epoch: [1][101/500]	Time 45.897 (45.598)	Loss 7.2675 (6.3047)	VQALoss 0.3718 (0.2344)	VGLoss 6.8957 (6.0703)
curr_lr =  [9.246822742474917e-06]
[2024-02-23 06:12:56,274] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step601 is about to be saved!
[2024-02-23 06:13:08,930] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_1/global_step601/mp_rank_00_model_states.pt
[2024-02-23 06:13:08,930] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step601/mp_rank_00_model_states.pt...
[2024-02-23 06:15:28,093] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step601/mp_rank_00_model_states.pt.
[2024-02-23 06:15:29,793] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step601/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-23 06:15:29,793] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step601/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-23 06:15:42,235] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step601/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-23 06:15:42,235] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step601/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-23 06:15:42,235] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step601 is ready now!
[2024-02-23 06:15:42,617] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step601/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-23 06:15:42,622] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step601/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-23 06:15:42,622] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step601 is ready now!
curr_lr =  [9.245484949832777e-06]
curr_lr =  [9.245484949832777e-06]
curr_lr =  [9.244147157190637e-06]
curr_lr =  [9.244147157190637e-06]
curr_lr =  [9.242809364548497e-06]
curr_lr =  [9.242809364548497e-06]
curr_lr =  [9.241471571906355e-06]
curr_lr =  [9.241471571906355e-06]
curr_lr =  [9.240133779264215e-06]
Epoch: [1][106/500]	Time 45.251 (79.351)	Loss 5.2188 (6.3944)	VQALoss 0.1580 (0.2213)	VGLoss 5.0608 (6.1731)
curr_lr =  [9.240133779264215e-06]
curr_lr =  [9.238795986622075e-06]
curr_lr =  [9.238795986622075e-06]
curr_lr =  [9.237458193979933e-06]
curr_lr =  [9.237458193979933e-06]
curr_lr =  [9.236120401337794e-06]
curr_lr =  [9.236120401337794e-06]
[2024-02-23 06:22:30,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=23, lr=[9.235451505016724e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.234782608695653e-06]
[2024-02-23 06:22:31,061] [INFO] [timer.py:260:stop] epoch=0/micro_step=12200/global_step=610, RunningAvgSamplesPerSec=3.5527888250538733, CurrSamplesPerSec=3.5018434196349624, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.234782608695653e-06]
curr_lr =  [9.233444816053512e-06]
Epoch: [1][111/500]	Time 45.532 (45.655)	Loss 5.8964 (6.8118)	VQALoss 0.2134 (0.2182)	VGLoss 5.6830 (6.5936)
curr_lr =  [9.233444816053512e-06]
curr_lr =  [9.232107023411372e-06]
curr_lr =  [9.232107023411372e-06]
curr_lr =  [9.230769230769232e-06]
curr_lr =  [9.230769230769232e-06]
curr_lr =  [9.22943143812709e-06]
curr_lr =  [9.22943143812709e-06]
curr_lr =  [9.22809364548495e-06]
curr_lr =  [9.22809364548495e-06]
Epoch: [1][116/500]	Time 45.885 (45.945)	Loss 3.8203 (6.0466)	VQALoss 0.1057 (0.2264)	VGLoss 3.7146 (5.8202)curr_lr = 
 [9.22675585284281e-06]
curr_lr =  [9.22675585284281e-06]
curr_lr =  [9.225418060200669e-06]
curr_lr =  [9.225418060200669e-06]
curr_lr =  [9.22408026755853e-06]
curr_lr =  [9.22408026755853e-06]
curr_lr =  [9.222742474916388e-06]
curr_lr =  [9.222742474916388e-06]
[2024-02-23 06:30:07,877] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=23, lr=[9.222073578595318e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.221404682274248e-06]
[2024-02-23 06:30:08,026] [INFO] [timer.py:260:stop] epoch=0/micro_step=12400/global_step=620, RunningAvgSamplesPerSec=3.5519465936698267, CurrSamplesPerSec=3.5172421972832897, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.221404682274248e-06]
curr_lr =  [9.220066889632108e-06]
Epoch: [1][121/500]	Time 45.183 (45.378)	Loss 6.0863 (6.2969)	VQALoss 0.1783 (0.2293)	VGLoss 5.9080 (6.0676)
curr_lr =  [9.220066889632108e-06]
curr_lr =  [9.218729096989966e-06]
curr_lr =  [9.218729096989966e-06]
curr_lr =  [9.217391304347826e-06]
curr_lr =  [9.217391304347826e-06]
curr_lr =  [9.216053511705686e-06]
curr_lr =  [9.216053511705686e-06]
curr_lr =  [9.214715719063546e-06]
curr_lr =  [9.214715719063546e-06]
curr_lr =  [9.213377926421404e-06]
Epoch: [1][126/500]	Time 45.494 (45.329)	Loss 8.8916 (6.6626)	VQALoss 0.2487 (0.2302)	VGLoss 8.6430 (6.4324)
curr_lr =  [9.213377926421404e-06]
curr_lr =  [9.212040133779266e-06]
curr_lr =  [9.212040133779266e-06]
curr_lr =  [9.210702341137124e-06]
curr_lr =  [9.210702341137124e-06]
curr_lr =  [9.209364548494984e-06]
curr_lr =  [9.209364548494984e-06]
[2024-02-23 06:37:42,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=23, lr=[9.208695652173914e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.208026755852844e-06]
[2024-02-23 06:37:42,769] [INFO] [timer.py:260:stop] epoch=0/micro_step=12600/global_step=630, RunningAvgSamplesPerSec=3.5514103853807772, CurrSamplesPerSec=3.5211441411495747, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.208026755852844e-06]
curr_lr =  [9.206688963210702e-06]
Epoch: [1][131/500]	Time 45.583 (45.700)	Loss 4.3513 (6.5874)	VQALoss 0.1276 (0.2358)	VGLoss 4.2237 (6.3516)
curr_lr =  [9.206688963210702e-06]
curr_lr =  [9.205351170568562e-06]
curr_lr =  [9.205351170568562e-06]
curr_lr =  [9.204013377926422e-06]
curr_lr =  [9.204013377926422e-06]
curr_lr =  [9.202675585284282e-06]
curr_lr =  [9.202675585284282e-06]
curr_lr =  [9.201337792642142e-06]
curr_lr =  [9.201337792642142e-06]
curr_lr =  [9.200000000000002e-06]
Epoch: [1][136/500]	Time 45.368 (45.357)	Loss 4.1149 (6.2465)	VQALoss 0.3201 (0.2256)	VGLoss 3.7949 (6.0209)
curr_lr =  [9.200000000000002e-06]
curr_lr =  [9.19866220735786e-06]
curr_lr =  [9.19866220735786e-06]
curr_lr =  [9.19732441471572e-06]
curr_lr =  [9.19732441471572e-06]
curr_lr =  [9.19598662207358e-06]
curr_lr =  [9.19598662207358e-06]
[2024-02-23 06:45:16,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=23, lr=[9.19531772575251e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.194648829431438e-06]
[2024-02-23 06:45:16,966] [INFO] [timer.py:260:stop] epoch=0/micro_step=12800/global_step=640, RunningAvgSamplesPerSec=3.5510226464603636, CurrSamplesPerSec=3.4822680464721025, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.194648829431438e-06]
curr_lr =  [9.193311036789298e-06]
Epoch: [1][141/500]	Time 45.411 (45.447)	Loss 8.9313 (6.8195)	VQALoss 0.2220 (0.2280)	VGLoss 8.7092 (6.5915)
curr_lr =  [9.193311036789298e-06]
curr_lr =  [9.191973244147158e-06]
curr_lr =  [9.191973244147158e-06]
curr_lr =  [9.190635451505018e-06]
curr_lr =  [9.190635451505018e-06]
curr_lr =  [9.189297658862878e-06]
curr_lr =  [9.189297658862878e-06]
curr_lr =  [9.187959866220737e-06]
curr_lr =  [9.187959866220737e-06]
curr_lr =  [9.186622073578596e-06]
Epoch: [1][146/500]	Time 45.820 (45.538)	Loss 2.4316 (5.8596)	VQALoss 0.2173 (0.2056)	VGLoss 2.2143 (5.6539)
curr_lr =  [9.186622073578596e-06]
curr_lr =  [9.185284280936456e-06]
curr_lr =  [9.185284280936456e-06]
curr_lr =  [9.183946488294316e-06]
curr_lr =  [9.183946488294316e-06]
curr_lr =  [9.182608695652174e-06]
curr_lr =  [9.182608695652174e-06]
[2024-02-23 06:52:53,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=23, lr=[9.181939799331104e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.181270903010034e-06]
[2024-02-23 06:52:53,772] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=650, RunningAvgSamplesPerSec=3.5502667333667044, CurrSamplesPerSec=3.5246640665434645, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.181270903010034e-06]
curr_lr =  [9.179933110367894e-06]
Epoch: [1][151/500]	Time 46.099 (45.961)	Loss 3.3590 (6.2942)	VQALoss 0.1373 (0.2167)	VGLoss 3.2217 (6.0774)
curr_lr =  [9.179933110367894e-06]
[2024-02-23 06:53:45,148] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step651 is about to be saved!
[2024-02-23 06:53:58,225] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_1/global_step651/mp_rank_00_model_states.pt
[2024-02-23 06:53:58,226] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step651/mp_rank_00_model_states.pt...
[2024-02-23 06:56:18,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step651/mp_rank_00_model_states.pt.
[2024-02-23 06:56:20,426] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step651/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-23 06:56:20,426] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_1/global_step651/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-23 06:56:33,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step651/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-23 06:56:33,383] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step651/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-23 06:56:33,383] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step651 is ready now!
[2024-02-23 06:56:33,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_1/global_step651/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-23 06:56:33,857] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_1/global_step651/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-23 06:56:33,857] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step651 is ready now!
curr_lr =  [9.178595317725753e-06]
curr_lr =  [9.178595317725753e-06]
curr_lr =  [9.177257525083613e-06]
curr_lr =  [9.177257525083613e-06]
curr_lr =  [9.175919732441473e-06]
curr_lr =  [9.175919732441473e-06]
curr_lr =  [9.174581939799331e-06]
curr_lr =  [9.174581939799331e-06]
curr_lr =  [9.173244147157191e-06]
Epoch: [1][156/500]	Time 44.985 (79.894)	Loss 4.8691 (6.8838)	VQALoss 0.1118 (0.2045)	VGLoss 4.7573 (6.6792)
curr_lr =  [9.173244147157191e-06]
curr_lr =  [9.171906354515051e-06]
curr_lr =  [9.171906354515051e-06]
curr_lr =  [9.17056856187291e-06]
curr_lr =  [9.17056856187291e-06]
curr_lr =  [9.169230769230771e-06]
curr_lr =  [9.169230769230771e-06]
[2024-02-23 07:03:21,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=23, lr=[9.1685618729097e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.16789297658863e-06]
[2024-02-23 07:03:21,542] [INFO] [timer.py:260:stop] epoch=0/micro_step=13200/global_step=660, RunningAvgSamplesPerSec=3.549896148843636, CurrSamplesPerSec=3.480183359968604, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.16789297658863e-06]
curr_lr =  [9.16655518394649e-06]
Epoch: [1][161/500]	Time 46.116 (45.663)	Loss 4.0376 (6.6184)	VQALoss 0.1705 (0.2183)	VGLoss 3.8670 (6.4001)
curr_lr =  [9.16655518394649e-06]
curr_lr =  [9.165217391304349e-06]
curr_lr =  [9.165217391304349e-06]
curr_lr =  [9.163879598662207e-06]
curr_lr =  [9.163879598662207e-06]
curr_lr =  [9.162541806020067e-06]
curr_lr =  [9.162541806020067e-06]
curr_lr =  [9.161204013377927e-06]
curr_lr =  [9.161204013377927e-06]
curr_lr =  [9.159866220735787e-06]
Epoch: [1][166/500]	Time 45.384 (45.777)	Loss 2.6828 (5.9646)	VQALoss 0.1290 (0.2187)	VGLoss 2.5538 (5.7459)
curr_lr =  [9.159866220735787e-06]
curr_lr =  [9.158528428093645e-06]
curr_lr =  [9.158528428093645e-06]
curr_lr =  [9.157190635451507e-06]
curr_lr =  [9.157190635451507e-06]
curr_lr =  [9.155852842809365e-06]
curr_lr =  [9.155852842809365e-06]
[2024-02-23 07:10:58,734] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=23, lr=[9.155183946488295e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.154515050167225e-06]
[2024-02-23 07:10:58,882] [INFO] [timer.py:260:stop] epoch=0/micro_step=13400/global_step=670, RunningAvgSamplesPerSec=3.5491172248371323, CurrSamplesPerSec=3.5183328639347318, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.154515050167225e-06]
curr_lr =  [9.153177257525085e-06]
Epoch: [1][171/500]	Time 46.216 (45.711)	Loss 5.7169 (6.1826)	VQALoss 0.1357 (0.2241)	VGLoss 5.5812 (5.9585)
curr_lr =  [9.153177257525085e-06]
curr_lr =  [9.151839464882943e-06]
curr_lr =  [9.151839464882943e-06]
curr_lr =  [9.150501672240803e-06]
curr_lr =  [9.150501672240803e-06]
curr_lr =  [9.149163879598663e-06]
curr_lr =  [9.149163879598663e-06]
                                                                                                                                                                                                                                                                                                                                                                                                              curr_lr =  [9.14247491638796e-06]
curr_lr =  [9.14247491638796e-06]
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py", line 244, in run
    self._run()
  File "/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py", line 289, in _run
    self._record_writer.flush()
  File "/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/record_writer.py", line 43, in flush
    self._writer.flush()
  File "/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py", line 221, in flush
    self._writable_file.flush()
tensorflow.python.framework.errors_impl.ResourceExhaustedError: runs/chatterbox/events.out.tfevents.1708639336.459b28a48274.51427.0; Disk quota exceeded
[2024-02-23 07:18:39,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=23, lr=[9.14180602006689e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.14113712374582e-06]
[2024-02-23 07:18:39,611] [INFO] [timer.py:260:stop] epoch=0/micro_step=13600/global_step=680, RunningAvgSamplesPerSec=3.547968552294971, CurrSamplesPerSec=3.4097634597241737, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.14113712374582e-06]
Traceback (most recent call last):
  File "/workspace/ChatterBox-Finetuning/custom_train_stage1.py", line 458, in <module>
    main(sys.argv[1:])
  File "/workspace/ChatterBox-Finetuning/custom_train_stage1.py", line 316, in main
    train(
  File "/workspace/ChatterBox-Finetuning/custom_train_stage1.py", line 445, in train
    writer.add_scalar("train/lr", curr_lr[0], global_step)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py", line 391, in add_scalar
    self._get_file_writer().add_summary(summary, global_step, walltime)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py", line 113, in add_summary
    self.add_event(event, global_step, walltime)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py", line 98, in add_event
    self.event_writer.add_event(event)
  File "/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py", line 117, in add_event
    self._async_writer.write(event.SerializeToString())
  File "/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py", line 171, in write
    self._check_worker_status()
  File "/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py", line 212, in _check_worker_status
    raise exception
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py", line 244, in run
    self._run()
  File "/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py", line 289, in _run
    self._record_writer.flush()
  File "/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/record_writer.py", line 43, in flush
    self._writer.flush()
  File "/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py", line 221, in flush
    self._writable_file.flush()
tensorflow.python.framework.errors_impl.ResourceExhaustedError: runs/chatterbox/events.out.tfevents.1708639336.459b28a48274.51427.0; Disk quota exceeded
[2024-02-23 07:18:43,670] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 51427
[2024-02-23 07:18:43,672] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 51428
[2024-02-23 07:18:44,449] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python', '-u', 'custom_train_stage1.py', '--local_rank=1', '--version', 'llava-llama-2-13b-chat-lightning-preview'] exits with return code = 1
[?2004hroot@459b28a48274:/workspace/ChatterBox-Finetuning# 
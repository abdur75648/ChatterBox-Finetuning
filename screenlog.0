 ______                 ______            _
(_____ \               (_____ \          | |
 _____) ) _   _  ____   _____) )___    __| |
|  __  / | | | ||  _ \ |  ____// _ \  / _  |
| |  \ \ | |_| || | | || |    | |_| |( (_| |
|_|   |_||____/ |_| |_||_|     \___/  \____|

For detailed documentation and guides, please visit:
[1;34mhttps://docs.runpod.io/[0m and [1;34mhttps://blog.runpod.io/[0m


[?2004hroot@459b28a48274:/workspace/ChatterBox-Finetuning# 
[?2004l[?2004hroot@459b28a48274:/workspace/ChatterBox-Finetuning# clear
[?2004l[H[2J[?2004hroot@459b28a48274:/workspace/ChatterBox-Finetuning# clearscreen -dmS test -Lclear[Kapt-get install screen[3Pscreen -dmS test -Lclear[Kmv screenlog.0 screenlog_old.txtclear[Kgit pull[3Pclearrm -rf outputs/[10Pcleardeepspeed --include localhost:0,1 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview[A[C[C[C[C[C[Cclear[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cdeepspeed --include localhost:0,1 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview[A[C[C[C[C[C[Cclear[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cdeepspeed --include localhost:0,1 --master_port 54906 custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview
[?2004l[2024-02-22 22:01:35,284] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-02-22 22:01:37.922053: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-22 22:01:37.925587: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-22 22:01:37.964724: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-22 22:01:38.815889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-02-22 22:01:40,076] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-02-22 22:01:40,077] [INFO] [runner.py:570:main] cmd = /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=54906 --enable_each_rank_log=None custom_train_stage1.py --version llava-llama-2-13b-chat-lightning-preview
[2024-02-22 22:01:41,755] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-02-22 22:01:44.130182: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-22 22:01:44.133651: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-22 22:01:44.174053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-22 22:01:44.928795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-02-22 22:01:46,013] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8
[2024-02-22 22:01:46,013] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1
[2024-02-22 22:01:46,013] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1
[2024-02-22 22:01:46,013] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-02-22 22:01:46,013] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8
[2024-02-22 22:01:46,013] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-02-22 22:01:46,014] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1
[2024-02-22 22:01:46,014] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2024-02-22 22:01:46,014] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-02-22 22:01:46,014] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-02-22 22:01:46,014] [INFO] [launch.py:163:main] dist_world_size=2
[2024-02-22 22:01:46,014] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2024-02-22 22:01:47,989] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-22 22:01:48,004] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[Kroot@459b28a48274:/workspace/ChatterBox-Finetuning# ^C[?2004l[?2004h[?2004l
[?2004hroot@459b28a48274:/workspace/ChatterBox-Finetuning# [?2004l
exit
2024-02-22 22:01:50.487303: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-22 22:01:50.490759: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-22 22:01:50.530504: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-22 22:01:50.627028: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-22 22:01:50.630486: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-22 22:01:50.669708: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-22 22:01:51.286959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-22 22:01:51.426352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
config  >>>  config  >>>  LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

LlavaConfig {
  "_name_or_path": "llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|                                                                               | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                               | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███████████████████████▋                                               | 1/3 [00:57<01:55, 57.85s/it]Loading checkpoint shards:  33%|███████████████████████▋                                               | 1/3 [00:57<01:55, 57.93s/it]Loading checkpoint shards:  67%|███████████████████████████████████████████████▎                       | 2/3 [01:42<00:50, 50.30s/it]Loading checkpoint shards:  67%|███████████████████████████████████████████████▎                       | 2/3 [01:42<00:50, 50.35s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [02:19<00:00, 43.94s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [02:19<00:00, 46.43s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [02:19<00:00, 43.93s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [02:19<00:00, 46.41s/it]
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
trainable params: 13,107,200 || all params: 13,034,219,520 || trainable%: 0.10055991446122275
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Number of trainable parameters: 791.85M
Number of trainable parameters in model.lm: 340.84M
Number of trainable parameters in model.visual_grounding_model: 123.22M
trainable params: 340,838,400 || all params: 13,034,270,720 || trainable%: 2.6149403163539633
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-22 22:05:42,612] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-22 22:05:42,612] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-22 22:05:42,612] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Number of trainable parameters: 791.85M
Number of trainable parameters in model.lm: 340.84M
Number of trainable parameters in model.visual_grounding_model: 123.22M
trainable params: 340,838,400 || all params: 13,034,270,720 || trainable%: 2.6149403163539633
loading visual_grounding_model  _IncompatibleKeys(missing_keys=['transformer.tgt_embed.weight'], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'feat_map.weight', 'feat_map.bias'])
loading vision branch  >>  None
[2024-02-22 22:05:45,121] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.11.1, git-hash=unknown, git-branch=unknown
[2024-02-22 22:05:45,122] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-22 22:06:06,270] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.08755898475646973 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10170459747314453 seconds
[2024-02-22 22:06:06,637] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-02-22 22:06:06,638] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-02-22 22:06:06,926] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-02-22 22:06:06,926] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-02-22 22:06:06,926] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-02-22 22:06:06,926] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 500000000
[2024-02-22 22:06:06,926] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 500000000
[2024-02-22 22:06:06,926] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: False
[2024-02-22 22:06:06,926] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
==> Epoch 0/30
[2024-02-22 22:06:09,642] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-02-22 22:06:09,643] [INFO] [utils.py:803:see_memory_usage] MA 27.36 GB         Max_MA 28.09 GB         CA 28.46 GB         Max_CA 28 GB 
[2024-02-22 22:06:09,643] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 80.57 GB, percent = 16.0%
[2024-02-22 22:06:09,852] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-02-22 22:06:09,853] [INFO] [utils.py:803:see_memory_usage] MA 30.31 GB         Max_MA 31.78 GB         CA 32.88 GB         Max_CA 33 GB 
[2024-02-22 22:06:09,853] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 80.66 GB, percent = 16.0%
[2024-02-22 22:06:09,854] [INFO] [stage_1_and_2.py:513:__init__] optimizer state initialized
[2024-02-22 22:06:10,061] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-02-22 22:06:10,062] [INFO] [utils.py:803:see_memory_usage] MA 30.31 GB         Max_MA 30.31 GB         CA 32.88 GB         Max_CA 33 GB 
[2024-02-22 22:06:10,062] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 80.87 GB, percent = 16.1%
[2024-02-22 22:06:10,084] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-02-22 22:06:10,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-02-22 22:06:10,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f3990b0f340>
[2024-02-22 22:06:10,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2024-02-22 22:06:10,090] [INFO] [config.py:968:print] DeepSpeedEngine configuration:
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   amp_enabled .................. False
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   amp_params ................... False
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   bfloat16_enabled ............. False
[2024-02-22 22:06:10,090] [INFO] [config.py:972:print]   checkpoint_parallel_write_pipeline  False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   checkpoint_tag_validation_enabled  True
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   checkpoint_tag_validation_fail  False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3990b0fdf0>
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   communication_data_type ...... None
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   curriculum_enabled_legacy .... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   curriculum_params_legacy ..... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   data_efficiency_enabled ...... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   dataloader_drop_last ......... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   disable_allgather ............ False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   dump_state ................... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   dynamic_loss_scale_args ...... None
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_enabled ........... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_gas_boundary_resolution  1
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_layer_num ......... 0
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_max_iter .......... 100
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_stability ......... 1e-06
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_tol ............... 0.01
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   eigenvalue_verbose ........... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   elasticity_enabled ........... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   fp16_auto_cast ............... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   fp16_enabled ................. True
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   fp16_master_weights_and_gradients  False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   global_rank .................. 0
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   grad_accum_dtype ............. None
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   gradient_accumulation_steps .. 20
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   gradient_clipping ............ 1.0
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   gradient_predivide_factor .... 1.0
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   initial_dynamic_scale ........ 65536
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   load_universal_checkpoint .... False
[2024-02-22 22:06:10,091] [INFO] [config.py:972:print]   loss_scale ................... 0
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   memory_breakdown ............. False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   mics_hierarchial_params_gather  False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   mics_shard_size .............. -1
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   optimizer_legacy_fusion ...... False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   optimizer_name ............... adamw
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   optimizer_params ............. {'lr': 1e-05, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   pld_enabled .................. False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   pld_params ................... False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   prescale_gradients ........... False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   scheduler_name ............... WarmupDecayLR
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   scheduler_params ............. {'total_num_steps': 15000, 'warmup_min_lr': 0, 'warmup_max_lr': 1e-05, 'warmup_num_steps': 50, 'warmup_type': 'linear'}
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   sparse_attention ............. None
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   sparse_gradients_enabled ..... False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   steps_per_print .............. 10
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   train_batch_size ............. 160
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   train_micro_batch_size_per_gpu  4
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   use_node_local_storage ....... False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   wall_clock_breakdown ......... False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   weight_quantization_config ... None
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   world_size ................... 2
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   zero_allow_untested_optimizer  False
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   zero_enabled ................. True
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   zero_force_ds_cpu_optimizer .. True
[2024-02-22 22:06:10,092] [INFO] [config.py:972:print]   zero_optimization_stage ...... 2
[2024-02-22 22:06:10,092] [INFO] [config.py:958:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 20, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 1e-05, 
            "weight_decay": 0.0, 
            "betas": [0.9, 0.95]
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "total_num_steps": 1.500000e+04, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 1e-05, 
            "warmup_num_steps": 50, 
            "warmup_type": "linear"
        }
    }, 
    "fp16": {
        "enabled": true
    }, 
    "bf16": {
        "enabled": false
    }, 
    "gradient_clipping": 1.0, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "allgather_bucket_size": 5.000000e+08
    }
}
==> Epoch 0/30
/workspace/ChatterBox-Finetuning/model/llava/model/llava_jack_gpt4roi.py:302: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  assert bbox_token_id == None or (torch.sum(torch.tensor(cur_input_ids == bbox_token_id)) == 0)
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/workspace/ChatterBox-Finetuning/model/llava/model/llava_jack_gpt4roi.py:302: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  assert bbox_token_id == None or (torch.sum(torch.tensor(cur_input_ids == bbox_token_id)) == 0)
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[2024-02-22 22:06:56,221] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
Epoch: [0][  1/500]	Time 45.756 (46.208)	Loss 20.6868 (20.3404)	VQALoss 2.7363 (2.7582)	VGLoss 17.9505 (17.5822)
[2024-02-22 22:06:56,227] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1 is about to be saved!
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-22 22:07:08,835] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./outputs/epoch_0/global_step1/mp_rank_00_model_states.pt
[2024-02-22 22:07:08,835] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/mp_rank_00_model_states.pt...
[2024-02-22 22:09:39,680] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/mp_rank_00_model_states.pt.
[2024-02-22 22:09:41,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-02-22 22:09:41,394] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./outputs/epoch_0/global_step1/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-02-22 22:09:54,038] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-02-22 22:09:54,039] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step1/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-22 22:09:54,039] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2024-02-22 22:09:54,635] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./outputs/epoch_0/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-02-22 22:09:54,654] [INFO] [engine.py:3381:_save_zero_checkpoint] zero checkpoint saved ./outputs/epoch_0/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-22 22:09:54,655] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2024-02-22 22:10:38,571] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
curr_lr =  [0.0]
curr_lr =  [0.0]
[2024-02-22 22:11:22,263] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
curr_lr =  curr_lr =  [2.0000000000000002e-07]
[2.0000000000000002e-07]
[2024-02-22 22:12:05,864] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
curr_lr =  [4.0000000000000003e-07]
curr_lr =  [4.0000000000000003e-07]
[2024-02-22 22:12:49,994] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
curr_lr =  [6.000000000000001e-07]
curr_lr =  [6.000000000000001e-07]
[2024-02-22 22:13:34,080] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
curr_lr = Epoch: [0][  6/500]	Time 44.086 (79.572)	Loss 19.6604 (20.5404)	VQALoss 2.8027 (2.7996)	VGLoss 16.8577 (17.7408) 
[8.000000000000001e-07]
curr_lr =  [8.000000000000001e-07]
[2024-02-22 22:14:18,488] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
curr_lr =  [1.0000000000000002e-06]
curr_lr =  [1.0000000000000002e-06]
[2024-02-22 22:15:02,486] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
curr_lr =  [1.2000000000000002e-06]
curr_lr =  [1.2000000000000002e-06]
[2024-02-22 22:15:46,304] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608
curr_lr =  [1.4000000000000001e-06]
curr_lr =  [1.4000000000000001e-06]
[2024-02-22 22:16:30,415] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304
curr_lr =  [1.6000000000000001e-06]
[2024-02-22 22:16:30,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[1.4000000000000001e-06], mom=[(0.9, 0.95)]
[2024-02-22 22:16:30,417] [INFO] [timer.py:260:stop] epoch=0/micro_step=200/global_step=10, RunningAvgSamplesPerSec=3.638054578505289, CurrSamplesPerSec=3.627239079725708, MemAllocated=31.1GB, MaxMemAllocated=36.52GB
curr_lr =  [1.6000000000000001e-06]
[2024-02-22 22:17:14,393] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152
Epoch: [0][ 11/500]	Time 43.977 (44.063)	Loss 18.7299 (20.6891)	VQALoss 2.6602 (2.8103)	VGLoss 16.0697 (17.8787)
curr_lr =  [1.8000000000000001e-06]
curr_lr =  [1.8000000000000001e-06]
[2024-02-22 22:17:58,202] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
curr_lr =  [2.0000000000000003e-06]
curr_lr =  [2.0000000000000003e-06]
[2024-02-22 22:18:41,971] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288
curr_lr =  [2.2e-06]
curr_lr =  [2.2e-06]
[2024-02-22 22:19:25,736] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144
curr_lr =  [2.4000000000000003e-06]
curr_lr =  [2.4000000000000003e-06]
[2024-02-22 22:20:09,376] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
curr_lr =  [2.6e-06]
curr_lr =  [2.6e-06]
[2024-02-22 22:20:54,214] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536
Epoch: [0][ 16/500]	Time 44.838 (43.964)	Loss 18.4172 (20.5097)	VQALoss 2.5488 (2.7912)	VGLoss 15.8684 (17.7185)
curr_lr =  [2.8000000000000003e-06]
curr_lr =  [2.8000000000000003e-06]
[2024-02-22 22:21:38,818] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
curr_lr =  [3e-06]
curr_lr =  [3e-06]
[2024-02-22 22:22:23,639] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
curr_lr =  [3.2000000000000003e-06]
curr_lr =  [3.2000000000000003e-06]
[2024-02-22 22:23:08,338] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
curr_lr =  [3.4000000000000005e-06]
curr_lr =  [3.4000000000000005e-06]
[2024-02-22 22:23:52,703] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
curr_lr =  [3.6000000000000003e-06]
[2024-02-22 22:23:52,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=20, lr=[3.4000000000000005e-06], mom=[(0.9, 0.95)]
[2024-02-22 22:23:52,704] [INFO] [timer.py:260:stop] epoch=0/micro_step=400/global_step=20, RunningAvgSamplesPerSec=3.6266919055726574, CurrSamplesPerSec=3.6065692478794658, MemAllocated=31.1GB, MaxMemAllocated=36.52GB
curr_lr =  [3.6000000000000003e-06]
[2024-02-22 22:24:36,343] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
curr_lr = Epoch: [0][ 21/500]	Time 43.640 (44.426)	Loss 15.6460 (20.7960)	VQALoss 2.6797 (2.7858)	VGLoss 12.9663 (18.0102) 
[3.8000000000000005e-06]
curr_lr =  [3.8000000000000005e-06]
curr_lr =  [4.2000000000000004e-06]
curr_lr =  [4.2000000000000004e-06]
curr_lr =  [4.600000000000001e-06]
curr_lr =  [4.600000000000001e-06]
[2024-02-22 22:26:49,523] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
curr_lr =  [4.800000000000001e-06]
curr_lr =  [4.800000000000001e-06]
curr_lr =  [5.2e-06]
curr_lr =  [5.2e-06]
curr_lr =  Epoch: [0][ 26/500]	Time 44.121 (44.358)	Loss 14.3240 (18.9280)	VQALoss 2.7422 (2.7897)	VGLoss 11.5818 (16.1382)[5.600000000000001e-06]

curr_lr =  [5.600000000000001e-06]
curr_lr =  [6e-06]
curr_lr =  [6e-06]
curr_lr =  [6.4000000000000006e-06]
curr_lr =  [6.4000000000000006e-06]
curr_lr =  [6.800000000000001e-06]
curr_lr =  [6.800000000000001e-06]
[2024-02-22 22:31:17,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=22, lr=[7e-06], mom=[(0.9, 0.95)]
curr_lr =  [7.2000000000000005e-06]
[2024-02-22 22:31:18,045] [INFO] [timer.py:260:stop] epoch=0/micro_step=600/global_step=30, RunningAvgSamplesPerSec=3.614536639492197, CurrSamplesPerSec=3.5238325883494683, MemAllocated=31.1GB, MaxMemAllocated=36.52GB
curr_lr =  [7.2000000000000005e-06]
curr_lr =  [7.600000000000001e-06]
Epoch: [0][ 31/500]	Time 44.587 (44.899)	Loss 12.6414 (17.0049)	VQALoss 2.2695 (2.7288)	VGLoss 10.3719 (14.2761)
curr_lr =  [7.600000000000001e-06]
curr_lr =  [8.000000000000001e-06]
curr_lr =  [8.000000000000001e-06]
curr_lr =  [8.400000000000001e-06]
curr_lr =  [8.400000000000001e-06]
curr_lr =  [8.8e-06]
curr_lr =  [8.8e-06]
curr_lr =  [9.200000000000002e-06]
curr_lr =  [9.200000000000002e-06]
curr_lr =  [9.600000000000001e-06]
Epoch: [0][ 36/500]	Time 45.001 (44.844)	Loss 11.2371 (16.2821)	VQALoss 2.5488 (2.6356)	VGLoss 8.6883 (13.6465)
curr_lr =  [9.600000000000001e-06]
curr_lr =  [1e-05]
curr_lr =  [1e-05]
curr_lr =  [9.998662207357859e-06]
curr_lr =  [9.998662207357859e-06]
curr_lr =  [9.99732441471572e-06]
curr_lr =  [9.99732441471572e-06]
[2024-02-22 22:38:46,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=22, lr=[9.99665551839465e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.995986622073579e-06]
[2024-02-22 22:38:46,777] [INFO] [timer.py:260:stop] epoch=0/micro_step=800/global_step=40, RunningAvgSamplesPerSec=3.6015570796782135, CurrSamplesPerSec=3.5385135785256647, MemAllocated=31.1GB, MaxMemAllocated=36.52GB
curr_lr =  [9.995986622073579e-06]
curr_lr =  [9.994648829431439e-06]
Epoch: [0][ 41/500]	Time 45.439 (45.072)	Loss 11.9831 (14.5810)	VQALoss 2.4355 (2.5239)	VGLoss 9.5476 (12.0571)
curr_lr =  [9.994648829431439e-06]
curr_lr =  [9.993311036789299e-06]
curr_lr =  [9.993311036789299e-06]
curr_lr =  [9.991973244147159e-06]
curr_lr =  [9.991973244147159e-06]
curr_lr =  [9.990635451505017e-06]
curr_lr =  [9.990635451505017e-06]
curr_lr =  [9.989297658862878e-06]
curr_lr =  [9.989297658862878e-06]
curr_lr = Epoch: [0][ 46/500]	Time 44.584 (44.842)	Loss 38.1763 (15.5640)	VQALoss 2.3965 (2.3738)	VGLoss 35.7798 (13.1902) 
[9.987959866220737e-06]
curr_lr =  [9.987959866220737e-06]
curr_lr =  [9.986622073578597e-06]
curr_lr =  [9.986622073578597e-06]
curr_lr =  [9.985284280936456e-06]
curr_lr =  [9.985284280936456e-06]
curr_lr =  [9.983946488294315e-06]
curr_lr =  [9.983946488294315e-06]
[2024-02-22 22:46:13,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=22, lr=[9.983277591973245e-06], mom=[(0.9, 0.95)]
curr_lr =  [9.982608695652175e-06]
[2024-02-22 22:46:13,828] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=50, RunningAvgSamplesPerSec=3.596856093301609, CurrSamplesPerSec=3.5971282970301224, MemAllocated=31.1GB, MaxMemAllocated=36.53GB
curr_lr =  [9.982608695652175e-06]
curr_lr =  [9.981270903010034e-06]
Epoch: [0][ 51/500]	Time 43.912 (44.262)	Loss 44.9737 (14.3341)	VQALoss 1.7373 (2.2667)	VGLoss 43.2364 (12.0675)
curr_lr =  [9.981270903010034e-06]
[2024-02-22 22:47:02,563] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step51 is about to be saved!
